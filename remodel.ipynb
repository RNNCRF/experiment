{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logging.getLogger().setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ReverseComplementLayer\n",
    "from models import CRF\n",
    "from models import CRF_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel('DEBUG')\n",
    "# config for server\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "os.environ['PATH'] = r'C:\\Users\\Rudolf\\Documents\\v9.0\\bin' + os.path.pathsep + os.environ['PATH'] \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "num_CPU = 2\n",
    "num_GPU = 0\n",
    "config = tf.ConfigProto( device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def func_metrics_display(funk):\n",
    "    def metricized(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        obj = funk(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        dt = end - start\n",
    "        if dt > 1:\n",
    "            format_str = ('%H hours' if dt >= 3600 else '') + ('%M minutes' if dt >= 60 else '') + '%S seconds'\n",
    "            logging.info('The function {} took {}'.format(funk.__name__, \n",
    "                                                          time.strftime( format_str ,time.gmtime(dt)) ))\n",
    "        else:\n",
    "            logging.info('The function {} took {:>10.4f}s'.format(funk.__name__, dt))\n",
    "        return obj\n",
    "    metricized.__name__=funk.__name__\n",
    "    return metricized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsufficientChromosomesException(Exception):\n",
    "    def __init__(self, chrom_avail, chrom_req, msg=None):\n",
    "        self.chrom_avail = chrom_avail\n",
    "        self.chrom_req = chrom_req\n",
    "        if not msg:\n",
    "            msg = 'Found {} chromosomes. Needed more than {} chromosomes.'.format(chrom_avail, chrom_req)\n",
    "        super(InsufficientChromosomesException, self).__init__(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(a, domain):\n",
    "    return domain[0]<=a<=domain[1]\n",
    "\n",
    "def is_in_domain(peak, domain):\n",
    "    '''\n",
    "    Checks to see if peak (tuple of ints) overlaps with domain (tuple of ints) \n",
    "    '''\n",
    "    p_start, p_end = peak\n",
    "    d_start, d_end = domain\n",
    "    return is_in(p_start, domain) or is_in(p_end, domain) or (is_in(d_start, peak) or is_in(d_end, peak))\n",
    "\n",
    "def any_in_domain(peaks, domain):\n",
    "    for peak in peaks:\n",
    "        if is_in_domain(peak, domain):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def which_in_domain(peaks,domain):\n",
    "    for peak in peaks:\n",
    "        if is_in_domain(peak,domain):\n",
    "            return peak\n",
    "    return None\n",
    "\n",
    "def domain2seq(chromosomes, chromosome, domain):\n",
    "    return chromosomes[chromosome][domain[0]:domain[1]]\n",
    "\n",
    "def where_in_domain(chromosomes, chromosome, peak, domain):\n",
    "    peak_start, peak_end = peak\n",
    "    domain_start, domain_end = domain\n",
    "    \n",
    "    seq = domain2seq(chromosomes, chromosome, domain)\n",
    "\n",
    "    labels = []\n",
    "    for i in range(domain_end-domain_start):\n",
    "        dna_location_ptr = domain_start+i\n",
    "        if dna_location_ptr<peak_start:\n",
    "            labels.append('O')\n",
    "        elif dna_location_ptr==peak_start:\n",
    "            labels.append('B')\n",
    "        elif peak_start<dna_location_ptr<peak_end:\n",
    "            labels.append('I')\n",
    "        elif dna_location_ptr==peak_end:\n",
    "            labels.append('E')\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    return ''.join(labels), (chromosome, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def if_not_pickled(pkl_file, generator,log=True,gen_name=None):\n",
    "    try:\n",
    "        name = gen_name if gen_name else generator.__name__\n",
    "    except AttributeError:\n",
    "        name = generator.func.__name__\n",
    "        \n",
    "    def generate_pkl():\n",
    "        obj = generator()\n",
    "        with open(pkl_file, 'wb') as pkl:\n",
    "            pickle.dump(obj, pkl)\n",
    "        return obj\n",
    "    \n",
    "    if not os.path.exists(pkl_file):\n",
    "        if log:\n",
    "            logging.info('No pickle for {} is found. Generating anew.'.format(name))               \n",
    "        obj = generate_pkl()\n",
    "    else:\n",
    "        if log:\n",
    "            logging.info('Loading pickled {}'.format(name))\n",
    "        try:\n",
    "            with open(pkl_file, 'rb') as pkl:\n",
    "                obj = pickle.load(pkl)\n",
    "        except (EOFError,pickle.UnpicklingError) as e :\n",
    "            logging.error('A pickle file was corrupted: {}'.format(pkl_file))\n",
    "            if log:\n",
    "                logging.info('Regenerating corrupted pickle: {}'.format(pkl_file))\n",
    "            obj = generate_pkl()\n",
    "    if log:\n",
    "        logging.info('Finished setting up {}'.format(name))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2locNbound(label_file, _cellline, log=True):\n",
    "    from tqdm import tqdm\n",
    "    chr2locNbound = {}\n",
    "    with open(label_file) as labels:\n",
    "        line_gen = (line for line in labels)\n",
    "        column_names = next(line_gen).strip().split() \n",
    "        # columns names are chr start stop <cell line 1> ... <cell line n>\n",
    "        _, _, _, *celllines = column_names\n",
    "        prev_chrom = None\n",
    "        for line in tqdm(line_gen, total=get_num_lines(label_file)):\n",
    "            chromosome, start, stop, *bound_per_cellline = [x.strip() for x in line.strip().split()]\n",
    "            try:\n",
    "                start,stop = [int(x) for x in [start,stop]]\n",
    "            except ValueError:\n",
    "                # ill formatted entry\n",
    "                continue\n",
    "            if log and prev_chrom!=chromosome:\n",
    "                prev_chrom=chromosome\n",
    "                logging.info('Working on {}\\n'.format(chromosome))\n",
    "            for cellline, is_bound in zip(celllines, bound_per_cellline):\n",
    "                if cellline.lower() != _cellline.lower():\n",
    "                    continue\n",
    "                try:\n",
    "                    chr2locNbound[chromosome].append(((start,stop), is_bound))\n",
    "                except KeyError:\n",
    "                    chr2locNbound[chromosome] = [((start,stop), is_bound)]                \n",
    "    return chr2locNbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_hg19(hg_genome_fasta):\n",
    "    chromosomes = {}\n",
    "    with open(hg_genome_fasta) as hg19:\n",
    "        chromosome = None\n",
    "        for line in hg19:\n",
    "            if line.startswith('>'):\n",
    "                chromosome = line[1:].strip()\n",
    "                chromosomes[chromosome] = []\n",
    "            else: \n",
    "                chromosomes[chromosome].append(line.strip().upper())\n",
    "\n",
    "    for k,v in chromosomes.items():\n",
    "        chromosomes[k] = ''.join(v)   \n",
    "    return chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2filter_locs(filter_file):\n",
    "    chr2filter_locs = {}\n",
    "    with open(filter_file) as filter_f:\n",
    "        for line in filter_f:\n",
    "            chromosome, start, end = line.strip().split()\n",
    "            start, end = [int(x) for x in (start, end)]\n",
    "            try:\n",
    "                chr2filter_locs[chromosome].append((start,end))\n",
    "            except KeyError:\n",
    "                chr2filter_locs[chromosome] = [(start,end)]\n",
    "    return chr2filter_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2locNpeaks(celllineNtf_peakfile, filter_file=None, chr2filter_locs=None):\n",
    "    chr2locNpeaks = {}\n",
    "    from tqdm import tqdm\n",
    "    with open(celllineNtf_peakfile) as peaks:\n",
    "        for line in tqdm(peaks, total=get_num_lines(celllineNtf_peakfile)):\n",
    "            chromosome, start, stop, name, score, strand, signal, p, q, peak = line.strip().split()\n",
    "            start, stop = [int(x) for x in [start,stop]]\n",
    "            try:\n",
    "                chr2locNpeaks[chromosome]\n",
    "            except KeyError:\n",
    "                chr2locNpeaks[chromosome]=[]\n",
    "                \n",
    "            if filter_file:\n",
    "                if not chr2filter_locs:\n",
    "                    raise ValueError('Need chr2filter_locs if using filter_file')\n",
    "                filter_locs = chr2filter_locs[chromosome]\n",
    "                overlap=False\n",
    "                for loc in chr2filter_locs[chromosome]:\n",
    "                    if loc[0]<=start<=loc[1] or loc[0]<=stop<=loc[1]:\n",
    "                        overlap=True\n",
    "                        break\n",
    "                if not overlap:\n",
    "                    continue\n",
    "            chr2locNpeaks[chromosome].append((start,\n",
    "                                              stop,\n",
    "                                             {'name' : name,\n",
    "                                             'score': int(score),\n",
    "                                             'strand': strand,\n",
    "                                             'p-value':float(p),\n",
    "                                             'q-value':float(q),\n",
    "                                             'peak':int(peak)}))\n",
    "        for chromosome, lst in chr2locNpeaks.items():\n",
    "            chr2locNpeaks[chromosome] = sorted(lst,key= lambda x:x[0])\n",
    "    \n",
    "    return chr2locNpeaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sequence(seq, chromosomes):\n",
    "    chromosome=seq[0]\n",
    "    domain=seq[1]\n",
    "    return domain2seq(chromosomes, chromosome, domain)\n",
    "\n",
    "@func_metrics_display\n",
    "def gen_chr2labelsNseq(chromosomes, chr2locNbound, chr2locNpeaks):\n",
    "    chr2labelsNseq = {}\n",
    "    from tqdm import tqdm\n",
    "    shared_chromosomes = set(chr2locNbound.keys()) & set(chr2locNpeaks.keys())\n",
    "    logging.info('Generating labels and seq for this set of chromosomes {}'.format(str(shared_chromosomes)))\n",
    "    for chromosome in tqdm(shared_chromosomes):\n",
    "        logging.info('Generating labels and seq for {}'.format(chromosome))\n",
    "        bound_locs = [x for x in chr2locNbound[chromosome] if x[1]=='B']\n",
    "        bound_locs = [loc[0] for loc in bound_locs]\n",
    "        peaks_locs = [x[:2] for x in chr2locNpeaks[chromosome]]\n",
    "        \n",
    "#         peak_offsets = [x[2]['peak'] for x in chr2locNpeaks[chromosome]]\n",
    "        count = 0\n",
    "        for bound_loc in bound_locs:\n",
    "            # generate peak information\n",
    "            the_peak = which_in_domain(peaks_locs, bound_loc)\n",
    "            if not the_peak:\n",
    "                logging.warning('Cannot find peak {}'.format(bound_loc))\n",
    "                continue\n",
    "            the_peak_i = peaks_locs.index(the_peak)\n",
    "            labels, seq = where_in_domain(chromosomes,\n",
    "                                          chromosome,\n",
    "                                          the_peak,\n",
    "                                          bound_loc)\n",
    "            if not ('B' in labels or 'I' in labels or 'E' in labels):\n",
    "                raise ValueError('BIE not found in labels\\nsequence:{}\\nlabels{}'.format(seq,labels))\n",
    "            try:\n",
    "                chr2labelsNseq[chromosome].append((labels, seq))\n",
    "            except KeyError:\n",
    "                chr2labelsNseq[chromosome] = [(labels, seq)]\n",
    "            count += 1\n",
    "        logging.info('Found {}/{} seqs for bound locations'.format(count, len(bound_locs)))        \n",
    "        unbound_locs = [x for x in chr2locNbound[chromosome] if x[1]=='U']\n",
    "        unbound_locs = [loc[0] for loc in unbound_locs]\n",
    "        count = 0\n",
    "        for unbound_loc in unbound_locs:\n",
    "            seq = (chromosome, unbound_loc)\n",
    "            try:\n",
    "                chr2labelsNseq[chromosome].append((None, seq))\n",
    "            except KeyError:\n",
    "                chr2labelsNseq[chromosome] = [(None, seq)]\n",
    "            count += 1\n",
    "        logging.info('Found {}/{} seqs for bound locations'.format(count, len(unbound_locs)))  \n",
    "    return chr2labelsNseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_structure_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = []\n",
    "        agg = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                lines.append(','.join(agg))\n",
    "                agg = []\n",
    "            else:\n",
    "                agg.append(line.strip())\n",
    "        lines.append(','.join(agg))\n",
    "    del lines[0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_paths_exist(*paths):\n",
    "    import os\n",
    "    return [os.path.exists(path) for path in paths]\n",
    "\n",
    "def all_paths_exists(*paths):\n",
    "    from functools import reduce\n",
    "    return reduce(lambda acc,x: acc and x ,map_paths_exist(*paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instanceHolder = {\"instance\": None}\n",
    "class ClassWrapper(CRF_ext):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        instanceHolder[\"instance\"] = self\n",
    "        super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "def loss(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"loss_function\")\n",
    "    return method(*args)\n",
    "def accuracy(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"accuracy\")\n",
    "    return method(*args)\n",
    "def viterbi_precision(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_precision\")\n",
    "    return method(*args)\n",
    "def f1(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_f1\")\n",
    "    return method(*args)\n",
    "def recall(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_recall\")\n",
    "    return method(*args)\n",
    "def precision(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_precision\")\n",
    "    return method(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from functools import partial \n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "_cache = {}\n",
    "class DataManager:\n",
    "    \n",
    "    def __init__(self,label_file, celllineNtf_peakfile, \n",
    "                 bigwig_duke_unique_file = './wgEncodeDukeMapabilityUniqueness35bp.bigWig',\n",
    "                 bigwig_dnase_file = None,\n",
    "                 filter_file=None,\n",
    "                 use_pickler=False, memory_avail=8192, \n",
    "                 output_dir='./',\n",
    "                 only_label_dataset = True,\n",
    "                 move_finished_src = None, \n",
    "                 reduce_negative_samples=True, \n",
    "                 chr_valid = ['chr11'], chr_test = ['chr1', 'chr8', 'chr21'], \n",
    "                 check_set_ratio=9, debug={}):\n",
    "        \n",
    "        self.label_file = label_file\n",
    "        self.celllineNtf_peakfile = celllineNtf_peakfile\n",
    "        self.bigwig_duke_unique_file = bigwig_duke_unique_file\n",
    "        self.duke_bw = bigwig_duke_unique_file\n",
    "        self.use_pickler = use_pickler\n",
    "        self.memory_avail=memory_avail\n",
    "        self.output_dir = output_dir\n",
    "        self.only_label_dataset=only_label_dataset\n",
    "        self.move_finished_src = move_finished_src\n",
    "        self.filter_file = filter_file\n",
    "        self.reduce_negative_samples = reduce_negative_samples\n",
    "        self.chr_valid = chr_valid\n",
    "        self.chr_test = chr_test\n",
    "        self.check_set_ratio = check_set_ratio\n",
    "        self.debug = debug\n",
    "        self.exp_type, self._cellline, self.tf_name,_, self.set_name, self.peak_type = os.path.basename(celllineNtf_peakfile).split('.')\n",
    "        self.dnase_bw = bigwig_dnase_file if bigwig_dnase_file else '%s.1x.bw' % self._cellline.lower()\n",
    "        self.exp_id = '{}_{}'.format(self.tf_name, self._cellline)\n",
    "#         self.bigwig_cellline_file = get_bigwig_celline_file(self._cellline)\n",
    "        self.positive_samples_out, self.negative_samples_out, self.labels_out = [self.exp_id+'_'+x for x in ['positive_samples.txt', 'negative_samples.txt', 'labels.txt']]\n",
    "        self.chr_positive_out, self.chr_negative_out = [self.exp_id+'_chr_%s.npy' % sign for sign in ['positive', 'negative']]\n",
    "\n",
    "    def _cacher_(func):\n",
    "        def to_cache_func(self, *args, **kwargs):\n",
    "            if func.__name__ in _cache:\n",
    "                return _cache[func.__name__]\n",
    "            obj = func(self, *args, **kwargs)\n",
    "            _cache[func.__name__] = obj\n",
    "            return obj\n",
    "        return to_cache_func\n",
    "    \n",
    "    def release_memory(self):\n",
    "        import gc\n",
    "        _cache = {}\n",
    "        gc.collect()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2locNbound(self):\n",
    "        chr2locNbound_pklfile = '{}_chr2locNbound.pkl'.format(self.exp_id)\n",
    "        pgen_chr2locNbound = partial(gen_chr2locNbound, self.label_file, self._cellline)\n",
    "        chr2locNbound = if_not_pickled(chr2locNbound_pklfile,\n",
    "                                       pgen_chr2locNbound)\n",
    "        return chr2locNbound\n",
    "    @property\n",
    "    def chr2locNbound(self):\n",
    "        return self._chr2locNbound()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2filter_locs(self):\n",
    "        chr2filter_locs_pklfile = '{}_chr2filter_locs.pkl'.format(self.exp_id)\n",
    "        pgen_chr2filter_locs = partial(gen_chr2filter_locs, \n",
    "                                       self.filter_file)\n",
    "        chr2filter_locs = if_not_pickled(chr2filter_locs_pklfile, \n",
    "                                         pgen_chr2filter_locs)\n",
    "        return chr2filter_locs\n",
    "    @property\n",
    "    def chr2filter_locs(self):\n",
    "        return self._chr2filter_locs()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2locNpeaks(self):\n",
    "        chr2locNpeaks_pklfile = '{}_chr2locNpeaks.pkl'.format(self.exp_id) if self.filter_file else '{}_chr2locNpeaks_full.pkl'.format(self.exp_id)\n",
    "        p_genchr2locNpeaks = partial(gen_chr2locNpeaks, \n",
    "                                     self.celllineNtf_peakfile, \n",
    "                                     self.filter_file, \n",
    "                                     self.chr2filter_locs if self.filter_file else None)\n",
    "        chr2locNpeaks = if_not_pickled(chr2locNpeaks_pklfile,\n",
    "                                       p_genchr2locNpeaks)\n",
    "        return chr2locNpeaks\n",
    "    @property\n",
    "    def chr2locNpeaks(self):\n",
    "        return self._chr2locNpeaks()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chromosomes(self):\n",
    "        hg_pkl = 'hg19.pkl'\n",
    "        hg_genome_fasta = './hg19.genome.fa'\n",
    "        pgen_hg19 = partial(gen_hg19, hg_genome_fasta)\n",
    "        chromosomes = if_not_pickled(hg_pkl, pgen_hg19)\n",
    "        return chromosomes\n",
    "    @property\n",
    "    def chromosomes(self):\n",
    "        return self._chromosomes()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2labelsNseq(self):\n",
    "        chr2labelsNseq_pkl = '{}_chr2labelsNseq.pkl'.format(self.exp_id)\n",
    "        pgen_chr2labelsNseq = partial(gen_chr2labelsNseq,\n",
    "                                      self.chromosomes,\n",
    "                                      self.chr2locNbound, \n",
    "                                      self.chr2locNpeaks)\n",
    "        chr2labelsNseq = if_not_pickled(chr2labelsNseq_pkl, \n",
    "                                        pgen_chr2labelsNseq)\n",
    "        del _cache['_chr2locNpeaks']\n",
    "        del _cache['_chr2locNbound']\n",
    "        del _cache['_chr2filter_locs']\n",
    "        return chr2labelsNseq\n",
    "    @property\n",
    "    def chr2labelsNseq(self):\n",
    "        return self._chr2labelsNseq()\n",
    "    \n",
    "    def _create_samples(self):\n",
    "        \n",
    "        logging.info('Creating positive/negative samples')\n",
    "        # generate samples\n",
    "        positive_samples, negative_samples, labels_positives = [],[],[]\n",
    "        pos_chr, neg_chr = [], [] # keep track of which chromosome each are from\n",
    "        \n",
    "        chr_training = set(self.chr2labelsNseq.keys()) - set(self.chr_valid) - set(self.chr_test)\n",
    "        if not len(chr_training):\n",
    "            raise InsufficientChromosomesException(set(self.chr2labelsNseq.keys()), set(self.chr_valid) | set(self.chr_test))\n",
    "        from tqdm import tqdm\n",
    "        for chromosome, labelsNseq in tqdm(self.chr2labelsNseq.items()):\n",
    "            _positive_samples,_negative_samples, _labels_positives = [],[],[]\n",
    "            _pos_chr, _neg_chr = [], []\n",
    "            logging.info('Working on {}'.format(chromosome))\n",
    "            count = 0\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            for label, seq in labelsNseq:\n",
    "                # labels is None if negative sample\n",
    "                sequence = seq2sequence(seq, self.chromosomes)\n",
    "                if 'N' not in sequence:\n",
    "                    if label==None:\n",
    "                        _negative_samples.append(sequence)\n",
    "                        _neg_chr.append(seq)\n",
    "                        neg_count += 1\n",
    "                    else:\n",
    "                        _positive_samples.append(sequence)\n",
    "                        _labels_positives.append(label)\n",
    "                        _pos_chr.append(seq)\n",
    "                        pos_count += 1\n",
    "                    count += 1\n",
    "            logging.info('Found {} samples for {}; {} positive and {} negative'.format(count, \n",
    "                                                                                       chromosome, \n",
    "                                                                                       pos_count, \n",
    "                                                                                       neg_count))\n",
    "            # throw away some negative samples\n",
    "#             if self.reduce_negative_samples :\n",
    "#                 logging.info('Reducing negative samples for {}'.format(chromosome))\n",
    "#                 ratio = 1 if not (chromosome in self.chr_valid or chromosome in self.chr_test) else self.check_set_ratio\n",
    "#                 if len(_negative_samples) > len(_positive_samples):\n",
    "#                     npified = np.array([_negative_samples, _neg_chr], dtype=np.object)\n",
    "#                     n_to_choose_from = ratio * len(_positive_samples)\n",
    "#                     if n_to_choose_from > len(_negative_samples):\n",
    "#                         n_to_choose_from = len(_negative_samples)\n",
    "#                     indices_chosen = np.random.choice(len(_negative_samples), \n",
    "#                                                       n_to_choose_from, \n",
    "#                                                       replace=False)\n",
    "#                     chosen_samples, chosen_chr = npified[:,indices_chosen]\n",
    "#                     _negative_samples, _neg_chr = list(chosen_samples), list(chosen_chr)\n",
    "            \n",
    "            logging.info('Found {} samples for {}; {} positive and {} negative'.format(count, \n",
    "                                                                                       chromosome, \n",
    "                                                                                       len(_positive_samples), \n",
    "                                                                                       len(_negative_samples)))\n",
    "            positive_samples.extend(_positive_samples)\n",
    "            negative_samples.extend(_negative_samples)\n",
    "            labels_positives.extend(_labels_positives)\n",
    "            pos_chr.extend(_pos_chr)\n",
    "            neg_chr.extend(_neg_chr)\n",
    "            \n",
    "        # write samples to disk\n",
    "        logging.info('Writing samples to disk')\n",
    "        def write_to_disk(path, samples):\n",
    "            with open(path, 'w') as samples_file:\n",
    "                for i,sample in enumerate(samples):\n",
    "                    samples_file.write('>{}__{}\\n'.format(self.tf_name, i))\n",
    "                    samples_file.write('{}\\n'.format(sample))\n",
    "        \n",
    "        assert len(positive_samples) == len(pos_chr) == len(labels_positives)\n",
    "        assert len(negative_samples) == len(neg_chr)\n",
    "        write_to_disk(self.positive_samples_out, positive_samples)\n",
    "        write_to_disk(self.negative_samples_out, negative_samples)\n",
    "        write_to_disk(self.labels_out, labels_positives)\n",
    "        np.save(self.chr_positive_out, np.array(pos_chr, dtype=np.object))\n",
    "        np.save(self.chr_negative_out, np.array(neg_chr, dtype=np.object))\n",
    "        logging.info('Done writing samples to disk')\n",
    "        self._positive_samples, self._negative_samples, self._labels_positives = positive_samples, negative_samples, labels_positives\n",
    "        self._pos_chr, self._neg_chr = pos_chr, neg_chr\n",
    "        return positive_samples, negative_samples, labels_positives, pos_chr, neg_chr\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        # load samples from disk\n",
    "        logging.info('Loading positive/negative samples from disk')\n",
    "        positive_samples, negative_samples, labels_positives = [], [], []\n",
    "        with open(self.positive_samples_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    positive_samples.append(line.strip())\n",
    "        with open(self.negative_samples_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    negative_samples.append(line.strip())\n",
    "        with open(self.labels_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    labels_positives.append(line.strip())  \n",
    "        pos_chr = np.load(self.chr_positive_out)\n",
    "        neg_chr = np.load(self.chr_negative_out) \n",
    "        logging.info('Done loading samples from disk')\n",
    "        self._positive_samples, self._negative_samples, self._labels_positives, = positive_samples, negative_samples, labels_positives\n",
    "        self._pos_chr, self._neg_chr = pos_chr, neg_chr\n",
    "        return positive_samples, negative_samples, labels_positives, pos_chr, neg_chr\n",
    "    \n",
    "    @_cacher_\n",
    "    def _samples(self):\n",
    "        sample_files = [self.positive_samples_out, \n",
    "                        self.negative_samples_out, \n",
    "                        self.labels_out, \n",
    "                        self.chr_positive_out, \n",
    "                        self.chr_negative_out ]\n",
    "        \n",
    "        if not all_paths_exists(*sample_files):\n",
    "            return self._create_samples()\n",
    "        else: \n",
    "            return self._load_samples()\n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples()\n",
    "    \n",
    "    @property\n",
    "    def positive_samples(self):\n",
    "        try:\n",
    "            return self._positive_samples\n",
    "        except:\n",
    "            return self.samples[0]\n",
    "    @property\n",
    "    def negative_samples(self):\n",
    "        try:\n",
    "            return self._negative_samples\n",
    "        except:\n",
    "            return self.samples[1]\n",
    "    @property\n",
    "    def labels_positives(self):\n",
    "        try:\n",
    "            return self._labels_positives\n",
    "        except:\n",
    "            return self.samples[2]\n",
    "    @property\n",
    "    def pos_chr(self):\n",
    "        try:\n",
    "            return self._pos_chr\n",
    "        except:\n",
    "            return self.samples[3]\n",
    "    @property\n",
    "    def neg_chr(self):\n",
    "        try:\n",
    "            return self._neg_chr\n",
    "        except:\n",
    "            return self.samples[4]\n",
    "    \n",
    "    @func_metrics_display\n",
    "    def dnashapeR(self, r_lib_location = \"C:/Users/Rudolf/Documents/R/win-library/3.5\"):\n",
    "        # setup structural information\n",
    "        # setup R\n",
    "        import rpy2\n",
    "        from rpy2.robjects.packages import importr\n",
    "        import rpy2.robjects as robjects\n",
    "\n",
    "        # set the available amount of memory\n",
    "        robjects.r('memory.limit(size = {})'.format(self.memory_avail))\n",
    "\n",
    "        base = importr('base')\n",
    "        utils = importr('utils')\n",
    "        logging.info('Using {}'.format(str(base._libPaths())))\n",
    "\n",
    "        # if DNAshapeR cannot be found try this:\n",
    "        robjects.r( \".libPaths('{}')\".format(r_lib_location))\n",
    "\n",
    "        from functools import reduce\n",
    "\n",
    "        if not all_paths_exists(*[self.positive_samples_out+ext for ext in ['.EP', '.HelT', '.MGW', '.ProT', '.Roll']]):\n",
    "            logging.info('Running DNAshapeR for positive samples')\n",
    "            base = importr('base')\n",
    "            utils = importr('utils')\n",
    "            dna_shape = importr('DNAshapeR', lib_loc=r_lib_location)\n",
    "            #rpy2 does not know how to release memory\n",
    "            @rmem_manage\n",
    "            def process_positive():\n",
    "                r_statements = []\n",
    "                r_statements.append('library(DNAshapeR)')\n",
    "                r_statements.append('pred <- getShape(\"./{}\")'.format(self.positive_samples_out))\n",
    "                r_cmd = '\\n'.join(r_statements)\n",
    "                robjects.r(r_cmd)\n",
    "            process_positive()\n",
    "        else:\n",
    "            logging.info('Skipping DNAshapeR for positive samples; already exists')\n",
    "        if not all_paths_exists(*[self.negative_samples_out+ext for ext in ['.EP', '.HelT', '.MGW', '.ProT', '.Roll']]):\n",
    "            logging.info('Running DNAshapeR for negative samples')\n",
    "            base = importr('base')\n",
    "            utils = importr('utils')\n",
    "            dna_shape = importr('DNAshapeR', lib_loc=r_lib_location)\n",
    "            gc.collect()\n",
    "            @rmem_manage\n",
    "            def process_negative():\n",
    "                r_statements = []\n",
    "                r_statements.append('library(DNAshapeR)')\n",
    "                r_statements.append('pred <- getShape(\"./{}\")'.format(self.negative_samples_out))\n",
    "                r_cmd = '\\n'.join(r_statements)\n",
    "                robjects.r(r_cmd)\n",
    "            process_negative()\n",
    "        else:\n",
    "            logging.info('Skipping DNAshapeR for negative samples; already exists')\n",
    "    \n",
    "    @_cacher_\n",
    "    def _h5py_duke(self):\n",
    "        return gen_h5py4bw(self.duke_bw, 'duke_unique')\n",
    "    \n",
    "    @property\n",
    "    def h5py_duke(self):\n",
    "        return self._h5py_duke()\n",
    "    \n",
    "    def duke_unique(self, chromosome, start, stop):\n",
    "        return self.h5py_duke[chromosome][start:stop]\n",
    "        \n",
    "    @_cacher_\n",
    "    def _h5py_dnase(self):\n",
    "        return gen_h5py4bw(self.dnase_bw, self._cellline)\n",
    "    \n",
    "    @property\n",
    "    def h5py_dnase(self):\n",
    "        return self._h5py_dnase()\n",
    "    \n",
    "    def dnase(self, chromosome, start, stop):\n",
    "        return self.h5py_dnase[chromosome][start:stop]\n",
    "    \n",
    "    def create_datagen_from_samples(self, balance_valid_ratio=9, useDNAshapeR=False):\n",
    "        '''\n",
    "        Returns functions that takes a batch_size as input and returns generators\n",
    "        '''\n",
    "        import pickle\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        # files describing structure of dna\n",
    "        exts = ['']\n",
    "        if useDNAshapeR:\n",
    "            raise NotImplementedError('DNAshapeR is not streamable for large datasets')\n",
    "        dataset_pkl = '%s_dataset.pkl'%self.exp_id\n",
    "        dataset_pkl = dataset_pkl.lower()\n",
    "        import os\n",
    "        import pickle\n",
    "        if os.path.exists(dataset_pkl):\n",
    "            logging.info('Reading static data')\n",
    "            with open(dataset_pkl, 'rb') as pkl:\n",
    "                train_positive_samples = pickle.load(pkl)\n",
    "                train_negative_samples = pickle.load(pkl)\n",
    "                train_labels_positives = pickle.load(pkl)\n",
    "                train_pos_chr = pickle.load(pkl) \n",
    "                train_neg_chr = pickle.load(pkl)\n",
    "                training_cls = pickle.load(pkl)\n",
    "                training_labels = pickle.load(pkl)\n",
    "                valid_samples = pickle.load(pkl)\n",
    "                valid_chr = pickle.load(pkl)\n",
    "                valid_cls = pickle.load(pkl)\n",
    "                valid_labels = pickle.load(pkl)\n",
    "                test_samples = pickle.load(pkl)\n",
    "                test_chr = pickle.load(pkl)\n",
    "                test_cls = pickle.load(pkl)\n",
    "                test_labels = pickle.load(pkl)\n",
    "                self.dataset2counts_pos = pickle.load(pkl)\n",
    "                self.dataset2counts_neg = pickle.load(pkl)\n",
    "                self.training_length = pickle.load(pkl)\n",
    "                self.valid_length = pickle.load(pkl)\n",
    "                self.test_length = pickle.load(pkl)\n",
    "                self.sample_length = pickle.load(pkl)\n",
    "                self.feature_dimensions = pickle.load(pkl)  \n",
    "                logging.info('Loaded training/valid/test of sizes {}/{}/{}'.format(self.training_length, self.valid_length, self.test_length))\n",
    "        else:     \n",
    "            logging.info('Getting samples')\n",
    "            # positive_samples and negative_samples are lists of sequences:str\n",
    "            # pos_chr and neg_chr are lists of (chromosome:str, (start:int, stop:int))\n",
    "            # labels_positives is a list of (for crf) labels:str\n",
    "            positive_samples, negative_samples, labels_positives, pos_chr, neg_chr = self.samples\n",
    "\n",
    "            logging.info('Shuffling samples')\n",
    "            assert len(positive_samples) == len(labels_positives) == len(pos_chr)\n",
    "            assert len(negative_samples) == len(neg_chr)\n",
    "\n",
    "            pos_len = len(positive_samples)\n",
    "            neg_len = len(negative_samples)\n",
    "\n",
    "            class Pair:\n",
    "                __slots__=['chromosome', 'domain']\n",
    "                def __init__(self, chromosome, domain):\n",
    "                    self.chromosome = chromosome\n",
    "                    self.domain = domain\n",
    "                def __repr__(self):\n",
    "                    return '('+str(self.chromosome)+', '+str(self.domain)+')'\n",
    "                def reveal(self):\n",
    "                    return (self.chromosome, self.domain)\n",
    "\n",
    "            @func_metrics_display\n",
    "            def shuffle(*arrays):\n",
    "                import numpy as np\n",
    "                try:\n",
    "                    npified = np.array(arrays, dtype=np.object)\n",
    "                except Exception as e:\n",
    "                    logging.error('{}'.format([np.array(a).shape for a in arrays]))\n",
    "                    raise e\n",
    "                length = npified.shape[1]\n",
    "                indices_chosen = np.random.choice(length, length, replace=False)\n",
    "                return npified[:,indices_chosen]\n",
    "\n",
    "            pos_pairs = [Pair(*c) for c in pos_chr]\n",
    "            neg_pairs = [Pair(*c) for c in neg_chr]\n",
    "\n",
    "            positive_samples, labels_positives, pos_pairs = shuffle(positive_samples, labels_positives, pos_pairs)\n",
    "            negative_samples, neg_pairs = shuffle(negative_samples, neg_pairs)\n",
    "\n",
    "            pos_chr = [pair.reveal() for pair in pos_pairs]\n",
    "            neg_chr = [pair.reveal() for pair in neg_pairs]\n",
    "\n",
    "            # create stats on number of pos/neg for training/valid/test sets\n",
    "            logging.info('Creating stats on number of pos/neg for training/valid/test sets')\n",
    "\n",
    "            chr_valid_set = set(self.chr_valid)\n",
    "            chr_test_set = set(self.chr_test)\n",
    "\n",
    "            def try_addone(counts, key):\n",
    "                try:\n",
    "                    counts[key]+=1\n",
    "                except KeyError:\n",
    "                    counts[key]=1\n",
    "            @func_metrics_display\n",
    "            def count(chrs):            \n",
    "                counts = {}   \n",
    "                for chromosome, (start,stop) in chrs:\n",
    "                    if chromosome in chr_valid_set:\n",
    "                        try_addone(counts, 'valid')\n",
    "                    elif chromosome in chr_test_set:\n",
    "                        try_addone(counts, 'test')\n",
    "                    else: # in training set\n",
    "                        try_addone(counts, 'train')\n",
    "                return counts\n",
    "\n",
    "            pos_counts = count(pos_chr)\n",
    "            neg_counts = count(neg_chr)\n",
    "            logging.info('Counted positive for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[pos_counts[x]for x in ['train','valid','test']]))\n",
    "            logging.info('Counted negative for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[neg_counts[x]for x in ['train','valid','test']]))\n",
    "            self.dataset2counts_pos = pos_counts\n",
    "            self.dataset2counts_neg = neg_counts\n",
    "\n",
    "            logging.info('Splitting samples into training/valid/test')\n",
    "\n",
    "            train_positive_samples, train_labels_positives, train_pos_chr = [], [], []\n",
    "            valid_positive_samples, valid_labels_positives, valid_pos_chr = [], [], []\n",
    "            test_positive_samples, test_labels_positives, test_pos_chr = [], [], []\n",
    "            train_negative_samples, train_neg_chr = [], []\n",
    "            valid_negative_samples, valid_neg_chr = [], []\n",
    "            test_negative_samples, test_neg_chr = [], []\n",
    "\n",
    "            logging.info('Preparing for positive training/valid/test')\n",
    "            for sample, label, c in zip(positive_samples, labels_positives, pos_chr):\n",
    "                chromosome, (start, stop) = c\n",
    "                if chromosome in chr_valid_set:\n",
    "                    valid_positive_samples.append(sample)\n",
    "                    valid_labels_positives.append(label)\n",
    "                    valid_pos_chr.append(c)\n",
    "                elif chromosome in chr_test_set:\n",
    "                    test_positive_samples.append(sample)\n",
    "                    test_labels_positives.append(label)\n",
    "                    test_pos_chr.append(c)\n",
    "                else: # in training set\n",
    "                    train_positive_samples.append(sample)\n",
    "                    train_labels_positives.append(label)\n",
    "                    train_pos_chr.append(c)\n",
    "            logging.info('Preparing for negative training/valid/test')\n",
    "            for sample, c in zip(tqdm(negative_samples), neg_chr):\n",
    "                chromosome, (start, stop) = c\n",
    "                if chromosome in chr_valid_set:\n",
    "                    valid_negative_samples.append(sample)\n",
    "                    valid_neg_chr.append(c)\n",
    "                elif chromosome in chr_test_set:\n",
    "                    test_negative_samples.append(sample)\n",
    "                    test_neg_chr.append(c)\n",
    "                else: # in training set\n",
    "                    train_negative_samples.append(sample)\n",
    "                    train_neg_chr.append(c)\n",
    "\n",
    "            if self.use_pickler:\n",
    "                logging.info('Setting up pickler to intercept used data')\n",
    "                label_dataset = PickleManager('{}_label_dataset.pkl'.format(self.exp_id), overwrite=True)\n",
    "\n",
    "            import numpy as np\n",
    "            # start with positive samples\n",
    "            onehot = {'A':(1.,0.,0.,0.),\n",
    "                      'C':(0.,1.,0.,0.),\n",
    "                      'G':(0.,0.,1.,0.),\n",
    "                      'T':(0.,0.,0.,1.),}\n",
    "\n",
    "            logging.info('Generating feature generators')\n",
    "            # we want one for each pos/neg and train/valid/test\n",
    "            # we want same number of pos/neg samples for training\n",
    "            sample_start, sample_stop = train_pos_chr[0][1]\n",
    "            sample_length = sample_stop-sample_start\n",
    "\n",
    "            logging.info('Samples are found to have length {}'.format(sample_length))\n",
    "            self.sample_length = sample_length\n",
    "            self.feature_dimensions = 6\n",
    "\n",
    "            logging.info('Generating static data for training')\n",
    "            n_pos_train = len(train_positive_samples) \n",
    "            if self.reduce_negative_samples:\n",
    "                train_negative_samples=train_negative_samples[:n_pos_train]\n",
    "                train_neg_chr=train_neg_chr[:n_pos_train]\n",
    "            n_neg_train = len(train_negative_samples)\n",
    "            # shuffle the pos/neg for training\n",
    "            # x, metadata, y, labels (for crf)\n",
    "            training_samples, training_chr, training_cls, training_labels = shuffle(train_positive_samples + train_negative_samples,\n",
    "                                                                                    train_pos_chr          + train_neg_chr,\n",
    "                                                                                    [1]*n_pos_train        + [0]*n_neg_train,\n",
    "                                                                                    train_labels_positives + ['O'*sample_length]*n_neg_train)\n",
    "            \n",
    "            self.dataset2counts_pos['train'] = n_pos_train\n",
    "            self.dataset2counts_neg['train'] = n_neg_train\n",
    "            self.training_length = n_pos_train*2\n",
    "            logging.info('Counted positive for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[pos_counts[x]for x in ['train','valid','test']]))\n",
    "            logging.info('Counted negative for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[neg_counts[x]for x in ['train','valid','test']]))\n",
    "\n",
    "            logging.info('Generating static data for validation/test')\n",
    "            # take all negative samples for valid/test instead of limiting it like in training\n",
    "            if balance_valid_ratio:\n",
    "                n_pos_valid = len(valid_positive_samples)\n",
    "                valid_negative_samples = valid_negative_samples[:n_pos_valid*balance_valid_ratio]\n",
    "                valid_neg_chr = valid_neg_chr[:n_pos_valid*balance_valid_ratio]\n",
    "            self.valid_length = len(valid_positive_samples) + len(valid_negative_samples)\n",
    "            self.test_length = len(test_positive_samples) + len(test_negative_samples)\n",
    "\n",
    "            valid_samples, valid_chr, valid_cls, valid_labels = shuffle(valid_positive_samples          + valid_negative_samples,\n",
    "                                                                        valid_pos_chr                   + valid_neg_chr,\n",
    "                                                                        [1]*len(valid_positive_samples) + [0]*len(valid_negative_samples),\n",
    "                                                                        valid_labels_positives          + ['O'*sample_length]*len(valid_negative_samples))\n",
    "\n",
    "            test_samples, test_chr, test_cls, test_labels = shuffle(test_positive_samples          + test_negative_samples,\n",
    "                                                                    test_pos_chr                   + test_neg_chr,\n",
    "                                                                    [1]*len(test_positive_samples) + [0]*len(test_negative_samples),\n",
    "                                                                    test_labels_positives          + ['O'*sample_length]*len(test_negative_samples))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            logging.info('Saving static data')\n",
    "            with open(dataset_pkl, 'wb') as pkl:\n",
    "                datalst= [train_positive_samples, train_negative_samples, train_labels_positives,\n",
    "                          train_pos_chr, train_neg_chr,\n",
    "                          training_cls, training_labels, \n",
    "                           valid_samples, valid_chr, valid_cls, valid_labels, \n",
    "                           test_samples, test_chr, test_cls, test_labels,\n",
    "                          self.dataset2counts_pos, self.dataset2counts_neg, \n",
    "                          self.training_length, self.valid_length, self.test_length, self.sample_length,\n",
    "                          self.feature_dimensions]\n",
    "                for thing in tqdm(datalst):\n",
    "                    pickle.dump(thing, pkl)\n",
    "            \n",
    "        logging.info('Creating generators')\n",
    "    \n",
    "        onehot = {'A':(1.,0.,0.,0.),\n",
    "                  'C':(0.,1.,0.,0.),\n",
    "                  'G':(0.,0.,1.,0.),\n",
    "                  'T':(0.,0.,0.,1.),}\n",
    "        def features_gen(samples, chrs):\n",
    "            def batch_features_generator(batch_size):\n",
    "                max_k = len(samples) // batch_size\n",
    "                k = 0\n",
    "                while True:\n",
    "                    batch = []\n",
    "                    \n",
    "                    for i in range(batch_size):\n",
    "                        seq, features = samples[i+k*batch_size], chrs[i+k*batch_size]\n",
    "                        chromosome, domain = features\n",
    "                        start, stop = domain\n",
    "                        # this part is potentially very slow #\n",
    "                        uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                        openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                        uniqueness[np.isnan(uniqueness)] = 0\n",
    "                        openness[np.isnan(openness)] = 0\n",
    "                        # # # # # # # # # # # # # # # # # # # \n",
    "                        features = []\n",
    "                        for i in range((stop-start)):\n",
    "                            feature = []\n",
    "                            feature.extend( onehot[seq[i]] )\n",
    "                            feature.append( openness[i] )\n",
    "                            feature.append( uniqueness[i] )\n",
    "                            features.append(feature)\n",
    "                        batch.append(np.array(features))\n",
    "                    yield np.array(batch)\n",
    "                    # cycle for infinite generator\n",
    "                    k+=1\n",
    "                    if k==max_k:\n",
    "                        k=0\n",
    "            return batch_features_generator\n",
    "        \n",
    "        logging.info('Creating feature generators')\n",
    "#         train_x = features_gen(train_positive_samples + train_negative_samples, training_chr)\n",
    "        valid_x = features_gen(valid_samples, valid_chr)\n",
    "        test_x = features_gen(test_samples, test_chr)\n",
    "        \n",
    "        @func_metrics_display\n",
    "        def values_gen(items, dtype=np.object):\n",
    "            def batch_generator(batch_size):\n",
    "                max_k = len(items) // batch_size\n",
    "                k = 0\n",
    "                while True:\n",
    "                    result = [items[i+k*batch_size] for i in range(batch_size)]\n",
    "                    yield np.array(result, dtype=dtype)\n",
    "                    # cycle for infinite generator\n",
    "                    k+=1\n",
    "                    if k==max_k:\n",
    "                        k=0\n",
    "            return batch_generator\n",
    "        \n",
    "        logging.info('Calculating sample weights')\n",
    "        def sample_weights(c, pos_count, neg_count):\n",
    "            neg_weight = pos_count/neg_count\n",
    "            return (1-c)*neg_weight + c        \n",
    "        \n",
    "        logging.info('Creating features sequencers')\n",
    "        train_labels_sequencer = lambda batch_size: TrainingFeaturesSequence((train_positive_samples, train_negative_samples), \n",
    "                                                                             (train_pos_chr, train_neg_chr), \n",
    "                                                                             (train_labels_positives, None), \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        train_cls_sequencer = lambda batch_size: TrainingFeaturesSequence((train_positive_samples, train_negative_samples), \n",
    "                                                                          (train_pos_chr, train_neg_chr), \n",
    "                                                                          ([1]*self.dataset2counts_pos['train'] , None), \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        valid_labels_sequencer = lambda batch_size: FeaturesSequence(valid_samples, valid_chr, valid_labels, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        valid_cls_sequencer = lambda batch_size: FeaturesSequence(valid_samples, valid_chr, valid_cls, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        test_labels_sequencer = lambda batch_size: FeaturesSequence(test_samples, test_chr, test_labels, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        test_cls_sequencer = lambda batch_size: FeaturesSequence(test_samples, test_chr, test_cls, \n",
    "                                                                    batch_size, \n",
    "                                                                    self.sample_length, self.feature_dimensions, \n",
    "                                                                    self.duke_unique, self.dnase)\n",
    "        \n",
    "        \n",
    "        logging.info('Creating value generators')\n",
    "        \n",
    "#         train_label = values_gen(training_labels)\n",
    "        valid_label = values_gen(valid_labels)\n",
    "        test_label = values_gen(test_labels)\n",
    "        \n",
    "#         train_seq = values_gen(train_pos_chr+train_neg_chr)\n",
    "        valid_seq = values_gen(valid_chr)\n",
    "        test_seq = values_gen(test_chr)\n",
    "        \n",
    "#         train_y = values_gen(training_cls, int)\n",
    "        valid_y = values_gen(valid_cls, int)\n",
    "        test_y = values_gen(test_cls, int)\n",
    "\n",
    "        return  (\n",
    "#                 train_x, train_label, train_seq, train_y, \n",
    "                 train_labels_sequencer, train_cls_sequencer,\n",
    "                 valid_x, valid_label, valid_seq, valid_y, valid_labels_sequencer, valid_cls_sequencer, \n",
    "                 test_x, test_label, test_seq, test_y, test_labels_sequencer, test_cls_sequencer)\n",
    "        \n",
    "    def load_data(self, *names):\n",
    "        label_dataset_output_dir = os.path.join(self.output_dir, self.exp_id+'_label_dataset')  \n",
    "        paths = [os.path.join(label_dataset_output_dir, name+'.npy') for name in names]\n",
    "        return [np.load(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class FeaturesSequence(keras.utils.Sequence):\n",
    "    def __init__(self, x_sequence, x_meta, y_set, \n",
    "                         batch_size, sample_length, feature_dimensions, \n",
    "                         duke_unique, dnase):\n",
    "\n",
    "        assert len(x_sequence) == len(x_meta) == len(y_set)\n",
    "        self.x_sequence, self.x_meta, self.y = x_sequence, x_meta, y_set\n",
    "        self.y_are_labels = type(y_set[0])==str\n",
    "        logging.info('Found y are labels; using OBIE')\n",
    "        if self.y_are_labels:\n",
    "            self.label2onehot = {'O':np.array([1,0,0,0]),'B':np.array([0,1,0,0]),'I':np.array([0.,0.,1.,0.]),'E':np.array([0.,0.,0.,1.]),}\n",
    "        self.sample_length=sample_length\n",
    "        self.n_samples = len(self.x_meta)\n",
    "        self.feature_dimensions=feature_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.onehot = {'A':np.array([1,0,0,0]),'C':np.array([0,1,0,0]),'G':np.array([0.,0.,1.,0.]),'T':np.array([0.,0.,0.,1.]),}\n",
    "        self.duke_unique = duke_unique\n",
    "        self.dnase = dnase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_samples / float(self.batch_size))) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # setup y\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            if self.y_are_labels:\n",
    "                labels = self.y[(i+idx*self.batch_size)%self.n_samples]\n",
    "                labels_onehot = np.empty((len(labels), 4))\n",
    "                for j in range(len(labels)):\n",
    "                    labels_onehot[j, :] = self.label2onehot[labels[j]]\n",
    "                batch_y.append(labels_onehot)\n",
    "            else:\n",
    "                batch_y.append(self.y[(i+idx*self.batch_size)%self.n_samples])\n",
    "\n",
    "        # setup x\n",
    "        batch_x = []\n",
    "        for i in range(self.batch_size):\n",
    "            seq, (chromosome, (start,stop)) = self.x_sequence[(i+idx*self.batch_size)%self.n_samples], self.x_meta[(i+idx*self.batch_size)%self.n_samples]\n",
    "            # this part is potentially very slow #\n",
    "            uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "            openness = np.array(self.dnase(chromosome, start, stop))\n",
    "            uniqueness[np.isnan(uniqueness)] = 0\n",
    "            openness[np.isnan(openness)] = 0\n",
    "            # # # # # # # # # # # # # # # # # # #\n",
    "            features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "            for j in range(self.sample_length):\n",
    "                features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "            batch_x.append(features)\n",
    "            \n",
    "        # setup sample weights\n",
    "#         batch_weight = []\n",
    "#         for i in range(self.batch_size):\n",
    "#             batch_weight.append(self.sample_weights[(i+idx*self.batch_size)%self.n_samples])\n",
    "        return np.array(batch_x), np.array(batch_y) #, np.array(batch_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class TrainingFeaturesSequence(keras.utils.Sequence):\n",
    "    def __init__(self, x_sequence, x_meta, y_set, \n",
    "                         batch_size, sample_length, feature_dimensions, \n",
    "                         duke_unique, dnase, sample_weights=None):\n",
    "\n",
    "        assert len(x_sequence) == len(x_meta) == len(y_set)\n",
    "        self.x_sequence_pos, self.x_meta_pos, self.y_pos = x_sequence[0], x_meta[0], y_set[0]\n",
    "        self.x_sequence_neg, self.x_meta_neg = x_sequence[1], x_meta[1]\n",
    "        self.y_are_labels = type(y_set[0][0])==str\n",
    "        logging.info('Found y are labels; using OBIE')\n",
    "        if self.y_are_labels:\n",
    "            self.label2onehot = {'O':np.array([1,0,0,0]),'B':np.array([0,1,0,0]),'I':np.array([0.,0.,1.,0.]),'E':np.array([0.,0.,0.,1.]),}\n",
    "        self.sample_length=sample_length\n",
    "        self.n_samples_pos, self.n_samples_neg = len(self.x_meta_pos), len(self.x_meta_neg)\n",
    "        self.feature_dimensions=feature_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.onehot = {'A':np.array([1,0,0,0]),'C':np.array([0,1,0,0]),'G':np.array([0.,0.,1.,0.]),'T':np.array([0.,0.,0.,1.]),}\n",
    "        self.duke_unique = duke_unique\n",
    "        self.dnase = dnase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_samples_pos / float(self.batch_size))) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # setup y\n",
    "        half_batch_size = int(self.batch_size/2)\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            if i<half_batch_size:\n",
    "                # work on positive samples\n",
    "                if self.y_are_labels:\n",
    "                    labels = self.y_pos[(i+idx*half_batch_size)%self.n_samples_pos]\n",
    "                    labels_onehot = np.empty((len(labels), 4))\n",
    "                    for j in range(len(labels)):\n",
    "                        labels_onehot[j, :] = self.label2onehot[labels[j]]\n",
    "                    batch_y.append(labels_onehot)\n",
    "                else:\n",
    "                    batch_y.append(1)\n",
    "            else:\n",
    "                if self.y_are_labels:\n",
    "#                     labels = self.y_neg[(i+idx*self.batch_size/2)%self.n_samples_neg]\n",
    "                    labels_onehot = np.empty((self.sample_length, 4))\n",
    "                    for j in range(self.sample_length):\n",
    "                        labels_onehot[j, :] = self.label2onehot['O']\n",
    "                    batch_y.append(labels_onehot)\n",
    "                else:\n",
    "                    batch_y.append(0)\n",
    "\n",
    "        # setup x\n",
    "        batch_x = []\n",
    "        for i in range(self.batch_size):\n",
    "            if i<half_batch_size:\n",
    "                seq, (chromosome, (start,stop)) = self.x_sequence_pos[(i+idx*half_batch_size)%self.n_samples_pos], self.x_meta_pos[(i+idx*half_batch_size)%self.n_samples_pos]\n",
    "                # this part is potentially very slow #\n",
    "                uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                uniqueness[np.isnan(uniqueness)] = 0\n",
    "                openness[np.isnan(openness)] = 0\n",
    "                # # # # # # # # # # # # # # # # # # #\n",
    "                features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "                for j in range(self.sample_length):\n",
    "                    features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "                batch_x.append(features)\n",
    "            else:\n",
    "                seq, (chromosome, (start,stop)) = self.x_sequence_neg[(i+idx*half_batch_size-half_batch_size)%self.n_samples_neg], self.x_meta_neg[(i+idx*half_batch_size-half_batch_size)%self.n_samples_neg]\n",
    "                # this part is potentially very slow #\n",
    "                uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                uniqueness[np.isnan(uniqueness)] = 0\n",
    "                openness[np.isnan(openness)] = 0\n",
    "                # # # # # # # # # # # # # # # # # # #\n",
    "                features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "                for j in range(self.sample_length):\n",
    "                    features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "                batch_x.append(features)\n",
    "            \n",
    "        # setup sample weights\n",
    "#         batch_weight = []\n",
    "#         for i in range(self.batch_size):\n",
    "#             batch_weight.append(self.sample_weights[(i+idx*self.batch_size)%self.n_samples])\n",
    "        return np.array(batch_x), np.array(batch_y) #,np.array(batch_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in CTCF_outputs\n",
      ">Looking in sub output models\\CTCF_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in CTCF_bigru_crf_model_outputs, best checkpoint CTCF_bigru_crf-0004-0.95.hdf5\n",
      "DEBUG:root:Found model in CTCF_deepbind_model_outputs, best checkpoint CTCF_deepbind-0013-0.91.hdf5\n",
      "DEBUG:root:Found model in CTCF_deepbind_plus_model_outputs, best checkpoint CTCF_deepbind_plus-0005-0.92.hdf5\n",
      "DEBUG:root:Found model in CTCF_factor_net_model_outputs, best checkpoint CTCF_factor_net-0041-0.96.hdf5\n",
      "DEBUG:root:Found model in CTCF_s_bigru_crf_model_outputs, best checkpoint CTCF_s_bigru_crf-0005-0.93.hdf5\n",
      "DEBUG:root:Found model in CTCF_s_bilstm_crf_model_outputs, best checkpoint CTCF_s_bilstm_crf-0055-0.93.hdf5\n",
      "DEBUG:root:Found model in CTCF_s_cnn_bigru_crf_model_outputs, best checkpoint CTCF_s_cnn_bigru_crf-0067-0.96.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in E2F1_outputs\n",
      ">Looking in sub output models\\E2F1_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in E2F1_bigru_crf_model_outputs, best checkpoint E2F1_bigru_crf-0011-0.97.hdf5\n",
      "DEBUG:root:Found model in E2F1_factor_net_model_outputs, best checkpoint E2F1_factor_net-0021-0.96.hdf5\n",
      "DEBUG:root:Found model in E2F1_s_bigru_crf_model_outputs, best checkpoint E2F1_s_bigru_crf-0200-0.96.hdf5\n",
      "DEBUG:root:Found model in E2F1_s_bilstm_crf_model_outputs, best checkpoint E2F1_s_bilstm_crf-0002-0.96.hdf5\n",
      "DEBUG:root:Found model in E2F1_s_cnn_bigru_crf_model_outputs, best checkpoint E2F1_s_cnn_bigru_crf-0002-0.08.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in EGR1_outputs\n",
      ">Looking in sub output models\\EGR1_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in EGR1_bigru_crf_model_outputs, best checkpoint EGR1_bigru_crf-0009-0.97.hdf5\n",
      "DEBUG:root:Found model in EGR1_s_bilstm_crf_model_outputs, best checkpoint EGR1_s_bilstm_crf-0015-0.96.hdf5\n",
      "DEBUG:root:Found model in EGR1_s_cnn_bigru_crf_model_outputs, best checkpoint EGR1_s_cnn_bigru_crf-0002-0.74.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in FOXA1_outputs\n",
      ">Looking in sub output models\\FOXA1_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in FOXA1_bigru_crf_model_outputs, best checkpoint FOXA1_bigru_crf-0048-0.95.hdf5\n",
      "DEBUG:root:Found model in FOXA1_deepbind_model_outputs, best checkpoint FOXA1_deepbind-0009-0.91.hdf5\n",
      "DEBUG:root:Found model in FOXA1_deepbind_plus_model_outputs, best checkpoint FOXA1_deepbind_plus-0005-0.92.hdf5\n",
      "DEBUG:root:Found model in FOXA1_factor_net_model_outputs, best checkpoint FOXA1_factor_net-0001-0.94.hdf5\n",
      "DEBUG:root:Found model in FOXA1_s_bigru_crf_model_outputs, best checkpoint FOXA1_s_bigru_crf-0018-0.95.hdf5\n",
      "DEBUG:root:Found model in FOXA1_s_bilstm_crf_model_outputs, best checkpoint FOXA1_s_bilstm_crf-0008-0.95.hdf5\n",
      "DEBUG:root:Found model in FOXA1_s_cnn_bigru_crf_model_outputs, best checkpoint FOXA1_s_cnn_bigru_crf-0002-0.94.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in FOXA2_outputs\n",
      ">Looking in sub output models\\FOXA2_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in FOXA2_bigru_crf_model_outputs, best checkpoint FOXA2_bigru_crf-0043-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in GABPA_outputs\n",
      ">Looking in sub output models\\GABPA_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in GABPA_bigru_crf_model_outputs, best checkpoint GABPA_bigru_crf-0005-0.97.hdf5\n",
      "DEBUG:root:Found model in GABPA_factor_net_model_outputs, best checkpoint GABPA_factor_net-0013-0.95.hdf5\n",
      "DEBUG:root:Found model in GABPA_s_bigru_crf_model_outputs, best checkpoint GABPA_s_bigru_crf-0025-0.94.hdf5\n",
      "DEBUG:root:Found model in GABPA_s_bilstm_crf_model_outputs, best checkpoint GABPA_s_bilstm_crf-0004-0.97.hdf5\n",
      "DEBUG:root:Found model in GABPA_s_cnn_bigru_crf_model_outputs, best checkpoint GABPA_s_cnn_bigru_crf-0040-0.93.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in HNF4A_outputs\n",
      ">Looking in sub output models\\HNF4A_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in HNF4A_bigru_crf_model_outputs, best checkpoint HNF4A_bigru_crf-0064-0.95.hdf5\n",
      "DEBUG:root:Found model in HNF4A_deepbind_model_outputs, best checkpoint HNF4A_deepbind-0009-0.92.hdf5\n",
      "DEBUG:root:Found model in HNF4A_deepbind_plus_model_outputs, best checkpoint HNF4A_deepbind_plus-0020-0.93.hdf5\n",
      "DEBUG:root:Found model in HNF4A_factor_net_model_outputs, best checkpoint HNF4A_factor_net-0015-0.94.hdf5\n",
      "DEBUG:root:Found model in HNF4A_s_bigru_crf_model_outputs, best checkpoint HNF4A_s_bigru_crf-0039-0.95.hdf5\n",
      "DEBUG:root:Found model in HNF4A_s_bilstm_crf_model_outputs, best checkpoint HNF4A_s_bilstm_crf-0042-0.95.hdf5\n",
      "DEBUG:root:Found model in HNF4A_s_cnn_bigru_crf_model_outputs, best checkpoint HNF4A_s_cnn_bigru_crf-0032-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in JUND_outputs\n",
      ">Looking in sub output models\\JUND_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in JUND_bigru_crf_model_outputs, best checkpoint JUND_bigru_crf-0019-0.96.hdf5\n",
      "DEBUG:root:Found model in JUND_bigru_crf_model_outputs - Copy, best checkpoint JUND_bigru_crf-0007-0.96.hdf5\n",
      "DEBUG:root:Found model in JUND_factor_net_model_outputs, best checkpoint JUND_factor_net-0007-0.95.hdf5\n",
      "DEBUG:root:Found model in JUND_s_bigru_crf_model_outputs, best checkpoint JUND_s_bigru_crf-0200-0.96.hdf5\n",
      "DEBUG:root:Found model in JUND_s_bigru_crf_model_outputs - Copy, best checkpoint JUND_s_bigru_crf-0013-0.96.hdf5\n",
      "DEBUG:root:Found model in JUND_s_bilstm_crf_model_outputs, best checkpoint JUND_s_bilstm_crf-0005-0.96.hdf5\n",
      "DEBUG:root:Found model in JUND_s_cnn_bigru_crf_model_outputs, best checkpoint JUND_s_cnn_bigru_crf-0001-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in MAX_outputs\n",
      ">Looking in sub output models\\MAX_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in MAX_bigru_crf_model_outputs, best checkpoint MAX_bigru_crf-0035-0.96.hdf5\n",
      "DEBUG:root:Found model in MAX_deepbind_model_outputs, best checkpoint MAX_deepbind-0005-0.95.hdf5\n",
      "DEBUG:root:Found model in MAX_deepbind_plus_model_outputs, best checkpoint MAX_deepbind_plus-0011-0.95.hdf5\n",
      "DEBUG:root:Found model in MAX_factor_net_model_outputs, best checkpoint MAX_factor_net-0002-0.95.hdf5\n",
      "DEBUG:root:Found model in MAX_s_bigru_crf_model_outputs, best checkpoint MAX_s_bigru_crf-0019-0.96.hdf5\n",
      "DEBUG:root:Found model in MAX_s_cnn_bigru_crf_model_outputs, best checkpoint MAX_s_cnn_bigru_crf-0001-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in REST_outputs\n",
      ">Looking in sub output models\\REST_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in REST_bigru_crf_model_outputs, best checkpoint REST_bigru_crf-0036-0.96.hdf5\n",
      "DEBUG:root:Found model in REST_factor_net_model_outputs, best checkpoint REST_factor_net-0001-0.94.hdf5\n",
      "DEBUG:root:Found model in REST_s_bigru_crf_model_outputs, best checkpoint REST_s_bigru_crf-0030-0.95.hdf5\n",
      "DEBUG:root:Found model in REST_s_bilstm_crf_model_outputs, best checkpoint REST_s_bilstm_crf-0045-0.96.hdf5\n",
      "DEBUG:root:Found model in REST_s_cnn_bigru_crf_model_outputs, best checkpoint REST_s_cnn_bigru_crf-0001-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in TAF1_outputs\n",
      ">Looking in sub output models\\TAF1_outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Found model in TAF1_bigru_crf_model_outputs, best checkpoint TAF1_bigru_crf-0029-0.96.hdf5\n",
      "DEBUG:root:Found model in TAF1_factor_net_model_outputs, best checkpoint TAF1_factor_net-0001-0.96.hdf5\n",
      "DEBUG:root:Found model in TAF1_s_bigru_crf_model_outputs, best checkpoint TAF1_s_bigru_crf-0018-0.96.hdf5\n",
      "DEBUG:root:Found model in TAF1_s_cnn_bigru_crf_model_outputs, best checkpoint TAF1_s_cnn_bigru_crf-0002-0.95.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_paths = []\n",
    "for d in os.listdir('output models/'):\n",
    "    print('Looking in '+d)\n",
    "    subdir = os.path.join('output models',d)\n",
    "    if os.path.isdir(subdir):\n",
    "        print('>Looking in sub '+subdir)\n",
    "        for model_d in os.listdir(subdir):\n",
    "            path = os.path.join(subdir,model_d)\n",
    "            tokens = model_d.split('_')\n",
    "            tf = tokens[0]\n",
    "            model_type = '_'.join(tokens[1:-2])\n",
    "            \n",
    "            saved_points = list(filter(lambda f: f.endswith('hdf5'),os.listdir(path)))\n",
    "            x = [pt.split('-') for pt in saved_points]\n",
    "            x = sorted(saved_points, key= lambda pt : pt.split('-')[1], reverse=True)\n",
    "            try:\n",
    "                logging.debug(\"Found model in %s, best checkpoint %s\"%(model_d, x[0]))\n",
    "                model_paths.append(os.path.join('output models', d, model_d, x[0]))\n",
    "            except:\n",
    "                pass\n",
    "    else:\n",
    "        print('> '+d+' is not a directory')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def create_binary_model(model_path):\n",
    "    model = load_model(model_paths[0], custom_objects={'ReverseComplementLayer': ReverseComplementLayer, \n",
    "                                                             \"ClassWrapper\": ClassWrapper ,\n",
    "                                                             \"CRF_ext\": ClassWrapper, \"loss\": loss, \"accuracy\":accuracy,\n",
    "                                                             \"viterbi_precision\":viterbi_precision, \"f1\":f1,\n",
    "                                                             \"recall\":recall, \"precision\":precision})\n",
    "    model.trainable = False\n",
    "    x = model.output\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    layered_model = Model(inputs=model.input, outputs=x)\n",
    "    layered_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return layered_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = list(filter(lambda p: 'bigru' in p and 'TAF1' in p, model_paths))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output models\\\\TAF1_outputs\\\\TAF1_bigru_crf_model_outputs\\\\TAF1_bigru_crf-0029-0.96.hdf5'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = model_path[len('output models\\\\'):].split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = '_'.join(model_path.split('\\\\')[2].split('_')[1:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = '%s._.labels.tsv' % (tf.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "peakfile_regex = re.compile(r'chipseq\\.\\w+\\.%s\\..*\\.\\_\\.narrowpeak'%tf, re.IGNORECASE)\n",
    "peakfile = list(filter(lambda f: peakfile_regex.match(f), os.listdir('.')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_file='total_regions.blacklistfiltered.merged.bed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_negative_samples=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct datamanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auROC(labels, predictions):\n",
    "    return roc_auc_score(labels, predictions)\n",
    "\n",
    "def auPRC(labels, predictions):\n",
    "    precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "    return auc(recall, precision)\n",
    "\n",
    "def recall_at_precision(labels, predictions, precision_at):\n",
    "    threshold = 1.0-precision_at\n",
    "    precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "    return 100 * recall[np.searchsorted(precision - threshold, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Working on chipseq.liver.taf1.conservative._.narrowpeak\n",
      "INFO:root:Loading positive/negative samples from disk\n",
      "INFO:root:Done loading samples from disk\n",
      "INFO:root:Reading static data\n",
      "INFO:root:Loaded training/valid/test of sizes 209314/65140/7902816\n",
      "INFO:root:Creating generators\n",
      "INFO:root:Creating feature generators\n",
      "INFO:root:Calculating sample weights\n",
      "INFO:root:Creating features sequencers\n",
      "INFO:root:Creating value generators\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:Found y are labels; using OBIE\n",
      "INFO:root:Found y are labels; using OBIE\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-af0b37216a95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m                                          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_cls_sequencer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m                                          \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m                                          verbose=1)\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbin_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mtest_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[0;32m    496\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 497\u001b[1;33m                         loss=self.total_loss)\n\u001b[0m\u001b[0;32m    498\u001b[0m                 updates = (self.updates +\n\u001b[0;32m    499\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\WinPython\\python-3.5.4.amd64\\lib\\site-packages\\keras\\optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[1;34m(self, loss, params)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             raise ValueError('An operation has `None` for gradient. '\n\u001b[0m\u001b[0;32m     81\u001b[0m                              \u001b[1;34m'Please make sure that all of your ops have a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                              \u001b[1;34m'gradient defined (i.e. are differentiable). '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval."
     ]
    }
   ],
   "source": [
    "binary_model = None\n",
    "if peakfile and os.path.exists(label_file):\n",
    "    \n",
    "    logging.info('Working on %s'%peakfile)\n",
    "    dm = DataManager(label_file, peakfile, output_dir='datasets', \n",
    "                        move_finished_src='finished_peakfiles', \n",
    "                        filter_file=filter_file,\n",
    "                        reduce_negative_samples=reduce_negative_samples, # check_set_ratio=9, # use for reduce_negative_samples=True\n",
    "                        )\n",
    "    dm.samples # make sure samples are already created\n",
    "    generators = dm.create_datagen_from_samples(useDNAshapeR=False)\n",
    "    train_labels_sequencer, train_cls_sequencer = generators[:2]\n",
    "    valid_x, valid_label, valid_seq, valid_cls, valid_labels_sequencer, valid_cls_sequencer = generators[2:8]\n",
    "    test_x, test_label, test_seq, test_cls, test_labels_sequencer, test_cls_sequencer = generators[8:]\n",
    "    \n",
    "    binary_model = create_binary_model(model_path)\n",
    "    batch_n, epoch_n = 512, 200\n",
    "    train_len = dm.dataset2counts_pos['train'] * 2\n",
    "    test_len = dm.dataset2counts_pos['test'] + dm.dataset2counts_neg['test']\n",
    "\n",
    "    train_steps = train_len//batch_n\n",
    "    validation_steps = int((train_len/4)//batch_n)\n",
    "    test_steps = test_len//batch_n\n",
    "\n",
    "    train_generator = train_cls_sequencer(batch_n)\n",
    "    validation_generator = valid_cls_sequencer(batch_n)\n",
    "\n",
    "    history = binary_model.fit_generator(train_cls_sequencer,\n",
    "                                         steps_per_epoch= train_steps,\n",
    "                                         epochs=epoch_n,\n",
    "                                         callbacks=[],\n",
    "                                         validation_data=valid_cls_sequencer,\n",
    "                                         validation_steps=validation_steps,\n",
    "                                         verbose=1)\n",
    "    test_pred = bin_model.predict_generator(test_x(batch_n), steps=test_steps, verbose=1)\n",
    "    test_pred = list(test_pred[:,0])\n",
    "    import itertools\n",
    "    test_real = [x for batch in itertools.islice(test_cls(batch_n),test_steps) for x in batch ]\n",
    "\n",
    "    # auROC,  auPRC, recalls\n",
    "    logging.info('Calculating auROC')\n",
    "    auroc = auROC(test_real, test_pred)\n",
    "    logging.info('auROC: {}'.format(auroc))\n",
    "    logging.info('Calculating auPRC')\n",
    "    auprc = auPRC(test_real, test_pred)\n",
    "    logging.info('auPRC: {}'.format(auprc))\n",
    "    logging.info('Calculating recall @ precisions')\n",
    "    re5, re10, re25, re50 = [recall_at_precision(test_real, test_pred, precision) for precision in [0.05,0.1,0.25,0.5]]\n",
    "    # confusion matrix \n",
    "    confusion = confusion_matrix(test_real, np.round(test_pred))\n",
    "\n",
    "    out = 'auROC: {} auPRC {} re@5/10/25/50: {:>7.5}/{:>7.5}/{:>7.5}/{:>7.5}'.format(auroc, auprc, re5, re10, re25, re50)\n",
    "    with open(tf+'_'+model_type+'_remodeled_output', 'w') as write_to:\n",
    "        write_to.write(out)\n",
    "else:\n",
    "    logging.info(peakfile + \" \" + str(os.path.exists(label_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
