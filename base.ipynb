{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    ">* import required modules \n",
    ">* setup required environment variables for logging and gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel('DEBUG')\n",
    "# config for server\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "os.environ['PATH'] = r'C:\\Users\\Rudolf\\Documents\\v9.0\\bin' + os.path.pathsep + os.environ['PATH'] \n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "num_CPU = 8\n",
    "num_GPU = 1\n",
    "config = tf.ConfigProto( device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import gc\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugObject(Exception):\n",
    "    def __init__(self, obj, message):\n",
    "        self.content = obj\n",
    "        self.message = message\n",
    "        Exception.__init__(self, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "def func_metrics_display(funk):\n",
    "    def metricized(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        obj = funk(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        dt = end - start\n",
    "        if dt > 1:\n",
    "            format_str = ('%H hours' if dt >= 3600 else '') + ('%M minutes' if dt >= 60 else '') + '%S seconds'\n",
    "            logging.info('The function {} took {}'.format(funk.__name__, \n",
    "                                                          time.strftime( format_str ,time.gmtime(dt)) ))\n",
    "        else:\n",
    "            logging.info('The function {} took {:>10.4f}s'.format(funk.__name__, dt))\n",
    "        return obj\n",
    "    metricized.__name__=funk.__name__\n",
    "    return metricized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def use_bw(bw_file):\n",
    "    import pyBigWig\n",
    "    import numpy as np\n",
    "    bigwig = pyBigWig.open(bw_file)\n",
    "    def get_values(chrom, start, stop):\n",
    "        temp = np.array(bigwig.values(chrom, start, stop))\n",
    "        temp[np.isnan(temp)] = 0\n",
    "        return temp\n",
    "    return get_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigwig_celline_file(cell_line):\n",
    "    return [f for f in os.listdir() if f.endswith('1x.bw') and f[:f.index('.')].lower()==cell_line.lower()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "from scipy.sparse import csc_matrix\n",
    "import tqdm\n",
    "@func_metrics_display\n",
    "def create_bwchrcsc(bw_file, base_filename='duke_unique'):\n",
    "    bigwig = pyBigWig.open(bw_file)\n",
    "    chroms = bigwig.chroms()    \n",
    "    ext = '.npz'\n",
    "    for chrom in tqdm.tqdm(chroms):\n",
    "        logging.info('Creating sparse matrix for {}'.format(chrom))\n",
    "        prefix = '{}_'.format(chrom)\n",
    "        filename = prefix+base_filename+ext\n",
    "        if not all_paths_exists(filename):\n",
    "            csc = csc_matrix(bigwig.values(chrom, 0, chroms[chrom]))\n",
    "            logging.info('Saving to {}'.format(filename))\n",
    "            scipy.sparse.save_npz(filename, csc)\n",
    "        else:\n",
    "            logging.info('Skipping {}; already exists'.format(chrom))\n",
    "    bigwig.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chrcsc(chromosome, base_filename):\n",
    "    prefix = '{}_'.format(chromosome)\n",
    "    ext = '.npz'\n",
    "    filename = prefix+base_filename+ext\n",
    "    csc = scipy.sparse.load_npz(filename)\n",
    "    return csc\n",
    "\n",
    "@func_metrics_display\n",
    "def gen_chr2csc(bw_filename, base_filename):\n",
    "    import re\n",
    "    import os\n",
    "    regex = re.compile(r'\\w+_%s.npz'%base_filename, re.IGNORECASE)\n",
    "    if not [file for file in os.listdir('.') if regex.match(file)]:\n",
    "        create_bwchrcsc(bw_filename, base_filename)\n",
    "    \n",
    "    chr2csc = {}\n",
    "    for file in os.listdir('.'):\n",
    "        if regex.match(file):\n",
    "            chrom = file.split('_')[0]\n",
    "            logging.info('Loading sparse for {}'.format(chrom))\n",
    "            chr2csc[chrom] = gen_chrcsc(chrom, base_filename)\n",
    "    return chr2csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "@func_metrics_display\n",
    "def create_h5py4bw(bw_filename, base_filename):\n",
    "    bigwig = pyBigWig.open(bw_file)\n",
    "    chroms = bigwig.chroms()   \n",
    "    ext = '.hdf5'\n",
    "    filename = base_filename+ext\n",
    "    h5 = h5py.File(filename, \"w\")\n",
    "    if not all_paths_exists(filename):\n",
    "        for chrom in tqdm.tqdm(chroms):\n",
    "            logging.info('Creating h5py entry for {}'.format(chrom))\n",
    "            data = bigwig.values(chrom, 0, chroms[chrom])\n",
    "            h5.create_dataset(chrom, data=data, compression='lzf', chunks=(400,))\n",
    "    else:\n",
    "        logging.info('Skipping {}; already exists'.format(chrom))\n",
    "    h5.close()\n",
    "    return filename\n",
    "\n",
    "def gen_h5py4bw(bw_filename, base_filename):\n",
    "    ext = '.hdf5'\n",
    "    filename = base_filename+ext\n",
    "    if not os.path.exists(filename):\n",
    "        create_h5py4bw(bw_filename, base_filename)\n",
    "    return h5py.File(filename, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsufficientChromosomesException(Exception):\n",
    "    def __init__(self, chrom_avail, chrom_req, msg=None):\n",
    "        self.chrom_avail = chrom_avail\n",
    "        self.chrom_req = chrom_req\n",
    "        if not msg:\n",
    "            msg = 'Found {} chromosomes. Needed more than {} chromosomes.'.format(chrom_avail, chrom_req)\n",
    "        super(InsufficientChromosomesException, self).__init__(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(a, domain):\n",
    "    return domain[0]<=a<=domain[1]\n",
    "\n",
    "def is_in_domain(peak, domain):\n",
    "    '''\n",
    "    Checks to see if peak (tuple of ints) overlaps with domain (tuple of ints) \n",
    "    '''\n",
    "    p_start, p_end = peak\n",
    "    d_start, d_end = domain\n",
    "    return is_in(p_start, domain) or is_in(p_end, domain) or (is_in(d_start, peak) or is_in(d_end, peak))\n",
    "\n",
    "def any_in_domain(peaks, domain):\n",
    "    for peak in peaks:\n",
    "        if is_in_domain(peak, domain):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def which_in_domain(peaks,domain):\n",
    "    for peak in peaks:\n",
    "        if is_in_domain(peak,domain):\n",
    "            return peak\n",
    "    return None\n",
    "\n",
    "def domain2seq(chromosomes, chromosome, domain):\n",
    "    return chromosomes[chromosome][domain[0]:domain[1]]\n",
    "\n",
    "def where_in_domain(chromosomes, chromosome, peak, domain):\n",
    "    peak_start, peak_end = peak\n",
    "    domain_start, domain_end = domain\n",
    "    \n",
    "    seq = domain2seq(chromosomes, chromosome, domain)\n",
    "\n",
    "    labels = []\n",
    "    for i in range(domain_end-domain_start):\n",
    "        dna_location_ptr = domain_start+i\n",
    "        if dna_location_ptr<peak_start:\n",
    "            labels.append('O')\n",
    "        elif dna_location_ptr==peak_start:\n",
    "            labels.append('B')\n",
    "        elif peak_start<dna_location_ptr<peak_end:\n",
    "            labels.append('I')\n",
    "        elif dna_location_ptr==peak_end:\n",
    "            labels.append('E')\n",
    "        else:\n",
    "            labels.append('O')\n",
    "    return ''.join(labels), (chromosome, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def if_not_pickled(pkl_file, generator,log=True,gen_name=None):\n",
    "    try:\n",
    "        name = gen_name if gen_name else generator.__name__\n",
    "    except AttributeError:\n",
    "        name = generator.func.__name__\n",
    "        \n",
    "    def generate_pkl():\n",
    "        obj = generator()\n",
    "        with open(pkl_file, 'wb') as pkl:\n",
    "            pickle.dump(obj, pkl)\n",
    "        return obj\n",
    "    \n",
    "    if not os.path.exists(pkl_file):\n",
    "        if log:\n",
    "            logging.info('No pickle for {} is found. Generating anew.'.format(name))               \n",
    "        obj = generate_pkl()\n",
    "    else:\n",
    "        if log:\n",
    "            logging.info('Loading pickled {}'.format(name))\n",
    "        try:\n",
    "            with open(pkl_file, 'rb') as pkl:\n",
    "                obj = pickle.load(pkl)\n",
    "        except (EOFError,pickle.UnpicklingError) as e :\n",
    "            logging.error('A pickle file was corrupted: {}'.format(pkl_file))\n",
    "            if log:\n",
    "                logging.info('Regenerating corrupted pickle: {}'.format(pkl_file))\n",
    "            obj = generate_pkl()\n",
    "    if log:\n",
    "        logging.info('Finished setting up {}'.format(name))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2locNbound(label_file, _cellline, log=True):\n",
    "    from tqdm import tqdm\n",
    "    chr2locNbound = {}\n",
    "    with open(label_file) as labels:\n",
    "        line_gen = (line for line in labels)\n",
    "        column_names = next(line_gen).strip().split() \n",
    "        # columns names are chr start stop <cell line 1> ... <cell line n>\n",
    "        _, _, _, *celllines = column_names\n",
    "        prev_chrom = None\n",
    "        for line in tqdm(line_gen, total=get_num_lines(label_file)):\n",
    "            chromosome, start, stop, *bound_per_cellline = [x.strip() for x in line.strip().split()]\n",
    "            try:\n",
    "                start,stop = [int(x) for x in [start,stop]]\n",
    "            except ValueError:\n",
    "                # ill formatted entry\n",
    "                continue\n",
    "            if log and prev_chrom!=chromosome:\n",
    "                prev_chrom=chromosome\n",
    "                logging.info('Working on {}\\n'.format(chromosome))\n",
    "            for cellline, is_bound in zip(celllines, bound_per_cellline):\n",
    "                if cellline.lower() != _cellline.lower():\n",
    "                    continue\n",
    "                try:\n",
    "                    chr2locNbound[chromosome].append(((start,stop), is_bound))\n",
    "                except KeyError:\n",
    "                    chr2locNbound[chromosome] = [((start,stop), is_bound)]                \n",
    "    return chr2locNbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_hg19(hg_genome_fasta):\n",
    "    chromosomes = {}\n",
    "    with open(hg_genome_fasta) as hg19:\n",
    "        chromosome = None\n",
    "        for line in hg19:\n",
    "            if line.startswith('>'):\n",
    "                chromosome = line[1:].strip()\n",
    "                chromosomes[chromosome] = []\n",
    "            else: \n",
    "                chromosomes[chromosome].append(line.strip().upper())\n",
    "\n",
    "    for k,v in chromosomes.items():\n",
    "        chromosomes[k] = ''.join(v)   \n",
    "    return chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2filter_locs(filter_file):\n",
    "    chr2filter_locs = {}\n",
    "    with open(filter_file) as filter_f:\n",
    "        for line in filter_f:\n",
    "            chromosome, start, end = line.strip().split()\n",
    "            start, end = [int(x) for x in (start, end)]\n",
    "            try:\n",
    "                chr2filter_locs[chromosome].append((start,end))\n",
    "            except KeyError:\n",
    "                chr2filter_locs[chromosome] = [(start,end)]\n",
    "    return chr2filter_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_metrics_display\n",
    "def gen_chr2locNpeaks(celllineNtf_peakfile, filter_file=None, chr2filter_locs=None):\n",
    "    chr2locNpeaks = {}\n",
    "    from tqdm import tqdm\n",
    "    with open(celllineNtf_peakfile) as peaks:\n",
    "        for line in tqdm(peaks, total=get_num_lines(celllineNtf_peakfile)):\n",
    "            chromosome, start, stop, name, score, strand, signal, p, q, peak = line.strip().split()\n",
    "            start, stop = [int(x) for x in [start,stop]]\n",
    "            try:\n",
    "                chr2locNpeaks[chromosome]\n",
    "            except KeyError:\n",
    "                chr2locNpeaks[chromosome]=[]\n",
    "                \n",
    "            if filter_file:\n",
    "                if not chr2filter_locs:\n",
    "                    raise ValueError('Need chr2filter_locs if using filter_file')\n",
    "                filter_locs = chr2filter_locs[chromosome]\n",
    "                overlap=False\n",
    "                for loc in chr2filter_locs[chromosome]:\n",
    "                    if loc[0]<=start<=loc[1] or loc[0]<=stop<=loc[1]:\n",
    "                        overlap=True\n",
    "                        break\n",
    "                if not overlap:\n",
    "                    continue\n",
    "            chr2locNpeaks[chromosome].append((start,\n",
    "                                              stop,\n",
    "                                             {'name' : name,\n",
    "                                             'score': int(score),\n",
    "                                             'strand': strand,\n",
    "                                             'p-value':float(p),\n",
    "                                             'q-value':float(q),\n",
    "                                             'peak':int(peak)}))\n",
    "        for chromosome, lst in chr2locNpeaks.items():\n",
    "            chr2locNpeaks[chromosome] = sorted(lst,key= lambda x:x[0])\n",
    "    \n",
    "    return chr2locNpeaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2sequence(seq, chromosomes):\n",
    "    chromosome=seq[0]\n",
    "    domain=seq[1]\n",
    "    return domain2seq(chromosomes, chromosome, domain)\n",
    "\n",
    "@func_metrics_display\n",
    "def gen_chr2labelsNseq(chromosomes, chr2locNbound, chr2locNpeaks):\n",
    "    chr2labelsNseq = {}\n",
    "    from tqdm import tqdm\n",
    "    shared_chromosomes = set(chr2locNbound.keys()) & set(chr2locNpeaks.keys())\n",
    "    logging.info('Generating labels and seq for this set of chromosomes {}'.format(str(shared_chromosomes)))\n",
    "    for chromosome in tqdm(shared_chromosomes):\n",
    "        logging.info('Generating labels and seq for {}'.format(chromosome))\n",
    "        bound_locs = [x for x in chr2locNbound[chromosome] if x[1]=='B']\n",
    "        bound_locs = [loc[0] for loc in bound_locs]\n",
    "        peaks_locs = [x[:2] for x in chr2locNpeaks[chromosome]]\n",
    "        \n",
    "#         peak_offsets = [x[2]['peak'] for x in chr2locNpeaks[chromosome]]\n",
    "        count = 0\n",
    "        for bound_loc in bound_locs:\n",
    "            # generate peak information\n",
    "            the_peak = which_in_domain(peaks_locs, bound_loc)\n",
    "            if not the_peak:\n",
    "                logging.warning('Cannot find peak {}'.format(bound_loc))\n",
    "                continue\n",
    "            the_peak_i = peaks_locs.index(the_peak)\n",
    "            labels, seq = where_in_domain(chromosomes,\n",
    "                                          chromosome,\n",
    "                                          the_peak,\n",
    "                                          bound_loc)\n",
    "            if not ('B' in labels or 'I' in labels or 'E' in labels):\n",
    "                raise ValueError('BIE not found in labels\\nsequence:{}\\nlabels{}'.format(seq,labels))\n",
    "            try:\n",
    "                chr2labelsNseq[chromosome].append((labels, seq))\n",
    "            except KeyError:\n",
    "                chr2labelsNseq[chromosome] = [(labels, seq)]\n",
    "            count += 1\n",
    "        logging.info('Found {}/{} seqs for bound locations'.format(count, len(bound_locs)))        \n",
    "        unbound_locs = [x for x in chr2locNbound[chromosome] if x[1]=='U']\n",
    "        unbound_locs = [loc[0] for loc in unbound_locs]\n",
    "        count = 0\n",
    "        for unbound_loc in unbound_locs:\n",
    "            seq = (chromosome, unbound_loc)\n",
    "            try:\n",
    "                chr2labelsNseq[chromosome].append((None, seq))\n",
    "            except KeyError:\n",
    "                chr2labelsNseq[chromosome] = [(None, seq)]\n",
    "            count += 1\n",
    "        logging.info('Found {}/{} seqs for bound locations'.format(count, len(unbound_locs)))  \n",
    "    return chr2labelsNseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_structure_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = []\n",
    "        agg = []\n",
    "        for line in f:\n",
    "            if line.startswith('>'):\n",
    "                lines.append(','.join(agg))\n",
    "                agg = []\n",
    "            else:\n",
    "                agg.append(line.strip())\n",
    "        lines.append(','.join(agg))\n",
    "    del lines[0]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_paths_exist(*paths):\n",
    "    import os\n",
    "    return [os.path.exists(path) for path in paths]\n",
    "\n",
    "def all_paths_exists(*paths):\n",
    "    from functools import reduce\n",
    "    return reduce(lambda acc,x: acc and x ,map_paths_exist(*paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmem_manage(rfunc):\n",
    "    from rpy2 import robjects\n",
    "    import gc\n",
    "    def rmem_func(*args,**kwargs):\n",
    "        gc.collect()\n",
    "        result = rfunc(*args,**kwargs)\n",
    "        robjects.r('rm(list = ls(all.names=TRUE))')\n",
    "        gc.collect()\n",
    "        return result\n",
    "    return rmem_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PickleManager\n",
    "module used for using list like file writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "class PickleManager:\n",
    "    def __init__(self, pkl_file_path, overwrite=False):\n",
    "        self.file_path = pkl_file_path\n",
    "        self.file_writer_ptr = None\n",
    "        self._length = 0\n",
    "        \n",
    "        self.overwrite=overwrite\n",
    "        \n",
    "    def __enter__(self):\n",
    "        self.file_write_ptr = open(self.file_path, 'wb' if self.overwrite else 'ab')\n",
    "    def __exit__(self):\n",
    "        self.file_write_ptr.close()\n",
    "        \n",
    "    def __len__(self):\n",
    "        if not self._length:\n",
    "            self._length = len(list(self.__iter__()))\n",
    "        return self._length\n",
    "    \n",
    "    @property\n",
    "    def file_dumper(self):\n",
    "        if not self.file_writer_ptr or self.file_writer_ptr.closed:\n",
    "            self.file_writer_ptr = open(self.file_path, 'wb' if self.overwrite else 'ab' )\n",
    "        return self.file_writer_ptr\n",
    "    \n",
    "    def dump(self, obj):\n",
    "        pickle.dump(obj, self.file_dumper)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.file_writer_ptr and not self.file_writer_ptr.closed:\n",
    "            self.file_dumper.flush()\n",
    "        pkl_file = open(self.file_path, 'rb')\n",
    "        try:\n",
    "            while True:\n",
    "                self._length += 1\n",
    "                yield pickle.load(pkl_file)\n",
    "        except EOFError:\n",
    "            pkl_file.close()\n",
    "            return \n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        self.extend(other)\n",
    "        \n",
    "    def extend(self, iterable):\n",
    "        for thing in iterable:\n",
    "            self.append(thing)\n",
    "            \n",
    "    def append(self, element):\n",
    "        self.dump(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataManager module\n",
    "module used for collecting io info and preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class FeaturesSequence(keras.utils.Sequence):\n",
    "    def __init__(self, x_sequence, x_meta, y_set, \n",
    "                         batch_size, sample_length, feature_dimensions, \n",
    "                         duke_unique, dnase):\n",
    "\n",
    "        assert len(x_sequence) == len(x_meta) == len(y_set)\n",
    "        self.x_sequence, self.x_meta, self.y = x_sequence, x_meta, y_set\n",
    "        self.y_are_labels = type(y_set[0])==str\n",
    "        logging.info('Found y are labels; using OBIE')\n",
    "        if self.y_are_labels:\n",
    "            self.label2onehot = {'O':np.array([1,0,0,0]),'B':np.array([0,1,0,0]),'I':np.array([0.,0.,1.,0.]),'E':np.array([0.,0.,0.,1.]),}\n",
    "        self.sample_length=sample_length\n",
    "        self.n_samples = len(self.x_meta)\n",
    "        self.feature_dimensions=feature_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.onehot = {'A':np.array([1,0,0,0]),'C':np.array([0,1,0,0]),'G':np.array([0.,0.,1.,0.]),'T':np.array([0.,0.,0.,1.]),}\n",
    "        self.duke_unique = duke_unique\n",
    "        self.dnase = dnase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_samples / float(self.batch_size))) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # setup y\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            if self.y_are_labels:\n",
    "                labels = self.y[(i+idx*self.batch_size)%self.n_samples]\n",
    "                labels_onehot = np.empty((len(labels), 4))\n",
    "                for j in range(len(labels)):\n",
    "                    labels_onehot[j, :] = self.label2onehot[labels[j]]\n",
    "                batch_y.append(labels_onehot)\n",
    "            else:\n",
    "                batch_y.append(self.y[(i+idx*self.batch_size)%self.n_samples])\n",
    "\n",
    "        # setup x\n",
    "        batch_x = []\n",
    "        for i in range(self.batch_size):\n",
    "            seq, (chromosome, (start,stop)) = self.x_sequence[(i+idx*self.batch_size)%self.n_samples], self.x_meta[(i+idx*self.batch_size)%self.n_samples]\n",
    "            # this part is potentially very slow #\n",
    "            uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "            openness = np.array(self.dnase(chromosome, start, stop))\n",
    "            uniqueness[np.isnan(uniqueness)] = 0\n",
    "            openness[np.isnan(openness)] = 0\n",
    "            # # # # # # # # # # # # # # # # # # #\n",
    "            features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "            for j in range(self.sample_length):\n",
    "                features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "            batch_x.append(features)\n",
    "            \n",
    "        # setup sample weights\n",
    "#         batch_weight = []\n",
    "#         for i in range(self.batch_size):\n",
    "#             batch_weight.append(self.sample_weights[(i+idx*self.batch_size)%self.n_samples])\n",
    "        return np.array(batch_x), np.array(batch_y) #, np.array(batch_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class TrainingFeaturesSequence(keras.utils.Sequence):\n",
    "    def __init__(self, x_sequence, x_meta, y_set, \n",
    "                         batch_size, sample_length, feature_dimensions, \n",
    "                         duke_unique, dnase, sample_weights=None):\n",
    "\n",
    "        assert len(x_sequence) == len(x_meta) == len(y_set)\n",
    "        self.x_sequence_pos, self.x_meta_pos, self.y_pos = x_sequence[0], x_meta[0], y_set[0]\n",
    "        self.x_sequence_neg, self.x_meta_neg = x_sequence[1], x_meta[1]\n",
    "        self.y_are_labels = type(y_set[0][0])==str\n",
    "        logging.info('Found y are labels; using OBIE')\n",
    "        if self.y_are_labels:\n",
    "            self.label2onehot = {'O':np.array([1,0,0,0]),'B':np.array([0,1,0,0]),'I':np.array([0.,0.,1.,0.]),'E':np.array([0.,0.,0.,1.]),}\n",
    "        self.sample_length=sample_length\n",
    "        self.n_samples_pos, self.n_samples_neg = len(self.x_meta_pos), len(self.x_meta_neg)\n",
    "        self.feature_dimensions=feature_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.onehot = {'A':np.array([1,0,0,0]),'C':np.array([0,1,0,0]),'G':np.array([0.,0.,1.,0.]),'T':np.array([0.,0.,0.,1.]),}\n",
    "        self.duke_unique = duke_unique\n",
    "        self.dnase = dnase\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.n_samples_pos / float(self.batch_size))) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # setup y\n",
    "        half_batch_size = int(self.batch_size/2)\n",
    "        batch_y = []\n",
    "        for i in range(self.batch_size):\n",
    "            if i<half_batch_size:\n",
    "                # work on positive samples\n",
    "                if self.y_are_labels:\n",
    "                    labels = self.y_pos[(i+idx*half_batch_size)%self.n_samples_pos]\n",
    "                    labels_onehot = np.empty((len(labels), 4))\n",
    "                    for j in range(len(labels)):\n",
    "                        labels_onehot[j, :] = self.label2onehot[labels[j]]\n",
    "                    batch_y.append(labels_onehot)\n",
    "                else:\n",
    "                    batch_y.append(1)\n",
    "            else:\n",
    "                if self.y_are_labels:\n",
    "#                     labels = self.y_neg[(i+idx*self.batch_size/2)%self.n_samples_neg]\n",
    "                    labels_onehot = np.empty((self.sample_length, 4))\n",
    "                    for j in range(self.sample_length):\n",
    "                        labels_onehot[j, :] = self.label2onehot['O']\n",
    "                    batch_y.append(labels_onehot)\n",
    "                else:\n",
    "                    batch_y.append(0)\n",
    "\n",
    "        # setup x\n",
    "        batch_x = []\n",
    "        for i in range(self.batch_size):\n",
    "            if i<half_batch_size:\n",
    "                seq, (chromosome, (start,stop)) = self.x_sequence_pos[(i+idx*half_batch_size)%self.n_samples_pos], self.x_meta_pos[(i+idx*half_batch_size)%self.n_samples_pos]\n",
    "                # this part is potentially very slow #\n",
    "                uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                uniqueness[np.isnan(uniqueness)] = 0\n",
    "                openness[np.isnan(openness)] = 0\n",
    "                # # # # # # # # # # # # # # # # # # #\n",
    "                features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "                for j in range(self.sample_length):\n",
    "                    features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "                batch_x.append(features)\n",
    "            else:\n",
    "                seq, (chromosome, (start,stop)) = self.x_sequence_neg[(i+idx*half_batch_size-half_batch_size)%self.n_samples_neg], self.x_meta_neg[(i+idx*half_batch_size-half_batch_size)%self.n_samples_neg]\n",
    "                # this part is potentially very slow #\n",
    "                uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                uniqueness[np.isnan(uniqueness)] = 0\n",
    "                openness[np.isnan(openness)] = 0\n",
    "                # # # # # # # # # # # # # # # # # # #\n",
    "                features = np.empty((self.sample_length, self.feature_dimensions))\n",
    "                for j in range(self.sample_length):\n",
    "                    features[j,:] = (np.concatenate((self.onehot[seq[j]], [openness[j]], [uniqueness[j]])))\n",
    "                batch_x.append(features)\n",
    "            \n",
    "        # setup sample weights\n",
    "#         batch_weight = []\n",
    "#         for i in range(self.batch_size):\n",
    "#             batch_weight.append(self.sample_weights[(i+idx*self.batch_size)%self.n_samples])\n",
    "        return np.array(batch_x), np.array(batch_y) #,np.array(batch_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from functools import partial \n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "_cache = {}\n",
    "class DataManager:\n",
    "    \n",
    "    def __init__(self,label_file, celllineNtf_peakfile, \n",
    "                 bigwig_duke_unique_file = './wgEncodeDukeMapabilityUniqueness35bp.bigWig',\n",
    "                 bigwig_dnase_file = None,\n",
    "                 filter_file=None,\n",
    "                 use_pickler=False, memory_avail=8192, \n",
    "                 output_dir='./',\n",
    "                 only_label_dataset = True,\n",
    "                 move_finished_src = None, \n",
    "                 reduce_negative_samples=True, \n",
    "                 chr_valid = ['chr11'], chr_test = ['chr1', 'chr8', 'chr21'], \n",
    "                 check_set_ratio=9, debug={}):\n",
    "        \n",
    "        self.label_file = label_file\n",
    "        self.celllineNtf_peakfile = celllineNtf_peakfile\n",
    "        self.bigwig_duke_unique_file = bigwig_duke_unique_file\n",
    "        self.duke_bw = bigwig_duke_unique_file\n",
    "        self.use_pickler = use_pickler\n",
    "        self.memory_avail=memory_avail\n",
    "        self.output_dir = output_dir\n",
    "        self.only_label_dataset=only_label_dataset\n",
    "        self.move_finished_src = move_finished_src\n",
    "        self.filter_file = filter_file\n",
    "        self.reduce_negative_samples = reduce_negative_samples\n",
    "        self.chr_valid = chr_valid\n",
    "        self.chr_test = chr_test\n",
    "        self.check_set_ratio = check_set_ratio\n",
    "        self.debug = debug\n",
    "        self.exp_type, self._cellline, self.tf_name,_, self.set_name, self.peak_type = os.path.basename(celllineNtf_peakfile).split('.')\n",
    "        self.dnase_bw = bigwig_dnase_file if bigwig_dnase_file else '%s.1x.bw' % self._cellline.lower()\n",
    "        self.exp_id = '{}_{}'.format(self.tf_name, self._cellline)\n",
    "#         self.bigwig_cellline_file = get_bigwig_celline_file(self._cellline)\n",
    "        self.positive_samples_out, self.negative_samples_out, self.labels_out = [self.exp_id+'_'+x for x in ['positive_samples.txt', 'negative_samples.txt', 'labels.txt']]\n",
    "        self.chr_positive_out, self.chr_negative_out = [self.exp_id+'_chr_%s.npy' % sign for sign in ['positive', 'negative']]\n",
    "\n",
    "    def _cacher_(func):\n",
    "        def to_cache_func(self, *args, **kwargs):\n",
    "            if func.__name__ in _cache:\n",
    "                return _cache[func.__name__]\n",
    "            obj = func(self, *args, **kwargs)\n",
    "            _cache[func.__name__] = obj\n",
    "            return obj\n",
    "        return to_cache_func\n",
    "    \n",
    "    def release_memory(self):\n",
    "        import gc\n",
    "        _cache = {}\n",
    "        gc.collect()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2locNbound(self):\n",
    "        chr2locNbound_pklfile = '{}_chr2locNbound.pkl'.format(self.exp_id)\n",
    "        pgen_chr2locNbound = partial(gen_chr2locNbound, self.label_file, self._cellline)\n",
    "        chr2locNbound = if_not_pickled(chr2locNbound_pklfile,\n",
    "                                       pgen_chr2locNbound)\n",
    "        return chr2locNbound\n",
    "    @property\n",
    "    def chr2locNbound(self):\n",
    "        return self._chr2locNbound()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2filter_locs(self):\n",
    "        chr2filter_locs_pklfile = '{}_chr2filter_locs.pkl'.format(self.exp_id)\n",
    "        pgen_chr2filter_locs = partial(gen_chr2filter_locs, \n",
    "                                       self.filter_file)\n",
    "        chr2filter_locs = if_not_pickled(chr2filter_locs_pklfile, \n",
    "                                         pgen_chr2filter_locs)\n",
    "        return chr2filter_locs\n",
    "    @property\n",
    "    def chr2filter_locs(self):\n",
    "        return self._chr2filter_locs()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2locNpeaks(self):\n",
    "        chr2locNpeaks_pklfile = '{}_chr2locNpeaks.pkl'.format(self.exp_id) if self.filter_file else '{}_chr2locNpeaks_full.pkl'.format(self.exp_id)\n",
    "        p_genchr2locNpeaks = partial(gen_chr2locNpeaks, \n",
    "                                     self.celllineNtf_peakfile, \n",
    "                                     self.filter_file, \n",
    "                                     self.chr2filter_locs if self.filter_file else None)\n",
    "        chr2locNpeaks = if_not_pickled(chr2locNpeaks_pklfile,\n",
    "                                       p_genchr2locNpeaks)\n",
    "        return chr2locNpeaks\n",
    "    @property\n",
    "    def chr2locNpeaks(self):\n",
    "        return self._chr2locNpeaks()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chromosomes(self):\n",
    "        hg_pkl = 'hg19.pkl'\n",
    "        hg_genome_fasta = './hg19.genome.fa'\n",
    "        pgen_hg19 = partial(gen_hg19, hg_genome_fasta)\n",
    "        chromosomes = if_not_pickled(hg_pkl, pgen_hg19)\n",
    "        return chromosomes\n",
    "    @property\n",
    "    def chromosomes(self):\n",
    "        return self._chromosomes()\n",
    "    \n",
    "    @_cacher_\n",
    "    def _chr2labelsNseq(self):\n",
    "        chr2labelsNseq_pkl = '{}_chr2labelsNseq.pkl'.format(self.exp_id)\n",
    "        pgen_chr2labelsNseq = partial(gen_chr2labelsNseq,\n",
    "                                      self.chromosomes,\n",
    "                                      self.chr2locNbound, \n",
    "                                      self.chr2locNpeaks)\n",
    "        chr2labelsNseq = if_not_pickled(chr2labelsNseq_pkl, \n",
    "                                        pgen_chr2labelsNseq)\n",
    "        del _cache['_chr2locNpeaks']\n",
    "        del _cache['_chr2locNbound']\n",
    "        del _cache['_chr2filter_locs']\n",
    "        return chr2labelsNseq\n",
    "    @property\n",
    "    def chr2labelsNseq(self):\n",
    "        return self._chr2labelsNseq()\n",
    "    \n",
    "    def _create_samples(self):\n",
    "        logging.info('Creating positive/negative samples')\n",
    "        # generate samples\n",
    "        positive_samples, negative_samples, labels_positives = [],[],[]\n",
    "        pos_chr, neg_chr = [], [] # keep track of which chromosome each are from\n",
    "        \n",
    "        chr_training = set(self.chr2labelsNseq.keys()) - set(self.chr_valid) - set(self.chr_test)\n",
    "        if not len(chr_training):\n",
    "            raise InsufficientChromosomesException(set(self.chr2labelsNseq.keys()), set(self.chr_valid) | set(self.chr_test))\n",
    "        for chromosome, labelsNseq in tqdm(self.chr2labelsNseq.items()):\n",
    "            _positive_samples,_negative_samples, _labels_positives = [],[],[]\n",
    "            _pos_chr, _neg_chr = [], []\n",
    "            logging.info('Working on {}'.format(chromosome))\n",
    "            count = 0\n",
    "            pos_count = 0\n",
    "            neg_count = 0\n",
    "            for label, seq in labelsNseq:\n",
    "                # labels is None if negative sample\n",
    "                sequence = seq2sequence(seq, self.chromosomes)\n",
    "                if 'N' not in sequence:\n",
    "                    if label==None:\n",
    "                        _negative_samples.append(sequence)\n",
    "                        _neg_chr.append(seq)\n",
    "                        neg_count += 1\n",
    "                    else:\n",
    "                        _positive_samples.append(sequence)\n",
    "                        _labels_positives.append(label)\n",
    "                        _pos_chr.append(seq)\n",
    "                        pos_count += 1\n",
    "                    count += 1\n",
    "            logging.info('Found {} samples for {}; {} positive and {} negative'.format(count, \n",
    "                                                                                       chromosome, \n",
    "                                                                                       pos_count, \n",
    "                                                                                       neg_count))\n",
    "            # throw away some negative samples\n",
    "#             if self.reduce_negative_samples :\n",
    "#                 logging.info('Reducing negative samples for {}'.format(chromosome))\n",
    "#                 ratio = 1 if not (chromosome in self.chr_valid or chromosome in self.chr_test) else self.check_set_ratio\n",
    "#                 if len(_negative_samples) > len(_positive_samples):\n",
    "#                     npified = np.array([_negative_samples, _neg_chr], dtype=np.object)\n",
    "#                     n_to_choose_from = ratio * len(_positive_samples)\n",
    "#                     if n_to_choose_from > len(_negative_samples):\n",
    "#                         n_to_choose_from = len(_negative_samples)\n",
    "#                     indices_chosen = np.random.choice(len(_negative_samples), \n",
    "#                                                       n_to_choose_from, \n",
    "#                                                       replace=False)\n",
    "#                     chosen_samples, chosen_chr = npified[:,indices_chosen]\n",
    "#                     _negative_samples, _neg_chr = list(chosen_samples), list(chosen_chr)\n",
    "            \n",
    "            logging.info('Found {} samples for {}; {} positive and {} negative'.format(count, \n",
    "                                                                                       chromosome, \n",
    "                                                                                       len(_positive_samples), \n",
    "                                                                                       len(_negative_samples)))\n",
    "            positive_samples.extend(_positive_samples)\n",
    "            negative_samples.extend(_negative_samples)\n",
    "            labels_positives.extend(_labels_positives)\n",
    "            pos_chr.extend(_pos_chr)\n",
    "            neg_chr.extend(_neg_chr)\n",
    "            \n",
    "        # write samples to disk\n",
    "        logging.info('Writing samples to disk')\n",
    "        def write_to_disk(path, samples):\n",
    "            with open(path, 'w') as samples_file:\n",
    "                for i,sample in enumerate(samples):\n",
    "                    samples_file.write('>{}__{}\\n'.format(self.tf_name, i))\n",
    "                    samples_file.write('{}\\n'.format(sample))\n",
    "        \n",
    "        assert len(positive_samples) == len(pos_chr) == len(labels_positives)\n",
    "        assert len(negative_samples) == len(neg_chr)\n",
    "        write_to_disk(self.positive_samples_out, positive_samples)\n",
    "        write_to_disk(self.negative_samples_out, negative_samples)\n",
    "        write_to_disk(self.labels_out, labels_positives)\n",
    "        np.save(self.chr_positive_out, np.array(pos_chr, dtype=np.object))\n",
    "        np.save(self.chr_negative_out, np.array(neg_chr, dtype=np.object))\n",
    "        logging.info('Done writing samples to disk')\n",
    "        self._positive_samples, self._negative_samples, self._labels_positives = positive_samples, negative_samples, labels_positives\n",
    "        self._pos_chr, self._neg_chr = pos_chr, neg_chr\n",
    "        return positive_samples, negative_samples, labels_positives, pos_chr, neg_chr\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        # load samples from disk\n",
    "        logging.info('Loading positive/negative samples from disk')\n",
    "        positive_samples, negative_samples, labels_positives = [], [], []\n",
    "        with open(self.positive_samples_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    positive_samples.append(line.strip())\n",
    "        with open(self.negative_samples_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    negative_samples.append(line.strip())\n",
    "        with open(self.labels_out) as samples_file:\n",
    "            for line in samples_file:\n",
    "                if not line.startswith('>'):\n",
    "                    labels_positives.append(line.strip())  \n",
    "        pos_chr = np.load(self.chr_positive_out)\n",
    "        neg_chr = np.load(self.chr_negative_out) \n",
    "        logging.info('Done loading samples from disk')\n",
    "        self._positive_samples, self._negative_samples, self._labels_positives, = positive_samples, negative_samples, labels_positives\n",
    "        self._pos_chr, self._neg_chr = pos_chr, neg_chr\n",
    "        return positive_samples, negative_samples, labels_positives, pos_chr, neg_chr\n",
    "    \n",
    "    @_cacher_\n",
    "    def _samples(self):\n",
    "        sample_files = [self.positive_samples_out, \n",
    "                        self.negative_samples_out, \n",
    "                        self.labels_out, \n",
    "                        self.chr_positive_out, \n",
    "                        self.chr_negative_out ]\n",
    "        \n",
    "        if not all_paths_exists(*sample_files):\n",
    "            return self._create_samples()\n",
    "        else: \n",
    "            return self._load_samples()\n",
    "    @property\n",
    "    def samples(self):\n",
    "        return self._samples()\n",
    "    \n",
    "    @property\n",
    "    def positive_samples(self):\n",
    "        try:\n",
    "            return self._positive_samples\n",
    "        except:\n",
    "            return self.samples[0]\n",
    "    @property\n",
    "    def negative_samples(self):\n",
    "        try:\n",
    "            return self._negative_samples\n",
    "        except:\n",
    "            return self.samples[1]\n",
    "    @property\n",
    "    def labels_positives(self):\n",
    "        try:\n",
    "            return self._labels_positives\n",
    "        except:\n",
    "            return self.samples[2]\n",
    "    @property\n",
    "    def pos_chr(self):\n",
    "        try:\n",
    "            return self._pos_chr\n",
    "        except:\n",
    "            return self.samples[3]\n",
    "    @property\n",
    "    def neg_chr(self):\n",
    "        try:\n",
    "            return self._neg_chr\n",
    "        except:\n",
    "            return self.samples[4]\n",
    "    \n",
    "    @func_metrics_display\n",
    "    def dnashapeR(self, r_lib_location = \"C:/Users/Rudolf/Documents/R/win-library/3.5\"):\n",
    "        # setup structural information\n",
    "        # setup R\n",
    "        import rpy2\n",
    "        from rpy2.robjects.packages import importr\n",
    "        import rpy2.robjects as robjects\n",
    "\n",
    "        # set the available amount of memory\n",
    "        robjects.r('memory.limit(size = {})'.format(self.memory_avail))\n",
    "\n",
    "        base = importr('base')\n",
    "        utils = importr('utils')\n",
    "        logging.info('Using {}'.format(str(base._libPaths())))\n",
    "\n",
    "        # if DNAshapeR cannot be found try this:\n",
    "        robjects.r( \".libPaths('{}')\".format(r_lib_location))\n",
    "\n",
    "        from functools import reduce\n",
    "\n",
    "        if not all_paths_exists(*[self.positive_samples_out+ext for ext in ['.EP', '.HelT', '.MGW', '.ProT', '.Roll']]):\n",
    "            logging.info('Running DNAshapeR for positive samples')\n",
    "            base = importr('base')\n",
    "            utils = importr('utils')\n",
    "            dna_shape = importr('DNAshapeR', lib_loc=r_lib_location)\n",
    "            #rpy2 does not know how to release memory\n",
    "            @rmem_manage\n",
    "            def process_positive():\n",
    "                r_statements = []\n",
    "                r_statements.append('library(DNAshapeR)')\n",
    "                r_statements.append('pred <- getShape(\"./{}\")'.format(self.positive_samples_out))\n",
    "                r_cmd = '\\n'.join(r_statements)\n",
    "                robjects.r(r_cmd)\n",
    "            process_positive()\n",
    "        else:\n",
    "            logging.info('Skipping DNAshapeR for positive samples; already exists')\n",
    "        if not all_paths_exists(*[self.negative_samples_out+ext for ext in ['.EP', '.HelT', '.MGW', '.ProT', '.Roll']]):\n",
    "            logging.info('Running DNAshapeR for negative samples')\n",
    "            base = importr('base')\n",
    "            utils = importr('utils')\n",
    "            dna_shape = importr('DNAshapeR', lib_loc=r_lib_location)\n",
    "            gc.collect()\n",
    "            @rmem_manage\n",
    "            def process_negative():\n",
    "                r_statements = []\n",
    "                r_statements.append('library(DNAshapeR)')\n",
    "                r_statements.append('pred <- getShape(\"./{}\")'.format(self.negative_samples_out))\n",
    "                r_cmd = '\\n'.join(r_statements)\n",
    "                robjects.r(r_cmd)\n",
    "            process_negative()\n",
    "        else:\n",
    "            logging.info('Skipping DNAshapeR for negative samples; already exists')\n",
    "    \n",
    "    @_cacher_\n",
    "    def _h5py_duke(self):\n",
    "        return gen_h5py4bw(self.duke_bw, 'duke_unique')\n",
    "    \n",
    "    @property\n",
    "    def h5py_duke(self):\n",
    "        return self._h5py_duke()\n",
    "    \n",
    "    def duke_unique(self, chromosome, start, stop):\n",
    "        return self.h5py_duke[chromosome][start:stop]\n",
    "        \n",
    "    @_cacher_\n",
    "    def _h5py_dnase(self):\n",
    "        return gen_h5py4bw(self.dnase_bw, self._cellline)\n",
    "    \n",
    "    @property\n",
    "    def h5py_dnase(self):\n",
    "        return self._h5py_dnase()\n",
    "    \n",
    "    def dnase(self, chromosome, start, stop):\n",
    "        return self.h5py_dnase[chromosome][start:stop]\n",
    "    \n",
    "    def create_datagen_from_samples(self, balance_valid_ratio=9, useDNAshapeR=False):\n",
    "        '''\n",
    "        Returns functions that takes a batch_size as input and returns generators\n",
    "        '''\n",
    "        import pickle\n",
    "        import numpy as np\n",
    "        from tqdm import tqdm\n",
    "        # files describing structure of dna\n",
    "        exts = ['']\n",
    "        if useDNAshapeR:\n",
    "            raise NotImplementedError('DNAshapeR is not streamable for large datasets')\n",
    "        dataset_pkl = '%s_dataset.pkl'%self.exp_id\n",
    "        import os\n",
    "        import pickle\n",
    "        if os.path.exists(dataset_pkl):\n",
    "            logging.info('Reading static data')\n",
    "            with open(dataset_pkl, 'rb') as pkl:\n",
    "                train_positive_samples = pickle.load(pkl)\n",
    "                train_negative_samples = pickle.load(pkl)\n",
    "                train_labels_positives = pickle.load(pkl)\n",
    "                train_pos_chr = pickle.load(pkl) \n",
    "                train_neg_chr = pickle.load(pkl)\n",
    "                training_cls = pickle.load(pkl)\n",
    "                training_labels = pickle.load(pkl)\n",
    "                valid_samples = pickle.load(pkl)\n",
    "                valid_chr = pickle.load(pkl)\n",
    "                valid_cls = pickle.load(pkl)\n",
    "                valid_labels = pickle.load(pkl)\n",
    "                test_samples = pickle.load(pkl)\n",
    "                test_chr = pickle.load(pkl)\n",
    "                test_cls = pickle.load(pkl)\n",
    "                test_labels = pickle.load(pkl)\n",
    "                self.dataset2counts_pos = pickle.load(pkl)\n",
    "                self.dataset2counts_neg = pickle.load(pkl)\n",
    "                self.training_length = pickle.load(pkl)\n",
    "                self.valid_length = pickle.load(pkl)\n",
    "                self.test_length = pickle.load(pkl)\n",
    "                self.sample_length = pickle.load(pkl)\n",
    "                self.feature_dimensions = pickle.load(pkl)  \n",
    "                logging.info('Loaded training/valid/test of sizes {}/{}/{}'.format(self.training_length, self.valid_length, self.test_length))\n",
    "        else:     \n",
    "            logging.info('Getting samples')\n",
    "            # positive_samples and negative_samples are lists of sequences:str\n",
    "            # pos_chr and neg_chr are lists of (chromosome:str, (start:int, stop:int))\n",
    "            # labels_positives is a list of (for crf) labels:str\n",
    "            positive_samples, negative_samples, labels_positives, pos_chr, neg_chr = self.samples\n",
    "\n",
    "            logging.info('Shuffling samples')\n",
    "            assert len(positive_samples) == len(labels_positives) == len(pos_chr)\n",
    "            assert len(negative_samples) == len(neg_chr)\n",
    "\n",
    "            pos_len = len(positive_samples)\n",
    "            neg_len = len(negative_samples)\n",
    "\n",
    "            class Pair:\n",
    "                __slots__=['chromosome', 'domain']\n",
    "                def __init__(self, chromosome, domain):\n",
    "                    self.chromosome = chromosome\n",
    "                    self.domain = domain\n",
    "                def __repr__(self):\n",
    "                    return '('+str(self.chromosome)+', '+str(self.domain)+')'\n",
    "                def reveal(self):\n",
    "                    return (self.chromosome, self.domain)\n",
    "\n",
    "            @func_metrics_display\n",
    "            def shuffle(*arrays):\n",
    "                import numpy as np\n",
    "                try:\n",
    "                    npified = np.array(arrays, dtype=np.object)\n",
    "                except Exception as e:\n",
    "                    logging.error('{}'.format([np.array(a).shape for a in arrays]))\n",
    "                    raise e\n",
    "                length = npified.shape[1]\n",
    "                indices_chosen = np.random.choice(length, length, replace=False)\n",
    "                return npified[:,indices_chosen]\n",
    "\n",
    "            pos_pairs = [Pair(*c) for c in pos_chr]\n",
    "            neg_pairs = [Pair(*c) for c in neg_chr]\n",
    "\n",
    "            positive_samples, labels_positives, pos_pairs = shuffle(positive_samples, labels_positives, pos_pairs)\n",
    "            negative_samples, neg_pairs = shuffle(negative_samples, neg_pairs)\n",
    "\n",
    "            pos_chr = [pair.reveal() for pair in pos_pairs]\n",
    "            neg_chr = [pair.reveal() for pair in neg_pairs]\n",
    "\n",
    "            # create stats on number of pos/neg for training/valid/test sets\n",
    "            logging.info('Creating stats on number of pos/neg for training/valid/test sets')\n",
    "\n",
    "            chr_valid_set = set(self.chr_valid)\n",
    "            chr_test_set = set(self.chr_test)\n",
    "\n",
    "            def try_addone(counts, key):\n",
    "                try:\n",
    "                    counts[key]+=1\n",
    "                except KeyError:\n",
    "                    counts[key]=1\n",
    "            @func_metrics_display\n",
    "            def count(chrs):            \n",
    "                counts = {}   \n",
    "                for chromosome, (start,stop) in chrs:\n",
    "                    if chromosome in chr_valid_set:\n",
    "                        try_addone(counts, 'valid')\n",
    "                    elif chromosome in chr_test_set:\n",
    "                        try_addone(counts, 'test')\n",
    "                    else: # in training set\n",
    "                        try_addone(counts, 'train')\n",
    "                return counts\n",
    "\n",
    "            pos_counts = count(pos_chr)\n",
    "            neg_counts = count(neg_chr)\n",
    "            logging.info('Counted positive for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[pos_counts[x]for x in ['train','valid','test']]))\n",
    "            logging.info('Counted negative for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[neg_counts[x]for x in ['train','valid','test']]))\n",
    "            self.dataset2counts_pos = pos_counts\n",
    "            self.dataset2counts_neg = neg_counts\n",
    "\n",
    "            logging.info('Splitting samples into training/valid/test')\n",
    "\n",
    "            train_positive_samples, train_labels_positives, train_pos_chr = [], [], []\n",
    "            valid_positive_samples, valid_labels_positives, valid_pos_chr = [], [], []\n",
    "            test_positive_samples, test_labels_positives, test_pos_chr = [], [], []\n",
    "            train_negative_samples, train_neg_chr = [], []\n",
    "            valid_negative_samples, valid_neg_chr = [], []\n",
    "            test_negative_samples, test_neg_chr = [], []\n",
    "\n",
    "            logging.info('Preparing for positive training/valid/test')\n",
    "            for sample, label, c in zip(positive_samples, labels_positives, pos_chr):\n",
    "                chromosome, (start, stop) = c\n",
    "                if chromosome in chr_valid_set:\n",
    "                    valid_positive_samples.append(sample)\n",
    "                    valid_labels_positives.append(label)\n",
    "                    valid_pos_chr.append(c)\n",
    "                elif chromosome in chr_test_set:\n",
    "                    test_positive_samples.append(sample)\n",
    "                    test_labels_positives.append(label)\n",
    "                    test_pos_chr.append(c)\n",
    "                else: # in training set\n",
    "                    train_positive_samples.append(sample)\n",
    "                    train_labels_positives.append(label)\n",
    "                    train_pos_chr.append(c)\n",
    "            logging.info('Preparing for negative training/valid/test')\n",
    "            for sample, c in zip(tqdm(negative_samples), neg_chr):\n",
    "                chromosome, (start, stop) = c\n",
    "                if chromosome in chr_valid_set:\n",
    "                    valid_negative_samples.append(sample)\n",
    "                    valid_neg_chr.append(c)\n",
    "                elif chromosome in chr_test_set:\n",
    "                    test_negative_samples.append(sample)\n",
    "                    test_neg_chr.append(c)\n",
    "                else: # in training set\n",
    "                    train_negative_samples.append(sample)\n",
    "                    train_neg_chr.append(c)\n",
    "\n",
    "            if self.use_pickler:\n",
    "                logging.info('Setting up pickler to intercept used data')\n",
    "                label_dataset = PickleManager('{}_label_dataset.pkl'.format(self.exp_id), overwrite=True)\n",
    "\n",
    "            import numpy as np\n",
    "            # start with positive samples\n",
    "            onehot = {'A':(1.,0.,0.,0.),\n",
    "                      'C':(0.,1.,0.,0.),\n",
    "                      'G':(0.,0.,1.,0.),\n",
    "                      'T':(0.,0.,0.,1.),}\n",
    "\n",
    "            logging.info('Generating feature generators')\n",
    "            # we want one for each pos/neg and train/valid/test\n",
    "            # we want same number of pos/neg samples for training\n",
    "            sample_start, sample_stop = train_pos_chr[0][1]\n",
    "            sample_length = sample_stop-sample_start\n",
    "\n",
    "            logging.info('Samples are found to have length {}'.format(sample_length))\n",
    "            self.sample_length = sample_length\n",
    "            self.feature_dimensions = 6\n",
    "\n",
    "            logging.info('Generating static data for training')\n",
    "            n_pos_train = len(train_positive_samples) \n",
    "            if self.reduce_negative_samples:\n",
    "                train_negative_samples=train_negative_samples[:n_pos_train]\n",
    "                train_neg_chr=train_neg_chr[:n_pos_train]\n",
    "            n_neg_train = len(train_negative_samples)\n",
    "            # shuffle the pos/neg for training\n",
    "            # x, metadata, y, labels (for crf)\n",
    "            training_samples, training_chr, training_cls, training_labels = shuffle(train_positive_samples + train_negative_samples,\n",
    "                                                                                    train_pos_chr          + train_neg_chr,\n",
    "                                                                                    [1]*n_pos_train        + [0]*n_neg_train,\n",
    "                                                                                    train_labels_positives + ['O'*sample_length]*n_neg_train)\n",
    "            \n",
    "            self.dataset2counts_pos['train'] = n_pos_train\n",
    "            self.dataset2counts_neg['train'] = n_neg_train\n",
    "            self.training_length = n_pos_train*2\n",
    "            logging.info('Counted positive for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[pos_counts[x]for x in ['train','valid','test']]))\n",
    "            logging.info('Counted negative for training/valid/test: {:>12}  {:>12}  {:>12}'.format(*[neg_counts[x]for x in ['train','valid','test']]))\n",
    "\n",
    "            logging.info('Generating static data for validation/test')\n",
    "            # take all negative samples for valid/test instead of limiting it like in training\n",
    "            if balance_valid_ratio:\n",
    "                n_pos_valid = len(valid_positive_samples)\n",
    "                valid_negative_samples = valid_negative_samples[:n_pos_valid*balance_valid_ratio]\n",
    "                valid_neg_chr = valid_neg_chr[:n_pos_valid*balance_valid_ratio]\n",
    "            self.valid_length = len(valid_positive_samples) + len(valid_negative_samples)\n",
    "            self.test_length = len(test_positive_samples) + len(test_negative_samples)\n",
    "\n",
    "            valid_samples, valid_chr, valid_cls, valid_labels = shuffle(valid_positive_samples          + valid_negative_samples,\n",
    "                                                                        valid_pos_chr                   + valid_neg_chr,\n",
    "                                                                        [1]*len(valid_positive_samples) + [0]*len(valid_negative_samples),\n",
    "                                                                        valid_labels_positives          + ['O'*sample_length]*len(valid_negative_samples))\n",
    "\n",
    "            test_samples, test_chr, test_cls, test_labels = shuffle(test_positive_samples          + test_negative_samples,\n",
    "                                                                    test_pos_chr                   + test_neg_chr,\n",
    "                                                                    [1]*len(test_positive_samples) + [0]*len(test_negative_samples),\n",
    "                                                                    test_labels_positives          + ['O'*sample_length]*len(test_negative_samples))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            logging.info('Saving static data')\n",
    "            with open(dataset_pkl, 'wb') as pkl:\n",
    "                datalst= [train_positive_samples, train_negative_samples, train_labels_positives,\n",
    "                          train_pos_chr, train_neg_chr,\n",
    "                          training_cls, training_labels, \n",
    "                           valid_samples, valid_chr, valid_cls, valid_labels, \n",
    "                           test_samples, test_chr, test_cls, test_labels,\n",
    "                          self.dataset2counts_pos, self.dataset2counts_neg, \n",
    "                          self.training_length, self.valid_length, self.test_length, self.sample_length,\n",
    "                          self.feature_dimensions]\n",
    "                for thing in tqdm(datalst):\n",
    "                    pickle.dump(thing, pkl)\n",
    "            \n",
    "        logging.info('Creating generators')\n",
    "    \n",
    "        onehot = {'A':(1.,0.,0.,0.),\n",
    "                  'C':(0.,1.,0.,0.),\n",
    "                  'G':(0.,0.,1.,0.),\n",
    "                  'T':(0.,0.,0.,1.),}\n",
    "        def features_gen(samples, chrs):\n",
    "            def batch_features_generator(batch_size):\n",
    "                max_k = len(samples) // batch_size\n",
    "                k = 0\n",
    "                while True:\n",
    "                    batch = []\n",
    "                    \n",
    "                    for i in range(batch_size):\n",
    "                        seq, features = samples[i+k*batch_size], chrs[i+k*batch_size]\n",
    "                        chromosome, domain = features\n",
    "                        start, stop = domain\n",
    "                        # this part is potentially very slow #\n",
    "                        uniqueness = np.array(self.duke_unique(chromosome, start, stop))\n",
    "                        openness = np.array(self.dnase(chromosome, start, stop))\n",
    "                        uniqueness[np.isnan(uniqueness)] = 0\n",
    "                        openness[np.isnan(openness)] = 0\n",
    "                        # # # # # # # # # # # # # # # # # # # \n",
    "                        features = []\n",
    "                        for i in range((stop-start)):\n",
    "                            feature = []\n",
    "                            feature.extend( onehot[seq[i]] )\n",
    "                            feature.append( openness[i] )\n",
    "                            feature.append( uniqueness[i] )\n",
    "                            features.append(feature)\n",
    "                        batch.append(np.array(features))\n",
    "                    yield np.array(batch)\n",
    "                    # cycle for infinite generator\n",
    "                    k+=1\n",
    "                    if k==max_k:\n",
    "                        k=0\n",
    "            return batch_features_generator\n",
    "        \n",
    "        logging.info('Creating feature generators')\n",
    "#         train_x = features_gen(train_positive_samples + train_negative_samples, training_chr)\n",
    "        valid_x = features_gen(valid_samples, valid_chr)\n",
    "        test_x = features_gen(test_samples, test_chr)\n",
    "        \n",
    "        @func_metrics_display\n",
    "        def values_gen(items, dtype=np.object):\n",
    "            def batch_generator(batch_size):\n",
    "                max_k = len(items) // batch_size\n",
    "                k = 0\n",
    "                while True:\n",
    "                    result = [items[i+k*batch_size] for i in range(batch_size)]\n",
    "                    yield np.array(result, dtype=dtype)\n",
    "                    # cycle for infinite generator\n",
    "                    k+=1\n",
    "                    if k==max_k:\n",
    "                        k=0\n",
    "            return batch_generator\n",
    "        \n",
    "        logging.info('Calculating sample weights')\n",
    "        def sample_weights(c, pos_count, neg_count):\n",
    "            neg_weight = pos_count/neg_count\n",
    "            return (1-c)*neg_weight + c        \n",
    "        \n",
    "        logging.info('Creating features sequencers')\n",
    "        train_labels_sequencer = lambda batch_size: TrainingFeaturesSequence((train_positive_samples, train_negative_samples), \n",
    "                                                                             (train_pos_chr, train_neg_chr), \n",
    "                                                                             (train_labels_positives, None), \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        train_cls_sequencer = lambda batch_size: TrainingFeaturesSequence((train_positive_samples, train_negative_samples), \n",
    "                                                                          (train_pos_chr, train_neg_chr), \n",
    "                                                                          ([1]*self.dataset2counts_pos['train'] , None), \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        valid_labels_sequencer = lambda batch_size: FeaturesSequence(valid_samples, valid_chr, valid_labels, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        valid_cls_sequencer = lambda batch_size: FeaturesSequence(valid_samples, valid_chr, valid_cls, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        test_labels_sequencer = lambda batch_size: FeaturesSequence(test_samples, test_chr, test_labels, \n",
    "                                                                     batch_size, \n",
    "                                                                     self.sample_length, self.feature_dimensions, \n",
    "                                                                     self.duke_unique, self.dnase)\n",
    "        test_cls_sequencer = lambda batch_size: FeaturesSequence(test_samples, test_chr, test_cls, \n",
    "                                                                    batch_size, \n",
    "                                                                    self.sample_length, self.feature_dimensions, \n",
    "                                                                    self.duke_unique, self.dnase)\n",
    "        \n",
    "        \n",
    "        logging.info('Creating value generators')\n",
    "        \n",
    "#         train_label = values_gen(training_labels)\n",
    "        valid_label = values_gen(valid_labels)\n",
    "        test_label = values_gen(test_labels)\n",
    "        \n",
    "#         train_seq = values_gen(train_pos_chr+train_neg_chr)\n",
    "        valid_seq = values_gen(valid_chr)\n",
    "        test_seq = values_gen(test_chr)\n",
    "        \n",
    "#         train_y = values_gen(training_cls, int)\n",
    "        valid_y = values_gen(valid_cls, int)\n",
    "        test_y = values_gen(test_cls, int)\n",
    "\n",
    "        return  (\n",
    "#                 train_x, train_label, train_seq, train_y, \n",
    "                 train_labels_sequencer, train_cls_sequencer,\n",
    "                 valid_x, valid_label, valid_seq, valid_y, valid_labels_sequencer, valid_cls_sequencer, \n",
    "                 test_x, test_label, test_seq, test_y, test_labels_sequencer, test_cls_sequencer)\n",
    "        \n",
    "    def load_data(self, *names):\n",
    "        label_dataset_output_dir = os.path.join(self.output_dir, self.exp_id+'_label_dataset')  \n",
    "        paths = [os.path.join(label_dataset_output_dir, name+'.npy') for name in names]\n",
    "        return [np.load(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel('DEBUG')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <<<<<<<< Launch file setup >>>>>>>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Looking for peak files for file FOXA2._.labels.tsv\n",
      "INFO:root:Working on peakfile ChIPseq.liver.FOXA2.conservative._.narrowPeak\n",
      "INFO:root:Looking for peak files for file NANOG._.labels.tsv\n"
     ]
    }
   ],
   "source": [
    "debug = {}\n",
    "label_files = [f for f in os.listdir('.') if f.endswith('_.labels.tsv')]\n",
    "filter_file = './total_regions.blacklistfiltered.merged.bed'\n",
    "tf_nameNdm = []\n",
    "dm = None\n",
    "for label_file in label_files:\n",
    "    logging.info('Looking for peak files for file %s' % label_file)\n",
    "    with open(label_file) as f:\n",
    "        first_line = next((l for l in f))\n",
    "        # first line contains titles; chr, start, stop, <cellline 1>, ... , <cellline n> \n",
    "        _,_,_, *celllines = first_line.split() \n",
    "\n",
    "    _tf_name = os.path.basename(label_file).split('.')[0]\n",
    "    regex = re.compile(r'chipseq\\.\\w+\\.%s\\..*\\.\\_\\.narrowpeak'%_tf_name, re.IGNORECASE)\n",
    "    celllineNtf_peakfiles = []\n",
    "    for cellline in celllines:\n",
    "        for each_file in os.listdir('.'):\n",
    "            if regex.match(each_file):\n",
    "                celllineNtf_peakfiles.append(each_file)\n",
    "    for peakfile in celllineNtf_peakfiles:\n",
    "        logging.info('Working on peakfile %s'%peakfile)\n",
    "        try:\n",
    "#             dm = DataManager(label_file, peakfile, output_dir='datasets', \n",
    "#                         move_finished_src='finished_peakfiles',\n",
    "#                         filter_file=filter_file, \n",
    "#                         reduce_negative_samples=True, check_set_ratio=9,\n",
    "#                         memory_avail=8192, debug=debug)\n",
    "#             dm.samples\n",
    "#             generators = dm.create_datagen_from_samples(useDNAshapeR=False)\n",
    "#             train_x, train_label, train_seq, train_cls, train_labels_sequencer, train_cls_sequencer = generators[:6]\n",
    "#             valid_x, valid_label, valid_seq, valid_cls, valid_labels_sequencer, valid_cls_sequencer = generators[6:12]\n",
    "#             test_x, test_label, test_seq, test_cls, test_labels_sequencer, test_cls_sequencer = generators[12:]\n",
    "            tf_nameNdm.append((_tf_name, None))\n",
    "        except InsufficientChromosomesException as e:\n",
    "            logging.warning('Skipping creating data for {}'.format(peakfile))\n",
    "            logging.warning('Error message: {}'.format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <<<<<<<< RUN MODELS >>>>>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2*((p*r)/(p+r+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from tqdm import tqdm\n",
    "class Stats_callback(Callback):\n",
    "    def __init__(self, gen, steps, cls=None, convert_to_categorical=False, vs_display_length = 20, show_metric_interval=1, warning_threshold=0.85):\n",
    "        self.auROCs = []\n",
    "        self.auPRCs = []\n",
    "        self.re5FDRs = []\n",
    "        self.re10FDRs = []\n",
    "        self.re25FDRs = []\n",
    "        self.re50FDRs = []\n",
    "        self.confusion_matrix = []\n",
    "        self.valid_gen = gen\n",
    "        self.valid_steps = int(steps)\n",
    "        # convert crf label data into categorical\n",
    "        self.convert_to_categorical = convert_to_categorical\n",
    "        # if using convert_to_categorical (ie using crf) need to provide the classification along with OBIE from valid_gen\n",
    "        if convert_to_categorical:\n",
    "            if not cls:\n",
    "                raise ValueError('if using convert_to_categorical (ie using crf) need to provide the classification along with OBIE from valid_gen')\n",
    "            self.cls_gen = cls\n",
    "        self.vs_display_length = vs_display_length if vs_display_length>=0 else 0\n",
    "        \n",
    "        # display settings\n",
    "        self.show_metric_interval = show_metric_interval\n",
    "        self.warning_threshold = warning_threshold\n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return {\n",
    "            'auROCs': tuple(self.auROCs),\n",
    "            'auPRCs': tuple(self.auPRCs),\n",
    "            're5FDRs': tuple(self.re5FDRs),\n",
    "            're10FDRs': tuple(self.re10FDRs),\n",
    "            're25FDRs': tuple(self.re25FDRs),\n",
    "            're50FDRs': tuple(self.re50FDRs),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def auROC(labels, predictions):\n",
    "        return roc_auc_score(labels, predictions)\n",
    "\n",
    "    @staticmethod\n",
    "    def auPRC(labels, predictions):\n",
    "        precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "        return auc(recall, precision)\n",
    "\n",
    "    @staticmethod\n",
    "    def recall_at_precision(labels, predictions, precision_at):\n",
    "        threshold = 1.0-precision_at\n",
    "        precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "        return 100 * recall[np.searchsorted(precision - threshold, 0)]\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):      \n",
    "        if epoch%self.show_metric_interval or epoch<=1:\n",
    "            logging.info('Showing metric in {} epochs'.format(self.show_metric_interval-(epoch%self.show_metric_interval)) )\n",
    "            return\n",
    "        logging.info('Calculating metrics...')\n",
    "        def contains_BIE(sparse):\n",
    "            for c in sparse:\n",
    "                if c==1 or c==2 or c==3:\n",
    "                    return 1\n",
    "            return 0\n",
    "        epoch_auROC = []\n",
    "        epoch_auPRC = []\n",
    "        re5, re10, re25, re50 = [], [], [], []\n",
    "        tns, fps, fns, tps = 0,0,0,0\n",
    "        for batch_index in tqdm(range(self.valid_steps)):\n",
    "            features, y_true = self.valid_gen[batch_index]            \n",
    "            y_pred = np.asarray(self.model.predict(features, verbose=1))\n",
    "            y_pred = y_pred.round().astype(int) \n",
    "            \n",
    "            if self.convert_to_categorical:\n",
    "#                 logging.info('Converting to categorical')\n",
    "                maxed = np.argmax(y_pred, axis=2)\n",
    "                cls_pred =[contains_BIE(m) for m in maxed]\n",
    "                _y_pred = np.array(cls_pred)\n",
    "            else:\n",
    "                _y_pred = y_pred\n",
    "\n",
    "            _y = next(self.cls_gen) if self.convert_to_categorical else y_true\n",
    "            \n",
    "            if self.warning_threshold:\n",
    "                total_pos = sum(_y)\n",
    "                pred_pos = np.count_nonzero(_y_pred>=self.warning_threshold)\n",
    "                if pred_pos/ total_pos < 0.1:\n",
    "                    logging.warning('Very low positive prediction rate')\n",
    "                    logging.warning('{:25}{:25}'.format('y head :', 'y prediction head :'))\n",
    "                    for i in range(10):\n",
    "                        logging.warning('{:25}{:25}'.format(_y[i], _y_pred[i]))\n",
    "            # ROC\n",
    "            try:\n",
    "                epoch_auROC.append(self.auROC(_y, _y_pred))\n",
    "            except Exception as e:\n",
    "                logging.error('y shape: {}\\ny_pred shape: {}'.format(_y.shape, _y_pred.shape))\n",
    "                logging.error('y : {}\\ny_pred : {}'.format(_y, _y_pred))\n",
    "                raise e\n",
    "            # PRC\n",
    "            epoch_auPRC.append(self.auPRC(_y, _y_pred))\n",
    "            # Recalls\n",
    "            result5, result10, result25, result50 = [self.recall_at_precision(_y, _y_pred, precision) for precision in [0.05,0.1,0.25,0.5]]\n",
    "            re5.append(result5)\n",
    "            re10.append(result10)\n",
    "            re25.append(result25)\n",
    "            re50.append(result50)\n",
    "            # Compute confusion matrix\n",
    "#             logging.info('Calculating confusion matrix')\n",
    "            tn, fp, fn, tp = confusion_matrix(_y, np.around(_y_pred)).ravel()\n",
    "            tns += tn\n",
    "            fps += fp\n",
    "            fns += fn\n",
    "            tps += tp\n",
    "            \n",
    "        # collect into object's history\n",
    "        self.auROCs.append(np.mean(epoch_auROC))        \n",
    "        self.auPRCs.append(np.mean(epoch_auPRC))\n",
    "        self.re5FDRs.append(np.mean(re5))\n",
    "        self.re10FDRs.append(np.mean(re10))\n",
    "        self.re25FDRs.append(np.mean(re25))\n",
    "        self.re50FDRs.append(np.mean(re50))\n",
    "\n",
    "        self.confusion_matrix.append((tns, fps, fns, tps))\n",
    "        \n",
    "        logging.info('Epoch {}:'.format(epoch))\n",
    "        logging.info('avg auROC: {:8.4f} avg auPRC: {:8.4f}'.format(np.mean(epoch_auROC), np.mean(epoch_auPRC)))\n",
    "        logging.info('Recall@5%/10%/25%/50%: {:6.4f} {:6.4f} {:6.4f} {:6.4f}'.format(np.mean(re5), np.mean(re10), np.mean(re25), np.mean(re50)))\n",
    "        logging.info('tp:{:>10} fn:{:>10} tn:{:>10} fp:{:>10}'.format(tps, fns, tns, fps,))\n",
    "        \n",
    "        if self.vs_display_length:\n",
    "            tp_fn = int(tps/(tps+fns) * self.vs_display_length)\n",
    "            compl_tp_fn = self.vs_display_length - tp_fn\n",
    "            tn_fp = int(tns/(tns+fps) * self.vs_display_length)\n",
    "            compl_tn_fp = self.vs_display_length - tn_fp\n",
    "\n",
    "            logging.info('tp|{}|fn   tn|{}|fp'.format('#'*tp_fn + '_'*(compl_tp_fn), '#'*tn_fp + '_'*(compl_tn_fp) ))\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autofill_params(generator, params_dict):\n",
    "    '''\n",
    "    Auto fill generator with parameters from parmas_dict\n",
    "    returns curried generator\n",
    "    '''\n",
    "    import inspect\n",
    "    creation_params = inspect.signature(generator).parameters.keys()\n",
    "    best_params = {k:v for k,v in params_dict.items() if k in creation_params}\n",
    "    def recreated():\n",
    "        return create_baseline(**best_params)\n",
    "    return recreated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnd_subset(size, *data_arrs):\n",
    "    '''\n",
    "    Get a random subset of a set of data with matching indices\n",
    "    '''\n",
    "    import functools\n",
    "    same_sizes = [data_arrs[i].shape[0]==data_arrs[i+1].shape[0] if i < len(data_arrs)-1 else True for i, da in enumerate(data_arrs)]\n",
    "    if not  functools.reduce(lambda acc,x: acc and x, same_sizes):\n",
    "        raise ValueError('Not all datasets have the same number of samples (1st dimension size)')\n",
    "    subset_selection = np.random.choice(data_arrs[0].shape[0], size, replace=False)\n",
    "    return [data_arr[subset_selection,...] for data_arr in data_arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnd_subset_gen(size, data_arrs_gen, idempotent=False):\n",
    "    '''\n",
    "    Get a generator for random subset of a set of data with matching indices using generator. For minimizing memory usage.\n",
    "    '''\n",
    "    import itertools\n",
    "    if idempotent:\n",
    "        _, gen = itertools.tee(data_arrs_gen)\n",
    "    else: \n",
    "        gen = data_arrs_gen\n",
    "    peek = next(gen)\n",
    "    subset_selection = np.random.choice(peek.shape[0], size, replace=False)\n",
    "    \n",
    "    gen = itertools.chain([peek], gen)\n",
    "    for data_arr in gen:\n",
    "        yield data_arr[subset_selection, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, size):\n",
    "    '''\n",
    "    Get chunks from an iterable\n",
    "    '''\n",
    "    import itertools\n",
    "    it = iter(iterable)\n",
    "    return iter(lambda: tuple(itertools.islice(it, size)), ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_chunks_gen(files_to_load, n_types, size_limits=None):\n",
    "    c_gen = chunks(files_to_load, n_types)\n",
    "    it = zip(size_limits, c_gen) if size_limits else ((False, c) for c in c_gen)\n",
    "    for size_limit, chunk in it:\n",
    "        data_arr_gen = (np.load(path) for path in chunk)\n",
    "        if size_limit:\n",
    "            yield list(get_rnd_subset_gen(size_limit, data_arr_gen))\n",
    "        else:\n",
    "            yield list(data_arr_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ReverseComplementLayer\n",
    "from models import CRF\n",
    "from models import CRF_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "instanceHolder = {\"instance\": None}\n",
    "class ClassWrapper(CRF_ext):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        instanceHolder[\"instance\"] = self\n",
    "        super(ClassWrapper, self).__init__(*args, **kwargs)\n",
    "def loss(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"loss_function\")\n",
    "    return method(*args)\n",
    "def accuracy(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"accuracy\")\n",
    "    return method(*args)\n",
    "def viterbi_precision(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_precision\")\n",
    "    return method(*args)\n",
    "def f1(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_f1\")\n",
    "    return method(*args)\n",
    "def recall(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_recall\")\n",
    "    return method(*args)\n",
    "def precision(*args):\n",
    "    method = getattr(instanceHolder[\"instance\"], \"viterbi_precision\")\n",
    "    return method(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_hyper_objective(model_handle, id_name, output_path, dm,\n",
    "                           train_sequencer, train_len,\n",
    "                           valid_sequencer, valid_cls, valid_len,\n",
    "                           test_sequencer, test_x, test_cls, test_len,\n",
    "                           trials, \n",
    "                           use_generator=True,\n",
    "                           hyper_obj_pkl='hyper_obj_savepoint.pkl',\n",
    "                           subtrials_results_log='trial.log',\n",
    "                           convert_to_categorical=False,\n",
    "                           patience=20, show_metric_interval=5,\n",
    "                           best_model_h5 = 'best_model.h5', hyper_obj_log={}):\n",
    "    \n",
    "    hyper_obj_log['best_auprc_so_far'] = 0.0\n",
    "    def hyper_objective(params):\n",
    "        from hyperopt import STATUS_OK\n",
    "        import pickle\n",
    "        import os\n",
    "        if trials.statuses():\n",
    "            logging.info('Making checkpoint for hyperparameters')\n",
    "            with open(hyper_obj_pkl, 'wb') as pkl:\n",
    "                pickle.dump(trials, pkl)\n",
    "            \n",
    "        logging.info('Current parameters :\\n{}'.format('\\n'.join([str(k)+' : '+str(v) for k,v in params.items()])))\n",
    "        batch_n, epoch_n = params['batch_size'], params['epochs']\n",
    "        \n",
    "        train_steps = train_len//batch_n\n",
    "#         validation_steps = valid_len//batch_n\n",
    "        # scale down the number of validations but keep distribution\n",
    "        validation_steps = int((train_len/4)//batch_n)\n",
    "        test_steps = test_len//batch_n\n",
    "        \n",
    "        train_generator = train_sequencer(batch_n)\n",
    "        validation_generator = valid_sequencer(batch_n)\n",
    "        \n",
    "        # set up callbacks\n",
    "        monitor = params['monitor'] if 'monitor' in params else 'val_loss'\n",
    "        mode = 'max' if 'monitor' in params else 'auto'\n",
    "        logging.info(\"Current monitor mode is set to {}\".format(mode))\n",
    "        earlystopping_cb = keras.callbacks.EarlyStopping(monitor=monitor,\n",
    "                                          min_delta=0.0001,\n",
    "                                          patience=patience,\n",
    "                                          verbose=0, mode=mode)\n",
    "        stats_cb = Stats_callback(validation_generator, validation_steps,\n",
    "                                  convert_to_categorical=convert_to_categorical, \n",
    "                                  cls=valid_cls(batch_n) if convert_to_categorical else None,\n",
    "                                  show_metric_interval=show_metric_interval)\n",
    "        from keras.callbacks import ModelCheckpoint\n",
    "        checkpoint_path = os.path.join(output_path, '%s-{epoch:04d}-{val_acc:0.2f}.hdf5' % (id_name))\n",
    "        checkpoint_cb = ModelCheckpoint(checkpoint_path, monitor=monitor, verbose=1, save_best_only=True, mode=mode)\n",
    "        board_path = os.path.join(output_path, '{}_tensorboard.log'.format(id_name))\n",
    "        board_cb = keras.callbacks.TensorBoard(board_path, batch_size=batch_n)\n",
    "\n",
    "        cbs = [earlystopping_cb, stats_cb, checkpoint_cb, board_cb]\n",
    "        # check whether a model was under training\n",
    "        import os\n",
    "        epoch_restart = 0\n",
    "        load_file = ''\n",
    "        try:\n",
    "            if os.path.exists('in_progress.lck'):\n",
    "                logging.info('Continuing previous training session')\n",
    "                import re\n",
    "                regex = re.compile(r'(\\w+)-(\\d+)-(\\d*\\.\\d+)\\.hdf5', re.IGNORECASE)\n",
    "                matching_files = filter(lambda file: regex.match(file), os.listdir(output_path))\n",
    "                meta = [(regex.findall(file)[0], file) for file in matching_files]\n",
    "                # meta looks like ((prev_id, epoch, val_acc), file)\n",
    "                matching_meta = filter(lambda m: m[0][0]==id_name, meta)\n",
    "                max_meta = max(matching_meta, key=lambda m: m[0][1])\n",
    "                epoch_restart = int(max_meta[0][1])\n",
    "                logging.info('Restarting from {}'.format(epoch_restart))\n",
    "                load_file = os.path.join(output_path, max_meta[1])\n",
    "                logging.info('load file from {}'.format(load_file))\n",
    "        except ValueError:\n",
    "            logging.info('Error loading previous training. Continuing without loading')\n",
    "            \n",
    "        if load_file:\n",
    "            from keras.models import load_model\n",
    "            base = load_model(load_file, custom_objects={'ReverseComplementLayer': ReverseComplementLayer, \n",
    "                                                         \"ClassWrapper\": ClassWrapper ,\n",
    "                                                         \"CRF_ext\": ClassWrapper, \"loss\": loss, \"accuracy\":accuracy,\n",
    "                                                         \"viterbi_precision\":viterbi_precision, \"f1\":f1,\n",
    "                                                         \"recall\":recall, \"precision\":precision})\n",
    "        else:\n",
    "            base = model_handle.autofill_params(params)\n",
    "        try:\n",
    "            with open('in_progress.lck', 'w') as f:\n",
    "                pass\n",
    "            \n",
    "            if use_generator:\n",
    "                history = base.fit_generator(train_generator,\n",
    "                                             steps_per_epoch= train_steps,\n",
    "                                             epochs=epoch_n,\n",
    "                                             callbacks=cbs,\n",
    "                                             validation_data=validation_generator,\n",
    "                                             validation_steps=validation_steps,\n",
    "    #                                          use_multiprocessing=True, workers=1, # may not work on Windows\n",
    "                                             verbose=1, initial_epoch=epoch_restart)\n",
    "            else:\n",
    "                filename = id_name+'_train.hdf5'\n",
    "                import h5py\n",
    "                if not all_paths_exists(filename):\n",
    "                    x0, y0 = train_generator[0]                    \n",
    "                    logging.info('Constructing training data from metadata')\n",
    "                    _x = np.empty((batch_n*train_steps, *x0.shape[1:]))\n",
    "                    _y = np.empty((batch_n*train_steps, *y0.shape[1:]))\n",
    "                    for i in tqdm(range(train_steps)):\n",
    "                        sample = train_generator[i]\n",
    "                        _x[i*batch_n:(i+1)*batch_n] = sample[0]\n",
    "                        _y[i*batch_n:(i+1)*batch_n] = sample[1]\n",
    "                    logging.info('Constructing validation data from metadata')\n",
    "                    val_x = np.empty((batch_n*validation_steps, *x0.shape[1:]))\n",
    "                    val_y = np.empty((batch_n*validation_steps, *y0.shape[1:]))\n",
    "                    for i in tqdm(range(validation_steps)):\n",
    "                        sample = validation_generator[i]\n",
    "                        val_x[i*batch_n:(i+1)*batch_n] = sample[0]\n",
    "                        val_y[i*batch_n:(i+1)*batch_n] = sample[1]\n",
    "                    logging.info('Writing to disk')\n",
    "                    with h5py.File(filename, \"w\") as h5:\n",
    "                        h5.create_dataset('x', data=_x, compression='lzf',)\n",
    "                        h5.create_dataset('y', data=_y, compression='lzf',)\n",
    "                        h5.create_dataset('val_x', data=val_x, compression='lzf',)\n",
    "                        h5.create_dataset('val_y', data=val_y, compression='lzf',)\n",
    "                else:\n",
    "                    logging.info('Loading from disk')\n",
    "                    with h5py.File(filename, \"r\") as h5:\n",
    "                        _x = h5['x'].value\n",
    "                        _y = h5['y'].value\n",
    "                        val_x = h5['val_x'].value\n",
    "                        val_y = h5['val_y'].value\n",
    "                history = base.fit(_x,_y, batch_n, epochs=epoch_n, callbacks=cbs, validation_data=(val_x, val_y), \n",
    "                                   verbose=1, initial_epoch=epoch_restart)\n",
    "        except ValueError as e:\n",
    "            base.summary()\n",
    "            raise e\n",
    "        base.summary()\n",
    "        \n",
    "        import os\n",
    "        os.remove('in_progress.lck')\n",
    "        \n",
    "#         logging.info('Evaluating model on test set of size {}'.format(test_len))\n",
    "        \n",
    "#         test_generator = test_sequencer(batch_n)\n",
    "        \n",
    "#         score, acc = base.evaluate_generator(test_generator, \n",
    "#                                              steps=test_steps, \n",
    "#                                              verbose=1)\n",
    "        \n",
    "#         logging.info('Got score {} and accuracy {}'.format(score, acc))\n",
    "        \n",
    "        logging.info('Getting test y prediction')\n",
    "        test_pred = base.predict_generator(test_x(batch_n), steps=test_steps, verbose=1)\n",
    "        def contains_BIE(sparse):\n",
    "            for c in sparse:\n",
    "                if c==1 or c==2 or c==3:\n",
    "                    return 1\n",
    "            return 0\n",
    "        if convert_to_categorical:\n",
    "            logging.info('Converting test y prediction to categorical')\n",
    "            test_pred = test_pred.round().astype(int)\n",
    "            test_cls_pred = [ contains_BIE([ np.argmax(a) for a in each]) for each in test_pred ]\n",
    "#             maxed = [ [ np.argmax(a) for a in each] for each in test_pred]\n",
    "#             test_cls_pred =[ contains_BIE(m) for m in maxed]\n",
    "            test_pred = np.array(test_cls_pred)\n",
    "        else:\n",
    "            logging.info('Converting test y prediction to list')\n",
    "            test_pred = list(test_pred[:,0])\n",
    "        logging.info('Getting test y')\n",
    "        import itertools\n",
    "        test_real = [x for batch in itertools.islice(test_cls(batch_n),test_steps) for x in batch ]\n",
    "        \n",
    "        # make sure they are the same length\n",
    "        try:\n",
    "            assert len(test_pred) == len(test_real) \n",
    "        except:\n",
    "            logging.error('Lengths mismatch at testing; prediction {} real {} samples {}'.format(len(test_pred), len(test_real), test_len))\n",
    "            # memory usage?\n",
    "            test_pred, test_real = zip(*zip(test_pred, test_real))\n",
    "        \n",
    "        def auROC(labels, predictions):\n",
    "            return roc_auc_score(labels, predictions)\n",
    "\n",
    "        def auPRC(labels, predictions):\n",
    "            precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "            return auc(recall, precision)\n",
    "\n",
    "        def recall_at_precision(labels, predictions, precision_at):\n",
    "            threshold = 1.0-precision_at\n",
    "            precision, recall = precision_recall_curve(labels, predictions)[:2]\n",
    "            return 100 * recall[np.searchsorted(precision - threshold, 0)]\n",
    "        \n",
    "        # auROC,  auPRC, recalls\n",
    "        logging.info('Calculating auROC')\n",
    "        auroc = auROC(test_real, test_pred)\n",
    "        logging.info('auROC: {}'.format(auroc))\n",
    "        logging.info('Calculating auPRC')\n",
    "        auprc = auPRC(test_real, test_pred)\n",
    "        logging.info('auPRC: {}'.format(auprc))\n",
    "        logging.info('Calculating recall @ precisions')\n",
    "        re5, re10, re25, re50 = [recall_at_precision(test_real, test_pred, precision) for precision in [0.05,0.1,0.25,0.5]]\n",
    "        # confusion matrix \n",
    "        confusion = confusion_matrix(test_real, np.round(test_pred))\n",
    "    \n",
    "        logging.info('auROC: {} auPRC {} re@5/10/25/50: {:>7.5}/{:>7.5}/{:>7.5}/{:>7.5}'.format(auroc, auprc, re5, re10, re25, re50))\n",
    "        \n",
    "        if auprc > hyper_obj_log['best_auprc_so_far']:\n",
    "            base.save(best_model_h5)\n",
    "            hyper_obj_log['best_auprc_so_far'] = auprc\n",
    "            hyper_obj_log['best_model'] = base\n",
    "            hyper_obj_log['best_model_history'] = history\n",
    "        logging.info('Writing scores to file')\n",
    "        with open(subtrials_results_log,'a') as output:\n",
    "            base.summary(print_fn=lambda x:output.write('{}\\n'.format(x)))\n",
    "            output.write('auROC: {} auPRC {} re@5/10/25/50: {:>7.5}/{:>7.5}/{:>7.5}/{:>7.5}\\n'.format(auroc, auprc, re5, re10, re25, re50))\n",
    "            output.write('Confusion matrix:\\n{}'.format(str(confusion)))\n",
    "        return { \n",
    "            # results required by hyperopt\n",
    "            'loss': -auprc, 'status': STATUS_OK, \n",
    "            # other additional results\n",
    "            'metrics_history':stats_cb.history\n",
    "               }\n",
    "    return hyper_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_starter, parameter_space,\n",
    "              model_name, tf_name, \n",
    "              filter_file='total_regions.blacklistfiltered.merged.bed',\n",
    "              training_size_limit=None,\n",
    "              valid_size_limit=None,\n",
    "              test_size_limit=None,\n",
    "              n_trials=32,\n",
    "              patience=20,\n",
    "              use_labels_as_y=False,\n",
    "              use_generator=True,\n",
    "              output_path='.',\n",
    "              show_design=False, \n",
    "              seq_data_only=False, reduce_negative_samples=False,\n",
    "              show_metric_interval=5,\n",
    "              rerun_full=True):\n",
    "    \n",
    "    import logging\n",
    "    id_name = '{}_{}'.format(tf_name, model_name)\n",
    "    tf_folder = './{tf_name}_label_dataset'.format(tf_name=tf_name) \n",
    "\n",
    "    # setup datasets\n",
    "    logging.info('Setting up data streams')\n",
    "    import re\n",
    "    peakfile_regex = re.compile(r'chipseq\\.\\w+\\.%s\\..*\\.\\_\\.narrowpeak'%tf_name, re.IGNORECASE)\n",
    "    labelfile_regex = re.compile(r'%s\\.\\_\\.labels.tsv'%tf_name, re.IGNORECASE)\n",
    "    for file in os.listdir('.'):\n",
    "        if peakfile_regex.match(file):\n",
    "            peakfile = file\n",
    "        elif labelfile_regex.match(file):\n",
    "            label_file = file\n",
    "    try:\n",
    "        logging.info('Creating data manager for peakfile {} labelfile {}'.format(peakfile, label_file))\n",
    "        dm = DataManager(label_file, peakfile, output_dir='datasets', \n",
    "                        move_finished_src='finished_peakfiles', \n",
    "                        filter_file=filter_file,\n",
    "                        reduce_negative_samples=reduce_negative_samples, # check_set_ratio=9, # use for reduce_negative_samples=True\n",
    "                        )\n",
    "        dm.samples # make sure samples are already created\n",
    "        generators = dm.create_datagen_from_samples(useDNAshapeR=False)\n",
    "#         train_x, train_label, train_seq, train_cls, train_labels_sequencer, train_cls_sequencer = generators[:6]\n",
    "        train_labels_sequencer, train_cls_sequencer = generators[:2]\n",
    "        valid_x, valid_label, valid_seq, valid_cls, valid_labels_sequencer, valid_cls_sequencer = generators[2:8]\n",
    "        test_x, test_label, test_seq, test_cls, test_labels_sequencer, test_cls_sequencer = generators[8:]\n",
    "    except InsufficientChromosomesException as e:\n",
    "        logging.warning('Skipping creating data for {}'.format(peakfile))\n",
    "        logging.warning('Error message: {}'.format(str(e)))\n",
    "    \n",
    "    if seq_data_only:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # create model\n",
    "    \n",
    "    # select Tree Parzen Estimator \n",
    "    # check this for comparison with other methods\n",
    "    # http://www.cs.ubc.ca/~hutter/papers/13-BayesOpt_EmpiricalFoundation.pdf\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    \n",
    "    # store track progress\n",
    "    hyper_obj_pkl = '{}_hyper_obj_savepoint.pkl'.format(id_name) \n",
    "    hyper_obj_pkl = os.path.join(output_path, hyper_obj_pkl)\n",
    "    if not os.path.exists(hyper_obj_pkl):\n",
    "        tpe_progress = Trials()\n",
    "    else:\n",
    "        logging.info('Loading available previous hyperparameters settings')\n",
    "        try:\n",
    "            with open(hyper_obj_pkl, 'rb') as pkl:\n",
    "                tpe_progress = pickle.load(pkl)\n",
    "                print('tpe statuses {}'.format(tpe_progress.statuses()))\n",
    "                if 'new' == tpe_progress.statuses()[0]:\n",
    "                    tpe_progress = Trials()\n",
    "        except ValueError:\n",
    "            logging.error('Loading previous hyperparameters failed. Creating new set')\n",
    "            tpe_progress = Trials()\n",
    "    \n",
    "    trial_log_file = '{}_trials.log'.format(id_name)\n",
    "    trial_log_file = os.path.join(output_path, trial_log_file)\n",
    "    \n",
    "    model_handle = model_starter((dm.sample_length, dm.feature_dimensions))\n",
    "    best_model_h5 = os.path.join(output_path, '{}_best_model.h5'.format(id_name)) \n",
    "    hyper_obj_log = {}\n",
    "    logging.info('Setting up hyperopt objective')\n",
    "    if use_labels_as_y:\n",
    "        objective = create_hyper_objective(model_handle, id_name, output_path, dm,\n",
    "                                           train_labels_sequencer, \n",
    "                                           dm.dataset2counts_pos['train'] * 2,\n",
    "                                           valid_labels_sequencer,\n",
    "                                           valid_cls,\n",
    "                                           dm.dataset2counts_pos['valid'] + dm.dataset2counts_neg['valid'],\n",
    "                                           test_labels_sequencer, test_x, test_cls,\n",
    "                                           dm.dataset2counts_pos['test'] + dm.dataset2counts_neg['test'],\n",
    "                                           tpe_progress,\n",
    "                                           use_generator,\n",
    "                                           hyper_obj_pkl,\n",
    "                                           trial_log_file,\n",
    "                                           convert_to_categorical = True,\n",
    "                                           patience=patience, show_metric_interval=show_metric_interval,\n",
    "                                           best_model_h5=best_model_h5, hyper_obj_log=hyper_obj_log)\n",
    "    else:\n",
    "        objective = create_hyper_objective(model_handle, id_name, output_path, dm,\n",
    "                                           train_cls_sequencer,\n",
    "                                           dm.dataset2counts_pos['train'] * 2,\n",
    "                                           valid_cls_sequencer, \n",
    "                                           valid_cls,\n",
    "                                           dm.dataset2counts_pos['valid'] + dm.dataset2counts_neg['valid'],\n",
    "                                           test_cls_sequencer, test_x, test_cls,\n",
    "                                           dm.dataset2counts_pos['test'] + dm.dataset2counts_neg['test'],\n",
    "                                           tpe_progress,\n",
    "                                           use_generator,\n",
    "                                           hyper_obj_pkl,  \n",
    "                                           trial_log_file,\n",
    "                                           convert_to_categorical = False,\n",
    "                                           patience=patience, show_metric_interval=show_metric_interval,\n",
    "                                           best_model_h5=best_model_h5, hyper_obj_log=hyper_obj_log)        \n",
    "    # requires networkx1.11 newer versions may not work \n",
    "    optimum = fmin(objective, parameter_space, tpe_algorithm, trials=tpe_progress, max_evals=n_trials)\n",
    "    \n",
    "    try:\n",
    "        if show_design:\n",
    "            from keras.utils.vis_utils import plot_model\n",
    "            logging.info('drawing model architecture')\n",
    "            if type(show_design)==str:\n",
    "                plot_model(model_handle.model, show_design)\n",
    "            else: \n",
    "                plot_model(model_handle.model, '{}_arch.png'.format(model_handle.model_factory.__name__))\n",
    "    except:\n",
    "        logging.warning(\"Not drawing model for {}\".format(model_name))\n",
    "    \n",
    "    import hyperopt\n",
    "    best_params = hyperopt.space_eval(parameter_space, optimum)\n",
    "    best_score = -tpe_progress.best_trial['result']['loss']\n",
    "    metrics_history = tpe_progress.best_trial['result']['metrics_history']\n",
    "    \n",
    "    with open(os.path.join(output_path, 'test_%s.txt'%(id_name)), 'a') as out:\n",
    "        out.write(\"Best: {} using {}\\n\".format(best_score, str(best_params)))\n",
    "        out.write(\"Metrics history\\n\")\n",
    "        n = len(metrics_history)\n",
    "        out.write(('{:>20} '* n +'\\n').format(*metrics_history.keys()))\n",
    "        for e in zip(*metrics_history.values()):\n",
    "            out.write (('{:>20} '* n +'\\n').format(*e))\n",
    "            \n",
    "    if 'best_model_history' in hyper_obj_log: # doesn't exist if no trials are run\n",
    "        best_model_history = hyper_obj_log['best_model_history']\n",
    "    \n",
    "        import history_plot\n",
    "        history_plot.plot_history(best_model_history, os.path.join(output_path, id_name))\n",
    "    \n",
    "    \n",
    "    # TODO: generate cb generators for both this and create_hyper_objective\n",
    "    \n",
    "    \n",
    "    #### Rerun best model from tpe trials ####\n",
    "    if not rerun_full:\n",
    "        return\n",
    "    \n",
    "    \n",
    "#     recreation = model_handle.autofill_params(best_params)\n",
    "#     recreation.summary() \n",
    "    \n",
    "#     logging.info('Running full training')\n",
    "    \n",
    "#     logging.info('Setting up to load full data')\n",
    "#     size_limit =  None\n",
    "#     dataset_gen = load_data_chunks_gen(paths_to_load, file_types, size_limit)\n",
    "\n",
    "#     # load datset\n",
    "#     train_x, train_seq, train_cls, train_label = next(dataset_gen)\n",
    "#     logging.info('Loaded training data')\n",
    "#     valid_x, valid_seq, valid_cls, valid_label = next(dataset_gen)\n",
    "#     logging.info('Loaded validation data')\n",
    "#     test_x, test_seq, test_cls, test_label = next(dataset_gen)\n",
    "#     logging.info('Loaded test data')\n",
    "#     if seq_data_only:\n",
    "#         train_x = train_x[:, :, :4]\n",
    "#         valid_x = valid_x[:, :, :4]\n",
    "#         test_x = test_x[:, :, :4]\n",
    "    \n",
    "#     earlystopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                       min_delta=0.0001,\n",
    "#                                       patience=patience,\n",
    "#                                       verbose=0, mode='auto')\n",
    "#     statistics = Stats_callback(valid_cls, convert_to_categorical=True)\n",
    "\n",
    "#     from keras.callbacks import ModelCheckpoint\n",
    "#     checkpoint_path = '%s-{epoch:04d}-{val_acc:0.2f}.hdf5' % (id_name)\n",
    "#     checkpoint_path = os.path.join(output_path, checkpoint_path)\n",
    "#     checkpoint_cb = ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=1, save_best_only=True)\n",
    "#     board_path = os.path.join(output_path, '{}_tensorboard.log'.format(id_name))\n",
    "#     board_cb = keras.callbacks.TensorBoard(board_path, batch_size=best_params['batch_size'])\n",
    "    \n",
    "#     cbs = [earlystopping_cb, statistics, checkpoint_cb]\n",
    "#     # TODO: add load from checkpoints\n",
    "#     if use_labels_as_y:\n",
    "#         history = recreation.fit(train_x, train_label, \n",
    "#                              epochs=best_params['epochs'],\n",
    "#                              batch_size=best_params['batch_size'],\n",
    "#                              validation_data=(valid_x, valid_label),\n",
    "#                              callbacks=cbs)\n",
    "#         test_metric_out = recreation.evaluate(test_x, test_label)\n",
    "#     else:\n",
    "#         history = recreation.fit(train_x, train_cls, \n",
    "#                              epochs=best_params['epochs'],\n",
    "#                              batch_size=best_params['batch_size'],\n",
    "#                              validation_data=(valid_x, valid_cls),\n",
    "#                              callbacks=cbs)\n",
    "#         test_metric_out = recreation.evaluate(test_x, test_cls)\n",
    "        \n",
    "#     with open(os.path.join(output_path, 'test_metrics'), 'w') as test_metrics:\n",
    "#         try:\n",
    "#             for metric_name, metric_out in zip(recreation.metrics_names, test_metric_out):\n",
    "#                 test_metrics.write('{:30}:{:>30}\\n'.format(metric_name, metric_out))\n",
    "#         except:\n",
    "#             test_metrics.write('{:30}:{:>30}\\n'.format('Test loss', test_metric_out))\n",
    "#     # plot history\n",
    "#     import history_plot\n",
    "#     history_plot.plot_history(history, os.path.join(output_path, id_name))\n",
    "\n",
    "#     # TODO: plot other things\n",
    "    \n",
    "#     def plot_curves(x, y, use_labels_as_y=False):\n",
    "#         from sklearn.metrics import roc_curve\n",
    "#         from sklearn.metrics import auc\n",
    "\n",
    "#         y_pred = recreation.predict(x)\n",
    "#         if use_labels_as_y:\n",
    "#             maxed = [ [ np.argmax(a) for a in each] for each in y_pred]\n",
    "#             cls_pred =[ 1 if (1 in m or 2 in m or 3 in m) else 0 for m in maxed]\n",
    "#             _y_pred = np.array(cls_pred)\n",
    "#         else:\n",
    "#             _y_pred = y_pred\n",
    "\n",
    "#         _y = y\n",
    "    \n",
    "#         fpr, tpr, thresholds = roc_curve(_y, _y_pred)\n",
    "#         auc_model = auc(fpr, tpr)\n",
    "    \n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure(1)\n",
    "#         plt.plot([0, 1], [0, 1], 'k--')\n",
    "#         plt.plot(fpr, tpr, label='{} (area = {:.3f})'.format(model_name, auc_model))\n",
    "#         plt.xlabel('False positive rate')\n",
    "#         plt.ylabel('True positive rate')\n",
    "#         plt.title('ROC curve for {}'.format(model_name))\n",
    "#         plt.legend(loc='best')\n",
    "#         figure_path = os.path.join(output_path, id_name+'_ROCAUC.png')\n",
    "#         plt.savefig(figure_path)\n",
    "#     plot_curves(valid_x, valid_cls, use_labels_as_y=use_labels_as_y)\n",
    "    \n",
    "#     # save final model\n",
    "#     final_model_path = os.path.join(output_path, id_name+\"_final.h5\")\n",
    "#     recreation.save(final_model_path)\n",
    "    \n",
    "#     with open(os.path.join(output_path, '{}_final_history.pkl'.format(id_name)), 'wb') as pkl:\n",
    "#         pickle.dump(statistics.history, pkl)\n",
    "#     with open(os.path.join(output_path, '{}_final_history.txt'.format(id_name)), 'w') as out:\n",
    "#         out.write(\"Metrics history\\n\")\n",
    "#         out.write('{:>20} {:>20} {:>20} {:>20}\\n'.format(*statistics.history.keys()))\n",
    "#         for e in zip(*statistics.history.values()):\n",
    "#             out.write ('{:>20} {:>20} {:>20} {:>20}\\n'.format(*e))\n",
    "            \n",
    "#     return statistics, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from model_handler import *\n",
    "\n",
    "#hyperparameter optimizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pickle\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# ROC scores\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = []\n",
    "labelled_test_models = []\n",
    "patience=10\n",
    "n_trials=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.cnn_cnn\n",
    "_model_name = 'cnn_cnn'\n",
    "\n",
    "search_space = {\n",
    "    'cnn1_n_filters': hp.choice('cnn1_n_filters', [8,16]),\n",
    "    'cnn1_kernel_size': hp.choice('cnn1_kernel_size', [8,16,32]),\n",
    "    'cnn2_n_filters': hp.choice('cnn2_n_filters', [4,8]),\n",
    "    'cnn2_kernel_size': None,\n",
    "    'do_rate': hp.choice('do_rate', [0.1,0.3,0.5]),\n",
    "    'dense_size': hp.choice('dense_size', [128 , 256, 1024]),\n",
    "    \n",
    "    'optimizer': hp.choice('optimizer', [Adam()]),\n",
    "    'batch_size': 256, 'epochs': 200,\n",
    "}\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.bilstm_crf\n",
    "_model_name = 'bilstm_crf'\n",
    "\n",
    "search_space = { \n",
    "    'lstm_size': hp.choice('lstm_size', [16,32,64]),\n",
    "    'bi_do_rate': hp.choice('bi_do_rate', [0.1,0.25,0.5]),\n",
    "    'rec_do_rate': hp.choice('rec_do_rate', [0.1,0.25,0.5]),\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.cnn_bilstm_crf\n",
    "_model_name = 'cnn_bilstm_crf'\n",
    "\n",
    "search_space = { \n",
    "    'cnn_kernel_size': hp.choice('cnn_kernel_size', [26]),\n",
    "    'cnn_n_filters': hp.choice('cnn_n_filters', [32]),\n",
    "    'lstm_size': hp.choice('lstm_size', [16,32,64]),\n",
    "    'crf_size': hp.choice('crf_size', [4]),\n",
    "    'do_rate': hp.choice('do_rate', [0.1,0.25,0.5]),\n",
    "    'd1_size' : hp.choice('d1_size', [4]),\n",
    "    \n",
    "\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.cnn_cnn_lstm\n",
    "_model_name = 'cnn_cnn_lstm'\n",
    "\n",
    "search_space = {\n",
    "    'cnn1_n_filters': hp.choice('cnn1_n_filters', [8,16,32]),\n",
    "    'cnn1_kernel_size': hp.choice('cnn1_kernel_size', [8,16,32]),\n",
    "    'cnn2_n_filters': hp.choice('cnn2_n_filters', [4,8]),\n",
    "    'do_rate': hp.choice('do_rate', [0.1,0.3,0.5]),\n",
    "    'lstm_size': hp.choice('lstm_size', [4,8,16,32]),\n",
    "    'dense_size': hp.choice('dense_size', [128 , 256, 1024]),\n",
    "    \n",
    "    'optimizer': hp.choice('optimizer', [Adam()]),\n",
    "    'batch_size': 256, 'epochs': 200,\n",
    "}\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.cnn_cnn_bilstm\n",
    "_model_name = 'cnn_cnn_bilstm'\n",
    "\n",
    "search_space = {\n",
    "    'cnn1_n_filters': hp.choice('cnn1_n_filters', [8,16,32]),\n",
    "    'cnn1_kernel_size': hp.choice('cnn1_kernel_size', [8,16,32]),\n",
    "    'cnn2_n_filters': hp.choice('cnn2_n_filters', [4,8]),\n",
    "    'do_rate': hp.choice('do_rate', [0.1,0.3,0.5]),\n",
    "    'lstm_size': hp.choice('lstm_size', [4,8,16,32]),\n",
    "    'dense_size': hp.choice('dense_size', [128 , 256, 1024]),\n",
    "    \n",
    "    'optimizer': hp.choice('optimizer', [Adam()]),\n",
    "    'batch_size': 256, 'epochs': 200,\n",
    "}\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_model_choice = Models.cnn_cnn_dnn\n",
    "_model_name = 'cnn_cnn_dnn'\n",
    "\n",
    "search_space = {\n",
    "    'cnn1_n_filters': hp.choice('cnn1_n_filters', [8,16,32]),\n",
    "    'cnn1_kernel_size': hp.choice('cnn1_kernel_size', [8,16,32]),\n",
    "    'cnn2_n_filters': hp.choice('cnn2_n_filters', [4,8]),\n",
    "    'do_rate': hp.choice('do_rate', [0.1,0.3,0.5]),\n",
    "    'd1_size': hp.choice('d1_size', [64,128, 256]),\n",
    "    'd2_size': hp.choice('d2_size', [16, 32]),\n",
    "    'd3_size': hp.choice('d3_size', [4,8]),\n",
    "    \n",
    "    'dense_size': hp.choice('dense_size', [8,16,32]),\n",
    "    \n",
    "    'optimizer': hp.choice('optimizer', [Adam()]),\n",
    "    'batch_size': 256, 'epochs': 200,\n",
    "}\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.cnn\n",
    "_model_name = 'deepbind'\n",
    "\n",
    "search_space = { \n",
    "    'cnn1_n_filters':hp.choice('cnn_n_filters', [3]), \n",
    "    'cnn1_kernel_size':hp.choice('cnn_kernel_size', [26]),\n",
    "    'do_rate':hp.choice('do_rate', [0.2, 0.5]),\n",
    "    'dense_size':hp.choice('dense_size', [1024]),\n",
    "        \n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "}\n",
    "\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.cnn_cnn\n",
    "_model_name = 'deepbind_plus'\n",
    "\n",
    "search_space = { \n",
    "    'cnn1_n_filters':hp.choice('cnn_n_filters', [3]), \n",
    "    'cnn1_kernel_size':hp.choice('cnn_kernel_size', [26]),\n",
    "    'do_rate':hp.choice('do_rate', [0.2, 0.5]),\n",
    "    'dense_size':hp.choice('dense_size', [1024]),\n",
    "        \n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "}\n",
    "\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.bigru_crf\n",
    "_model_name = 'bigru_crf'\n",
    "\n",
    "search_space = { \n",
    "    'lstm_size': hp.choice('lstm_size', [64]),\n",
    "    'bi_do_rate': hp.choice('bi_do_rate', [0.1]),\n",
    "    'rec_do_rate': hp.choice('rec_do_rate', [0.5]),\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "    'monitor': 'val_f1'\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.s_bigru_crf\n",
    "_model_name = 's_bigru_crf'\n",
    "\n",
    "search_space = { \n",
    "    'lstm_size': hp.choice('lstm_size', [64]),\n",
    "    'bi_do_rate': hp.choice('bi_do_rate', [0.1]),\n",
    "    'rec_do_rate': hp.choice('rec_do_rate', [0.1]),\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 1024, 'epochs': 200,\n",
    "    'monitor': 'val_f1'\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.s_bilstm_crf\n",
    "_model_name = 's_bilstm_crf'\n",
    "\n",
    "search_space = { \n",
    "    'lstm_size': hp.choice('lstm_size', [64]),\n",
    "    'bi_do_rate': hp.choice('bi_do_rate', [0.1]),\n",
    "    'rec_do_rate': hp.choice('rec_do_rate', [0.5]),\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "    'monitor': 'val_f1'\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.s_cnn_bigru_crf\n",
    "_model_name = 's_cnn_bigru_crf'\n",
    "\n",
    "search_space = { \n",
    "    'cnn_kernel_size': hp.choice('cnn_kernel_size', [26]),\n",
    "    'cnn_n_filters': hp.choice('cnn_n_filters', [32]),\n",
    "    'lstm_size': hp.choice('lstm_size', [64]),\n",
    "    'crf_size': hp.choice('crf_size', [4]),\n",
    "    'do_rate': hp.choice('do_rate', [0.5]),\n",
    "    'd1_size' : hp.choice('d1_size', [4]),\n",
    "    \n",
    "\n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "    'monitor': 'val_f1'\n",
    "}\n",
    "\n",
    "labelled_test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_choice = Models.factor_net\n",
    "_model_name = 'factor_net'\n",
    "\n",
    "search_space = { \n",
    "    'cnn_kernel_size': hp.choice('cnn_kernel_size', [26]),\n",
    "    'cnn_n_filters': hp.choice('cnn_n_filters', [32]),\n",
    "    'lstm_size': hp.choice('lstm_size', [64]),\n",
    "    'do_rate': hp.choice('do_rate', [0.5]),\n",
    "    \n",
    "    'optimizer': hp.choice('optimizer', [Adam(lr=0.0001)]),\n",
    "    'batch_size': 512, 'epochs': 200,\n",
    "}\n",
    "\n",
    "test_models.append((_model_choice, _model_name, search_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tf names from datasets available \n",
    "tf_names = ['_'.join(f.split('_')[:2]) for f in os.listdir('.') if f.endswith('_dataset') and len(f.split('_'))==4]\n",
    "tf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Working on FOXA2\n",
      "INFO:root:Testing bigru_crf\n",
      "INFO:root:Setting up data streams\n",
      "INFO:root:Creating data manager for peakfile ChIPseq.liver.FOXA2.conservative._.narrowPeak labelfile FOXA2._.labels.tsv\n",
      "INFO:root:Creating positive/negative samples\n",
      "INFO:root:Loading pickled gen_hg19\n",
      "INFO:root:Finished setting up gen_hg19\n",
      "INFO:root:The function if_not_pickled took 05 seconds\n",
      "INFO:root:No pickle for gen_chr2locNbound is found. Generating anew.\n",
      "  0%|                                                                                     | 0/60519749 [00:00<?, ?it/s]INFO:root:Working on chr10\n",
      "\n",
      "  4%|                                                                | 2697574/60519749 [00:13<04:46, 202105.54it/s]INFO:root:Working on chr11\n",
      "\n",
      "  9%|                                                             | 5356472/60519749 [00:26<04:34, 200595.89it/s]INFO:root:Working on chr12\n",
      "\n",
      " 13%|                                                          | 8016793/60519749 [00:40<04:24, 198646.12it/s]INFO:root:Working on chr13\n",
      "\n",
      " 17%|                                                      | 10326544/60519749 [00:52<04:14, 197089.49it/s]INFO:root:Working on chr14\n",
      "\n",
      " 21%|                                                    | 12469386/60519749 [01:03<04:05, 195423.25it/s]INFO:root:Working on chr15\n",
      "\n",
      " 24%|                                                  | 14520687/60519749 [01:15<03:57, 193555.36it/s]INFO:root:Working on chr16\n",
      "\n",
      " 27%|                                                | 16328247/60519749 [01:24<03:49, 192600.49it/s]INFO:root:Working on chr17\n",
      "\n",
      " 30%|                                              | 17940799/60519749 [01:33<03:42, 191430.81it/s]INFO:root:Working on chr18\n",
      "\n",
      " 32%|                                            | 19502668/60519749 [01:42<03:35, 190323.22it/s]INFO:root:Working on chr19\n",
      "\n",
      " 34%|                                           | 20671498/60519749 [01:49<03:31, 188744.84it/s]INFO:root:Working on chr2\n",
      "\n",
      " 42%|                                      | 25521176/60519749 [02:17<03:08, 185542.84it/s]INFO:root:Working on chr20\n",
      "\n",
      " 44%|                                    | 26792766/60519749 [02:24<03:02, 184962.52it/s]INFO:root:Working on chr22\n",
      "\n",
      " 46%|                                   | 27800529/60519749 [02:30<02:57, 184114.17it/s]INFO:root:Working on chr3\n",
      "\n",
      " 52%|                               | 31764139/60519749 [02:55<02:39, 180687.28it/s]INFO:root:Working on chr4\n",
      "\n",
      " 59%|                           | 35577614/60519749 [03:19<02:20, 178058.17it/s]INFO:root:Working on chr5\n",
      "\n",
      " 65%|                       | 39182604/60519749 [03:43<02:01, 175500.19it/s]INFO:root:Working on chr6\n",
      "\n",
      " 70%|                   | 42607221/60519749 [04:05<01:43, 173646.44it/s]INFO:root:Working on chr7\n",
      "\n",
      " 76%|                | 45757642/60519749 [04:26<01:26, 171632.93it/s]INFO:root:Working on chr9\n",
      "\n",
      " 80%|             | 48565594/60519749 [04:45<01:10, 170125.54it/s]INFO:root:Working on chrX\n",
      "\n",
      " 85%|         | 51672678/60519749 [05:07<00:52, 168138.01it/s]INFO:root:Working on chr1\n",
      "\n",
      " 94%|    | 56639611/60519749 [05:43<00:23, 164788.31it/s]INFO:root:Working on chr21\n",
      "\n",
      " 95%|   | 57613041/60519749 [05:51<00:17, 164116.13it/s]INFO:root:Working on chr8\n",
      "\n",
      "100%|| 60519748/60519749 [06:11<00:00, 162726.37it/s]\n",
      "INFO:root:The function gen_chr2locNbound took 06 minutes23 seconds\n",
      "INFO:root:Finished setting up gen_chr2locNbound\n",
      "INFO:root:The function if_not_pickled took 07 minutes16 seconds\n",
      "INFO:root:No pickle for gen_chr2filter_locs is found. Generating anew.\n",
      "INFO:root:The function gen_chr2filter_locs took     0.0176s\n",
      "INFO:root:Finished setting up gen_chr2filter_locs\n",
      "INFO:root:The function if_not_pickled took     0.0284s\n",
      "INFO:root:No pickle for gen_chr2locNpeaks is found. Generating anew.\n",
      "100%|| 28649/28649 [00:01<00:00, 15326.96it/s]\n",
      "INFO:root:The function gen_chr2locNpeaks took 02 seconds\n",
      "INFO:root:Finished setting up gen_chr2locNpeaks\n",
      "INFO:root:The function if_not_pickled took 02 seconds\n",
      "INFO:root:No pickle for gen_chr2labelsNseq is found. Generating anew.\n",
      "INFO:root:Generating labels and seq for this set of chromosomes {'chr19', 'chr4', 'chr16', 'chr9', 'chrX', 'chr18', 'chr20', 'chr8', 'chr22', 'chr21', 'chr11', 'chr10', 'chr17', 'chr7', 'chr15', 'chr12', 'chr6', 'chr14', 'chr1', 'chr13', 'chr5', 'chr2', 'chr3'}\n",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]INFO:root:Generating labels and seq for chr19\n",
      "INFO:root:Found 4125/4125 seqs for bound locations\n",
      "INFO:root:Found 1149404/1149404 seqs for bound locations\n",
      "  4%|                                                                               | 1/23 [00:05<02:06,  5.77s/it]INFO:root:Generating labels and seq for chr4\n",
      "INFO:root:Found 9213/9213 seqs for bound locations\n",
      "INFO:root:Found 3766596/3766596 seqs for bound locations\n",
      "  9%|                                                                           | 2/23 [00:26<04:42, 13.44s/it]INFO:root:Generating labels and seq for chr16\n",
      "INFO:root:Found 5902/5902 seqs for bound locations\n",
      "INFO:root:Found 1775070/1775070 seqs for bound locations\n",
      " 13%|                                                                        | 3/23 [00:35<03:54, 11.71s/it]INFO:root:Generating labels and seq for chr9\n",
      "INFO:root:Found 8216/8216 seqs for bound locations\n",
      "INFO:root:Found 2783355/2783355 seqs for bound locations\n",
      " 17%|                                                                    | 4/23 [00:52<04:09, 13.14s/it]INFO:root:Generating labels and seq for chrX\n",
      "INFO:root:Found 4200/4200 seqs for bound locations\n",
      "INFO:root:Found 3072391/3072391 seqs for bound locations\n",
      " 22%|                                                                 | 5/23 [01:06<03:58, 13.23s/it]INFO:root:Generating labels and seq for chr18\n",
      "INFO:root:Found 4412/4412 seqs for bound locations\n",
      "INFO:root:Found 1541026/1541026 seqs for bound locations\n",
      " 26%|                                                             | 6/23 [01:13<03:27, 12.18s/it]INFO:root:Generating labels and seq for chr20\n",
      "INFO:root:Found 4819/4819 seqs for bound locations\n",
      "INFO:root:Found 1239477/1239477 seqs for bound locations\n",
      " 30%|                                                         | 7/23 [01:20<03:03, 11.46s/it]INFO:root:Generating labels and seq for chr8\n",
      "INFO:root:Found 8931/8931 seqs for bound locations\n",
      "INFO:root:Found 2864984/2864984 seqs for bound locations\n",
      " 35%|                                                      | 8/23 [01:39<03:07, 12.49s/it]INFO:root:Generating labels and seq for chr22\n",
      "INFO:root:Found 3114/3114 seqs for bound locations\n",
      "INFO:root:Found 1014061/1014061 seqs for bound locations\n",
      " 39%|                                                  | 9/23 [01:45<02:44, 11.75s/it]INFO:root:Generating labels and seq for chr21\n",
      "INFO:root:Found 2139/2139 seqs for bound locations\n",
      "INFO:root:Found 948515/948515 seqs for bound locations\n",
      " 43%|                                              | 10/23 [01:49<02:21, 10.91s/it]INFO:root:Generating labels and seq for chr11\n",
      "INFO:root:Found 8710/8710 seqs for bound locations\n",
      "INFO:root:Found 2634020/2634020 seqs for bound locations\n",
      " 48%|                                          | 11/23 [02:06<02:18, 11.51s/it]INFO:root:Generating labels and seq for chr10\n",
      "INFO:root:Found 8916/8916 seqs for bound locations\n",
      "INFO:root:Found 2662693/2662693 seqs for bound locations\n",
      " 52%|                                       | 12/23 [02:27<02:14, 12.25s/it]INFO:root:Generating labels and seq for chr17\n",
      "INFO:root:Found 6820/6820 seqs for bound locations\n",
      "INFO:root:Found 1594569/1594569 seqs for bound locations\n",
      " 57%|                                   | 13/23 [02:37<02:00, 12.09s/it]INFO:root:Generating labels and seq for chr7\n",
      "INFO:root:Found 9570/9570 seqs for bound locations\n",
      "INFO:root:Found 3108363/3108363 seqs for bound locations\n",
      " 61%|                                | 14/23 [02:59<01:55, 12.83s/it]INFO:root:Generating labels and seq for chr15\n",
      "INFO:root:Found 5830/5830 seqs for bound locations\n",
      "INFO:root:Found 2023873/2023873 seqs for bound locations\n",
      " 65%|                            | 15/23 [03:13<01:43, 12.92s/it]INFO:root:Generating labels and seq for chr12\n",
      "INFO:root:Found 9150/9150 seqs for bound locations\n",
      "INFO:root:Found 2616911/2616911 seqs for bound locations\n",
      " 70%|                         | 16/23 [03:33<01:33, 13.33s/it]INFO:root:Generating labels and seq for chr6\n",
      "INFO:root:Found 11995/11995 seqs for bound locations\n",
      "INFO:root:Found 3363592/3363592 seqs for bound locations\n",
      " 74%|                     | 17/23 [04:01<01:25, 14.20s/it]INFO:root:Generating labels and seq for chr14\n",
      "INFO:root:Found 5540/5540 seqs for bound locations\n",
      "INFO:root:Found 2121292/2121292 seqs for bound locations\n",
      " 78%|                 | 18/23 [04:16<01:11, 14.23s/it]INFO:root:Generating labels and seq for chr1\n",
      "INFO:root:Found 17516/17516 seqs for bound locations\n",
      "INFO:root:Found 4903392/4903392 seqs for bound locations\n",
      " 83%|              | 19/23 [05:06<01:04, 16.13s/it]INFO:root:Generating labels and seq for chr13\n",
      "INFO:root:Found 5043/5043 seqs for bound locations\n",
      "INFO:root:Found 2279778/2279778 seqs for bound locations\n",
      " 87%|          | 20/23 [05:21<00:48, 16.07s/it]INFO:root:Generating labels and seq for chr5\n",
      "INFO:root:Found 10008/10008 seqs for bound locations\n",
      "INFO:root:Found 3553788/3553788 seqs for bound locations\n",
      " 91%|       | 21/23 [05:50<00:33, 16.68s/it]INFO:root:Generating labels and seq for chr2\n",
      "INFO:root:Found 15199/15199 seqs for bound locations\n",
      "INFO:root:Found 4787581/4787581 seqs for bound locations\n",
      " 96%|   | 22/23 [06:38<00:18, 18.11s/it]INFO:root:Generating labels and seq for chr3\n",
      "INFO:root:Found 12890/12890 seqs for bound locations\n",
      "INFO:root:Found 3895205/3895205 seqs for bound locations\n",
      "100%|| 23/23 [07:14<00:00, 18.90s/it]\n",
      "INFO:root:The function gen_chr2labelsNseq took 07 minutes14 seconds\n",
      "INFO:root:Finished setting up gen_chr2labelsNseq\n",
      "INFO:root:The function if_not_pickled took 08 minutes48 seconds\n",
      "  0%|                                                                                           | 0/23 [00:00<?, ?it/s]INFO:root:Working on chr19\n",
      "INFO:root:Found 1087192 samples for chr19; 4125 positive and 1083067 negative\n",
      "INFO:root:Found 1087192 samples for chr19; 4125 positive and 1083067 negative\n",
      "  4%|                                                                               | 1/23 [00:01<00:42,  1.92s/it]INFO:root:Working on chr10\n",
      "INFO:root:Found 2587233 samples for chr10; 8916 positive and 2578317 negative\n",
      "INFO:root:Found 2587233 samples for chr10; 8916 positive and 2578317 negative\n",
      "  9%|                                                                           | 2/23 [00:06<01:08,  3.25s/it]INFO:root:Working on chr4\n",
      "INFO:root:Found 3706077 samples for chr4; 9213 positive and 3696864 negative\n",
      "INFO:root:Found 3706077 samples for chr4; 9213 positive and 3696864 negative\n",
      " 13%|                                                                        | 3/23 [00:13<01:26,  4.35s/it]INFO:root:Working on chr16\n",
      "INFO:root:Found 1551595 samples for chr16; 5902 positive and 1545693 negative\n",
      "INFO:root:Found 1551595 samples for chr16; 5902 positive and 1545693 negative\n",
      " 17%|                                                                    | 4/23 [00:16<01:16,  4.03s/it]INFO:root:Working on chr21\n",
      "INFO:root:Found 690168 samples for chr21; 2138 positive and 688030 negative\n",
      "INFO:root:Found 690168 samples for chr21; 2138 positive and 688030 negative\n",
      " 22%|                                                                 | 5/23 [00:17<01:03,  3.53s/it]INFO:root:Working on chr17\n",
      "INFO:root:Found 1533369 samples for chr17; 6820 positive and 1526549 negative\n",
      "INFO:root:Found 1533369 samples for chr17; 6820 positive and 1526549 negative\n",
      " 26%|                                                             | 6/23 [00:20<00:58,  3.41s/it]INFO:root:Working on chr13\n",
      "INFO:root:Found 1893225 samples for chr13; 5043 positive and 1888182 negative\n",
      "INFO:root:Found 1893225 samples for chr13; 5043 positive and 1888182 negative\n",
      " 30%|                                                         | 7/23 [00:24<00:55,  3.48s/it]INFO:root:Working on chr7\n",
      "INFO:root:Found 3046242 samples for chr7; 9570 positive and 3036672 negative\n",
      "INFO:root:Found 3046242 samples for chr7; 9570 positive and 3036672 negative\n",
      " 35%|                                                      | 8/23 [00:29<00:56,  3.74s/it]INFO:root:Working on chr9\n",
      "INFO:root:Found 2370084 samples for chr9; 8216 positive and 2361868 negative\n",
      "INFO:root:Found 2370084 samples for chr9; 8216 positive and 2361868 negative\n",
      " 39%|                                                  | 9/23 [00:34<00:54,  3.86s/it]INFO:root:Working on chr15\n",
      "INFO:root:Found 1612959 samples for chr15; 5830 positive and 1607129 negative\n",
      "INFO:root:Found 1612959 samples for chr15; 5830 positive and 1607129 negative\n",
      " 43%|                                              | 10/23 [00:38<00:49,  3.81s/it]INFO:root:Working on chr12\n",
      "INFO:root:Found 2558638 samples for chr12; 9150 positive and 2549488 negative\n",
      "INFO:root:Found 2558638 samples for chr12; 9150 positive and 2549488 negative\n",
      " 48%|                                          | 11/23 [00:42<00:46,  3.89s/it]INFO:root:Working on chrX\n",
      "INFO:root:Found 2993149 samples for chrX; 4200 positive and 2988949 negative\n",
      "INFO:root:Found 2993149 samples for chrX; 4200 positive and 2988949 negative\n",
      " 52%|                                       | 12/23 [00:48<00:44,  4.04s/it]INFO:root:Working on chr18\n",
      "INFO:root:Found 1477023 samples for chr18; 4412 positive and 1472611 negative\n",
      "INFO:root:Found 1477023 samples for chr18; 4412 positive and 1472611 negative\n",
      " 57%|                                   | 13/23 [00:51<00:39,  3.94s/it]INFO:root:Working on chr6\n",
      "INFO:root:Found 3301189 samples for chr6; 11995 positive and 3289194 negative\n",
      "INFO:root:Found 3301189 samples for chr6; 11995 positive and 3289194 negative\n",
      " 61%|                                | 14/23 [00:57<00:36,  4.09s/it]INFO:root:Working on chr14\n",
      "INFO:root:Found 1745657 samples for chr14; 5540 positive and 1740117 negative\n",
      "INFO:root:Found 1745657 samples for chr14; 5540 positive and 1740117 negative\n",
      " 65%|                            | 15/23 [01:00<00:32,  4.05s/it]INFO:root:Working on chr1\n",
      "INFO:root:Found 4441429 samples for chr1; 17516 positive and 4423913 negative\n",
      "INFO:root:Found 4441429 samples for chr1; 17516 positive and 4423913 negative\n",
      " 70%|                         | 16/23 [01:09<00:30,  4.34s/it]INFO:root:Working on chr20\n",
      "INFO:root:Found 1173919 samples for chr20; 4819 positive and 1169100 negative\n",
      "INFO:root:Found 1173919 samples for chr20; 4819 positive and 1169100 negative\n",
      " 74%|                     | 17/23 [01:11<00:25,  4.22s/it]INFO:root:Working on chr5\n",
      "INFO:root:Found 3500521 samples for chr5; 10008 positive and 3490513 negative\n",
      "INFO:root:Found 3500521 samples for chr5; 10008 positive and 3490513 negative\n",
      " 78%|                 | 18/23 [01:18<00:21,  4.35s/it]INFO:root:Working on chr2\n",
      "INFO:root:Found 4702853 samples for chr2; 15199 positive and 4687654 negative\n",
      "INFO:root:Found 4702853 samples for chr2; 15199 positive and 4687654 negative\n",
      " 83%|              | 19/23 [01:26<00:18,  4.56s/it]INFO:root:Working on chr8\n",
      "INFO:root:Found 2804521 samples for chr8; 8931 positive and 2795590 negative\n",
      "INFO:root:Found 2804521 samples for chr8; 8931 positive and 2795590 negative\n",
      " 87%|          | 20/23 [01:32<00:13,  4.61s/it]INFO:root:Working on chr22\n",
      "INFO:root:Found 688963 samples for chr22; 3114 positive and 685849 negative\n",
      "INFO:root:Found 688963 samples for chr22; 3114 positive and 685849 negative\n",
      " 91%|       | 21/23 [01:33<00:08,  4.47s/it]INFO:root:Working on chr3\n",
      "INFO:root:Found 3843641 samples for chr3; 12890 positive and 3830751 negative\n",
      "INFO:root:Found 3843641 samples for chr3; 12890 positive and 3830751 negative\n",
      " 96%|   | 22/23 [01:40<00:04,  4.57s/it]INFO:root:Working on chr11\n",
      "INFO:root:Found 2565209 samples for chr11; 8710 positive and 2556499 negative\n",
      "INFO:root:Found 2565209 samples for chr11; 8710 positive and 2556499 negative\n",
      "100%|| 23/23 [01:45<00:00,  4.60s/it]\n",
      "INFO:root:Writing samples to disk\n",
      "INFO:root:Done writing samples to disk\n",
      "INFO:root:Reading static data\n",
      "INFO:root:Loaded training/valid/test of sizes 241560/68210/7912133\n",
      "INFO:root:Creating generators\n",
      "INFO:root:Creating feature generators\n",
      "INFO:root:Calculating sample weights\n",
      "INFO:root:Creating features sequencers\n",
      "INFO:root:Creating value generators\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:The function values_gen took     0.0000s\n",
      "INFO:root:Loading available previous hyperparameters settings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpe statuses ['new']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Setting up hyperopt objective\n",
      "INFO:hyperopt.tpe:tpe_transform took 0.002935 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "INFO:root:Making checkpoint for hyperparameters\n",
      "INFO:root:Current parameters :\n",
      "lstm_size : 64\n",
      "rec_do_rate : 0.5\n",
      "optimizer : <keras.optimizers.Adam object at 0x00000243641E49E8>\n",
      "bi_do_rate : 0.1\n",
      "epochs : 200\n",
      "batch_size : 512\n",
      "monitor : val_f1\n",
      "INFO:root:Found y are labels; using OBIE\n",
      "INFO:root:Found y are labels; using OBIE\n",
      "INFO:root:Current monitor mode is set to max\n",
      "INFO:root:Continuing previous training session\n",
      "INFO:root:Restarting from 5\n",
      "INFO:root:load file from FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0005-0.95.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200\n",
      "471/471 [==============================] - 524s 1s/step - loss: 0.3881 - acc: 0.5694 - recall: 0.6247 - precision: 0.8481 - f1: 0.7191 - val_loss: 0.2733 - val_acc: 0.9396 - val_recall: 0.7516 - val_precision: 0.6231 - val_f1: 0.6802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 45 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_f1 improved from -inf to 0.68024, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0006-0.94.hdf5\n",
      "Epoch 7/200\n",
      "471/471 [==============================] - 526s 1s/step - loss: 0.3618 - acc: 0.5694 - recall: 0.6404 - precision: 0.8495 - f1: 0.7299 - val_loss: 0.2617 - val_acc: 0.9347 - val_recall: 0.7661 - val_precision: 0.5926 - val_f1: 0.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 44 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00007: val_f1 did not improve from 0.68024\n",
      "Epoch 8/200\n",
      "471/471 [==============================] - 532s 1s/step - loss: 0.3384 - acc: 0.5693 - recall: 0.6513 - precision: 0.8496 - f1: 0.7370 - val_loss: 0.2425 - val_acc: 0.9334 - val_recall: 0.7712 - val_precision: 0.5851 - val_f1: 0.6643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 43 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: val_f1 did not improve from 0.68024\n",
      "Epoch 9/200\n",
      "471/471 [==============================] - 528s 1s/step - loss: 0.3166 - acc: 0.5694 - recall: 0.6610 - precision: 0.8490 - f1: 0.7430 - val_loss: 0.2214 - val_acc: 0.9345 - val_recall: 0.7696 - val_precision: 0.5913 - val_f1: 0.6677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 42 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_f1 did not improve from 0.68024\n",
      "Epoch 10/200\n",
      "471/471 [==============================] - 537s 1s/step - loss: 0.2955 - acc: 0.5694 - recall: 0.6703 - precision: 0.8479 - f1: 0.7484 - val_loss: 0.2062 - val_acc: 0.9328 - val_recall: 0.7777 - val_precision: 0.5812 - val_f1: 0.6641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 41 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: val_f1 did not improve from 0.68024\n",
      "Epoch 11/200\n",
      "471/471 [==============================] - 529s 1s/step - loss: 0.2755 - acc: 0.5693 - recall: 0.6801 - precision: 0.8464 - f1: 0.7539 - val_loss: 0.1821 - val_acc: 0.9374 - val_recall: 0.7677 - val_precision: 0.6072 - val_f1: 0.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 40 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00011: val_f1 did not improve from 0.68024\n",
      "Epoch 12/200\n",
      "471/471 [==============================] - 535s 1s/step - loss: 0.2568 - acc: 0.5694 - recall: 0.6907 - precision: 0.8444 - f1: 0.7595 - val_loss: 0.1735 - val_acc: 0.9337 - val_recall: 0.7807 - val_precision: 0.5853 - val_f1: 0.6679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 39 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_f1 did not improve from 0.68024\n",
      "Epoch 13/200\n",
      "471/471 [==============================] - 543s 1s/step - loss: 0.2393 - acc: 0.5694 - recall: 0.6997 - precision: 0.8425 - f1: 0.7642 - val_loss: 0.1570 - val_acc: 0.9356 - val_recall: 0.7782 - val_precision: 0.5957 - val_f1: 0.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 38 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_f1 did not improve from 0.68024\n",
      "Epoch 14/200\n",
      "471/471 [==============================] - 535s 1s/step - loss: 0.2235 - acc: 0.5693 - recall: 0.7069 - precision: 0.8410 - f1: 0.7678 - val_loss: 0.1476 - val_acc: 0.9342 - val_recall: 0.7837 - val_precision: 0.5877 - val_f1: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 37 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00014: val_f1 did not improve from 0.68024\n",
      "Epoch 15/200\n",
      "471/471 [==============================] - 534s 1s/step - loss: 0.2084 - acc: 0.5693 - recall: 0.7148 - precision: 0.8402 - f1: 0.7721 - val_loss: 0.1327 - val_acc: 0.9381 - val_recall: 0.7765 - val_precision: 0.6093 - val_f1: 0.6817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 36 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_f1 improved from 0.68024 to 0.68174, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0015-0.94.hdf5\n",
      "Epoch 16/200\n",
      "471/471 [==============================] - 539s 1s/step - loss: 0.1947 - acc: 0.5694 - recall: 0.7219 - precision: 0.8392 - f1: 0.7758 - val_loss: 0.1225 - val_acc: 0.9387 - val_recall: 0.7752 - val_precision: 0.6132 - val_f1: 0.6837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 35 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: val_f1 improved from 0.68174 to 0.68366, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0016-0.94.hdf5\n",
      "Epoch 17/200\n",
      "471/471 [==============================] - 539s 1s/step - loss: 0.1825 - acc: 0.5694 - recall: 0.7259 - precision: 0.8382 - f1: 0.7777 - val_loss: 0.1112 - val_acc: 0.9417 - val_recall: 0.7666 - val_precision: 0.6328 - val_f1: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 34 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_f1 improved from 0.68366 to 0.69222, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0017-0.94.hdf5\n",
      "Epoch 18/200\n",
      "471/471 [==============================] - 545s 1s/step - loss: 0.1703 - acc: 0.5693 - recall: 0.7320 - precision: 0.8389 - f1: 0.7815 - val_loss: 0.1041 - val_acc: 0.9416 - val_recall: 0.7680 - val_precision: 0.6315 - val_f1: 0.6920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 33 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_f1 did not improve from 0.69222\n",
      "Epoch 19/200\n",
      "471/471 [==============================] - 540s 1s/step - loss: 0.1595 - acc: 0.5695 - recall: 0.7363 - precision: 0.8391 - f1: 0.7841 - val_loss: 0.1055 - val_acc: 0.9333 - val_recall: 0.7940 - val_precision: 0.5820 - val_f1: 0.6705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 32 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00019: val_f1 did not improve from 0.69222\n",
      "Epoch 20/200\n",
      "471/471 [==============================] - 543s 1s/step - loss: 0.1494 - acc: 0.5693 - recall: 0.7395 - precision: 0.8397 - f1: 0.7861 - val_loss: 0.0959 - val_acc: 0.9371 - val_recall: 0.7849 - val_precision: 0.6032 - val_f1: 0.6810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 31 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00020: val_f1 did not improve from 0.69222\n",
      "Epoch 21/200\n",
      "471/471 [==============================] - 541s 1s/step - loss: 0.1401 - acc: 0.5694 - recall: 0.7416 - precision: 0.8389 - f1: 0.7869 - val_loss: 0.0932 - val_acc: 0.9336 - val_recall: 0.7980 - val_precision: 0.5832 - val_f1: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 30 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: val_f1 did not improve from 0.69222\n",
      "Epoch 22/200\n",
      "471/471 [==============================] - 536s 1s/step - loss: 0.1311 - acc: 0.5693 - recall: 0.7460 - precision: 0.8393 - f1: 0.7896 - val_loss: 0.0847 - val_acc: 0.9380 - val_recall: 0.7884 - val_precision: 0.6075 - val_f1: 0.6851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 29 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00022: val_f1 did not improve from 0.69222\n",
      "Epoch 23/200\n",
      "471/471 [==============================] - 541s 1s/step - loss: 0.1230 - acc: 0.5694 - recall: 0.7487 - precision: 0.8408 - f1: 0.7917 - val_loss: 0.0795 - val_acc: 0.9389 - val_recall: 0.7889 - val_precision: 0.6125 - val_f1: 0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 28 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00023: val_f1 did not improve from 0.69222\n",
      "Epoch 24/200\n",
      "471/471 [==============================] - 540s 1s/step - loss: 0.1155 - acc: 0.5694 - recall: 0.7501 - precision: 0.8410 - f1: 0.7926 - val_loss: 0.0740 - val_acc: 0.9412 - val_recall: 0.7845 - val_precision: 0.6262 - val_f1: 0.6952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 27 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_f1 improved from 0.69222 to 0.69524, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0024-0.94.hdf5\n",
      "Epoch 25/200\n",
      "471/471 [==============================] - 536s 1s/step - loss: 0.1082 - acc: 0.5693 - recall: 0.7529 - precision: 0.8430 - f1: 0.7951 - val_loss: 0.0712 - val_acc: 0.9394 - val_recall: 0.7921 - val_precision: 0.6145 - val_f1: 0.6908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 26 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00025: val_f1 did not improve from 0.69524\n",
      "Epoch 26/200\n",
      "471/471 [==============================] - 554s 1s/step - loss: 0.1017 - acc: 0.5694 - recall: 0.7546 - precision: 0.8431 - f1: 0.7961 - val_loss: 0.0687 - val_acc: 0.9370 - val_recall: 0.8005 - val_precision: 0.6003 - val_f1: 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 25 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00026: val_f1 did not improve from 0.69524\n",
      "Epoch 27/200\n",
      "471/471 [==============================] - 541s 1s/step - loss: 0.0955 - acc: 0.5693 - recall: 0.7571 - precision: 0.8432 - f1: 0.7975 - val_loss: 0.0634 - val_acc: 0.9404 - val_recall: 0.7939 - val_precision: 0.6198 - val_f1: 0.6949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 24 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00027: val_f1 did not improve from 0.69524\n",
      "Epoch 28/200\n",
      "471/471 [==============================] - 529s 1s/step - loss: 0.0900 - acc: 0.5693 - recall: 0.7576 - precision: 0.8445 - f1: 0.7983 - val_loss: 0.0604 - val_acc: 0.9403 - val_recall: 0.7966 - val_precision: 0.6188 - val_f1: 0.6954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 23 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00028: val_f1 improved from 0.69524 to 0.69535, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0028-0.94.hdf5\n",
      "Epoch 29/200\n",
      "471/471 [==============================] - 530s 1s/step - loss: 0.0849 - acc: 0.5695 - recall: 0.7585 - precision: 0.8448 - f1: 0.7990 - val_loss: 0.0586 - val_acc: 0.9382 - val_recall: 0.8038 - val_precision: 0.6061 - val_f1: 0.6898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 22 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00029: val_f1 did not improve from 0.69535\n",
      "Epoch 30/200\n",
      "471/471 [==============================] - 532s 1s/step - loss: 0.0800 - acc: 0.5693 - recall: 0.7588 - precision: 0.8452 - f1: 0.7993 - val_loss: 0.0559 - val_acc: 0.9380 - val_recall: 0.8056 - val_precision: 0.6045 - val_f1: 0.6895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 21 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_f1 did not improve from 0.69535\n",
      "Epoch 31/200\n",
      "471/471 [==============================] - 529s 1s/step - loss: 0.0755 - acc: 0.5693 - recall: 0.7616 - precision: 0.8461 - f1: 0.8013 - val_loss: 0.0505 - val_acc: 0.9432 - val_recall: 0.7897 - val_precision: 0.6370 - val_f1: 0.7040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 20 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00031: val_f1 improved from 0.69535 to 0.70395, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0031-0.94.hdf5\n",
      "Epoch 32/200\n",
      "471/471 [==============================] - 559s 1s/step - loss: 0.0714 - acc: 0.5695 - recall: 0.7611 - precision: 0.8452 - f1: 0.8006 - val_loss: 0.0508 - val_acc: 0.9374 - val_recall: 0.8080 - val_precision: 0.6012 - val_f1: 0.6882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 19 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00032: val_f1 did not improve from 0.70395\n",
      "Epoch 33/200\n",
      "471/471 [==============================] - 539s 1s/step - loss: 0.0675 - acc: 0.5694 - recall: 0.7635 - precision: 0.8457 - f1: 0.8022 - val_loss: 0.0470 - val_acc: 0.9413 - val_recall: 0.7980 - val_precision: 0.6243 - val_f1: 0.6992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 18 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00033: val_f1 did not improve from 0.70395\n",
      "Epoch 34/200\n",
      "471/471 [==============================] - 541s 1s/step - loss: 0.0640 - acc: 0.5694 - recall: 0.7649 - precision: 0.8464 - f1: 0.8032 - val_loss: 0.0458 - val_acc: 0.9386 - val_recall: 0.8072 - val_precision: 0.6074 - val_f1: 0.6919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 17 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_f1 did not improve from 0.70395\n",
      "Epoch 35/200\n",
      "471/471 [==============================] - 526s 1s/step - loss: 0.0607 - acc: 0.5693 - recall: 0.7645 - precision: 0.8465 - f1: 0.8030 - val_loss: 0.0437 - val_acc: 0.9397 - val_recall: 0.8047 - val_precision: 0.6139 - val_f1: 0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 16 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_f1 did not improve from 0.70395\n",
      "Epoch 36/200\n",
      "471/471 [==============================] - 528s 1s/step - loss: 0.0577 - acc: 0.5693 - recall: 0.7650 - precision: 0.8463 - f1: 0.8032 - val_loss: 0.0395 - val_acc: 0.9453 - val_recall: 0.7856 - val_precision: 0.6512 - val_f1: 0.7109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 15 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00036: val_f1 improved from 0.70395 to 0.71087, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0036-0.95.hdf5\n",
      "Epoch 37/200\n",
      "471/471 [==============================] - 542s 1s/step - loss: 0.0548 - acc: 0.5694 - recall: 0.7664 - precision: 0.8457 - f1: 0.8038 - val_loss: 0.0396 - val_acc: 0.9402 - val_recall: 0.8022 - val_precision: 0.6173 - val_f1: 0.6963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 14 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00037: val_f1 did not improve from 0.71087\n",
      "Epoch 38/200\n",
      "471/471 [==============================] - 543s 1s/step - loss: 0.0523 - acc: 0.5695 - recall: 0.7654 - precision: 0.8456 - f1: 0.8032 - val_loss: 0.0366 - val_acc: 0.9459 - val_recall: 0.7851 - val_precision: 0.6547 - val_f1: 0.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 13 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00038: val_f1 improved from 0.71087 to 0.71287, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0038-0.95.hdf5\n",
      "Epoch 39/200\n",
      "471/471 [==============================] - 537s 1s/step - loss: 0.0497 - acc: 0.5692 - recall: 0.7684 - precision: 0.8462 - f1: 0.8050 - val_loss: 0.0372 - val_acc: 0.9381 - val_recall: 0.8087 - val_precision: 0.6051 - val_f1: 0.6909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 12 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00039: val_f1 did not improve from 0.71287\n",
      "Epoch 40/200\n",
      "471/471 [==============================] - 527s 1s/step - loss: 0.0475 - acc: 0.5694 - recall: 0.7688 - precision: 0.8453 - f1: 0.8048 - val_loss: 0.0362 - val_acc: 0.9362 - val_recall: 0.8159 - val_precision: 0.5942 - val_f1: 0.6863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 11 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_f1 did not improve from 0.71287\n",
      "Epoch 41/200\n",
      "471/471 [==============================] - 538s 1s/step - loss: 0.0454 - acc: 0.5693 - recall: 0.7685 - precision: 0.8458 - f1: 0.8049 - val_loss: 0.0337 - val_acc: 0.9412 - val_recall: 0.7997 - val_precision: 0.6230 - val_f1: 0.6990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 10 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00041: val_f1 did not improve from 0.71287\n",
      "Epoch 42/200\n",
      "471/471 [==============================] - 522s 1s/step - loss: 0.0435 - acc: 0.5693 - recall: 0.7698 - precision: 0.8457 - f1: 0.8056 - val_loss: 0.0318 - val_acc: 0.9444 - val_recall: 0.7888 - val_precision: 0.6445 - val_f1: 0.7082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 9 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_f1 did not improve from 0.71287\n",
      "Epoch 43/200\n",
      "471/471 [==============================] - 521s 1s/step - loss: 0.0417 - acc: 0.5693 - recall: 0.7708 - precision: 0.8450 - f1: 0.8058 - val_loss: 0.0299 - val_acc: 0.9472 - val_recall: 0.7784 - val_precision: 0.6649 - val_f1: 0.7159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 8 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00043: val_f1 improved from 0.71287 to 0.71591, saving model to FOXA2_outputs\\FOXA2_bigru_crf_model_outputs\\FOXA2_bigru_crf-0043-0.95.hdf5\n",
      "Epoch 44/200\n",
      "471/471 [==============================] - 538s 1s/step - loss: 0.0401 - acc: 0.5695 - recall: 0.7713 - precision: 0.8444 - f1: 0.8058 - val_loss: 0.0312 - val_acc: 0.9372 - val_recall: 0.8121 - val_precision: 0.5996 - val_f1: 0.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 7 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_f1 did not improve from 0.71591\n",
      "Epoch 45/200\n",
      "471/471 [==============================] - 522s 1s/step - loss: 0.0385 - acc: 0.5693 - recall: 0.7718 - precision: 0.8443 - f1: 0.8060 - val_loss: 0.0304 - val_acc: 0.9366 - val_recall: 0.8144 - val_precision: 0.5962 - val_f1: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 6 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00045: val_f1 did not improve from 0.71591\n",
      "Epoch 46/200\n",
      "471/471 [==============================] - 522s 1s/step - loss: 0.0371 - acc: 0.5694 - recall: 0.7713 - precision: 0.8439 - f1: 0.8055 - val_loss: 0.0276 - val_acc: 0.9457 - val_recall: 0.7838 - val_precision: 0.6541 - val_f1: 0.7119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 5 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00046: val_f1 did not improve from 0.71591\n",
      "Epoch 47/200\n",
      "471/471 [==============================] - 524s 1s/step - loss: 0.0357 - acc: 0.5695 - recall: 0.7731 - precision: 0.8446 - f1: 0.8069 - val_loss: 0.0287 - val_acc: 0.9360 - val_recall: 0.8162 - val_precision: 0.5934 - val_f1: 0.6858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 4 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00047: val_f1 did not improve from 0.71591\n",
      "Epoch 48/200\n",
      "471/471 [==============================] - 527s 1s/step - loss: 0.0344 - acc: 0.5692 - recall: 0.7731 - precision: 0.8438 - f1: 0.8066 - val_loss: 0.0274 - val_acc: 0.9389 - val_recall: 0.8062 - val_precision: 0.6094 - val_f1: 0.6927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 3 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00048: val_f1 did not improve from 0.71591\n",
      "Epoch 49/200\n",
      "471/471 [==============================] - 526s 1s/step - loss: 0.0333 - acc: 0.5694 - recall: 0.7724 - precision: 0.8432 - f1: 0.8058 - val_loss: 0.0272 - val_acc: 0.9361 - val_recall: 0.8163 - val_precision: 0.5936 - val_f1: 0.6860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 2 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00049: val_f1 did not improve from 0.71591\n",
      "Epoch 50/200\n",
      "471/471 [==============================] - 525s 1s/step - loss: 0.0322 - acc: 0.5693 - recall: 0.7742 - precision: 0.8437 - f1: 0.8071 - val_loss: 0.0260 - val_acc: 0.9401 - val_recall: 0.8029 - val_precision: 0.6166 - val_f1: 0.6961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 1 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00050: val_f1 did not improve from 0.71591\n",
      "Epoch 51/200\n",
      "471/471 [==============================] - 521s 1s/step - loss: 0.0312 - acc: 0.5694 - recall: 0.7740 - precision: 0.8430 - f1: 0.8067 - val_loss: 0.0243 - val_acc: 0.9460 - val_recall: 0.7815 - val_precision: 0.6563 - val_f1: 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Calculating metrics...\n",
      "  0%|                                                                                          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|                                                                                 | 1/117 [00:05<10:01,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                                                                | 2/117 [00:08<08:32,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                                | 3/117 [00:12<08:02,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                               | 4/117 [00:16<07:43,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                              | 5/117 [00:20<07:33,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|                                                                             | 6/117 [00:24<07:24,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|                                                                             | 7/117 [00:27<07:15,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                            | 8/117 [00:31<07:09,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|                                                                           | 9/117 [00:35<07:02,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                          | 10/117 [00:38<06:57,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|                                                                         | 11/117 [00:42<06:51,  3.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                        | 12/117 [00:46<06:47,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|                                                                        | 13/117 [00:50<06:42,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|                                                                       | 14/117 [00:54<06:37,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                                      | 15/117 [00:57<06:33,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|                                                                      | 16/117 [01:01<06:29,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                     | 17/117 [01:05<06:25,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|                                                                    | 18/117 [01:09<06:21,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|                                                                   | 19/117 [01:13<06:17,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                   | 20/117 [01:17<06:13,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|                                                                  | 21/117 [01:20<06:09,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|                                                                 | 22/117 [01:24<06:06,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                 | 23/117 [01:28<06:02,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|                                                                | 24/117 [01:32<05:58,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|                                                               | 25/117 [01:36<05:54,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|                                                               | 26/117 [01:40<05:51,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                              | 27/117 [01:44<05:47,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|                                                             | 28/117 [01:47<05:43,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|                                                             | 29/117 [01:51<05:39,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                            | 30/117 [01:55<05:35,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|                                                           | 31/117 [01:59<05:31,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                          | 32/117 [02:03<05:27,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|                                                          | 33/117 [02:06<05:23,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|                                                         | 34/117 [02:10<05:19,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                        | 35/117 [02:15<05:16,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|                                                        | 36/117 [02:18<05:12,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                       | 37/117 [02:22<05:08,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|                                                      | 38/117 [02:26<05:05,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                      | 39/117 [02:30<05:00,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|                                                     | 40/117 [02:34<04:56,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|                                                    | 41/117 [02:37<04:52,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|                                                    | 42/117 [02:41<04:48,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                                   | 43/117 [02:45<04:44,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                  | 44/117 [02:49<04:40,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|                                                 | 45/117 [02:52<04:36,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|                                                 | 46/117 [02:56<04:32,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                                | 47/117 [03:00<04:28,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|                                               | 48/117 [03:03<04:24,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|                                               | 49/117 [03:07<04:20,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                              | 50/117 [03:11<04:16,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                             | 51/117 [03:15<04:12,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|                                             | 52/117 [03:18<04:08,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|                                            | 53/117 [03:22<04:04,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|                                           | 54/117 [03:26<04:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                           | 55/117 [03:29<03:56,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|                                          | 56/117 [03:33<03:52,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|                                         | 57/117 [03:37<03:48,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 58/117 [03:41<03:44,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                        | 59/117 [03:44<03:41,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|                                       | 60/117 [03:48<03:37,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|                                      | 61/117 [03:52<03:33,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                      | 62/117 [03:55<03:29,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|                                     | 63/117 [03:59<03:25,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|                                    | 64/117 [04:03<03:21,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                    | 65/117 [04:07<03:17,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|                                   | 66/117 [04:10<03:13,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|                                  | 67/117 [04:14<03:09,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|                                  | 68/117 [04:18<03:05,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|                                 | 69/117 [04:21<03:02,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|                                | 70/117 [04:25<02:58,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|                               | 71/117 [04:29<02:54,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|                               | 72/117 [04:32<02:50,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|                              | 73/117 [04:36<02:46,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                             | 74/117 [04:40<02:42,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|                             | 75/117 [04:43<02:38,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|                            | 76/117 [04:47<02:35,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|                           | 77/117 [04:51<02:31,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                           | 78/117 [04:55<02:27,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                          | 79/117 [04:58<02:23,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|                         | 80/117 [05:02<02:19,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|                         | 81/117 [05:06<02:16,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                        | 82/117 [05:09<02:12,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|                       | 83/117 [05:13<02:08,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|                      | 84/117 [05:17<02:04,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                      | 85/117 [05:20<02:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                     | 86/117 [05:24<01:57,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|                    | 87/117 [05:28<01:53,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|                    | 88/117 [05:32<01:49,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|                   | 89/117 [05:35<01:45,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|                  | 90/117 [05:39<01:41,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|                  | 91/117 [05:43<01:38,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|                 | 92/117 [05:46<01:34,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|                | 93/117 [05:50<01:30,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                | 94/117 [05:54<01:26,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|               | 95/117 [05:57<01:22,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|              | 96/117 [06:01<01:19,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|             | 97/117 [06:05<01:15,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|             | 98/117 [06:08<01:11,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|            | 99/117 [06:12<01:07,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|           | 100/117 [06:16<01:03,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|           | 101/117 [06:19<01:00,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|          | 102/117 [06:23<00:56,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|         | 103/117 [06:27<00:52,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|         | 104/117 [06:30<00:48,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|        | 105/117 [06:34<00:45,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|       | 106/117 [06:38<00:41,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|      | 107/117 [06:42<00:37,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|      | 108/117 [06:45<00:33,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|     | 109/117 [06:49<00:30,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|    | 110/117 [06:53<00:26,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|    | 111/117 [06:56<00:22,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|   | 112/117 [07:00<00:18,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|  | 113/117 [07:04<00:15,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|  | 114/117 [07:08<00:11,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%| | 115/117 [07:11<00:07,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|| 116/117 [07:15<00:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 3s 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 117/117 [07:19<00:00,  3.75s/it]\n",
      "INFO:root:Epoch 50:\n",
      "INFO:root:avg auROC:   0.8665 avg auPRC:   0.7644\n",
      "INFO:root:Recall@5%/10%/25%/50%: 0.0000 0.6322 31.7805 76.2451\n",
      "INFO:root:tp:      4597 fn:      1421 tn:     52304 fp:      1582\n",
      "INFO:root:tp|###############_____|fn   tn|###################_|fp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00051: val_f1 did not improve from 0.71591\n",
      "Epoch 52/200\n",
      "471/471 [==============================] - 521s 1s/step - loss: 0.0302 - acc: 0.5693 - recall: 0.7740 - precision: 0.8435 - f1: 0.8068 - val_loss: 0.0243 - val_acc: 0.9428 - val_recall: 0.7949 - val_precision: 0.6337 - val_f1: 0.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 49 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00052: val_f1 did not improve from 0.71591\n",
      "Epoch 53/200\n",
      "471/471 [==============================] - 525s 1s/step - loss: 0.0294 - acc: 0.5696 - recall: 0.7745 - precision: 0.8436 - f1: 0.8072 - val_loss: 0.0238 - val_acc: 0.9426 - val_recall: 0.7970 - val_precision: 0.6322 - val_f1: 0.7037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Showing metric in 48 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00053: val_f1 did not improve from 0.71591\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 200, 6)            0         \n",
      "_________________________________________________________________\n",
      "model_10 (Model)             (None, 200, 4)            27780     \n",
      "_________________________________________________________________\n",
      "crf_ext_4 (ClassWrapper)     (None, 200, 4)            44        \n",
      "=================================================================\n",
      "Total params: 27,824\n",
      "Trainable params: 27,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Getting test y prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15453/15453 [==============================] - 7486s 484ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Converting test y prediction to categorical\n"
     ]
    }
   ],
   "source": [
    "# %%memit\n",
    "model_statsNhistory = []\n",
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "err_obj = None\n",
    "err = None\n",
    "use_generator=True\n",
    "reduce_negative_samples=False\n",
    "try:\n",
    "    for tf_name, dm in tf_nameNdm:\n",
    "        logging.info('Working on {}'.format(tf_name))\n",
    "        tf_dir = '{}_{}'.format(tf_name, 'outputs')\n",
    "        \n",
    "        for _model_choice, _model_name, search_space  in labelled_test_models:\n",
    "            logging.info('Testing {}'.format(_model_name))\n",
    "            working_dir = '{}_{}_{}'.format(tf_name, _model_name, 'model_outputs')\n",
    "            output_path = os.path.relpath(os.path.join(cwd, tf_dir, working_dir))\n",
    "            # make a path for the outputs\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "            model_statsNhistory.append(run_model(Models.use(_model_choice, _model_name),\n",
    "                                                 search_space,\n",
    "                                                 _model_name, tf_name,\n",
    "                                                 training_size_limit=None, \n",
    "                                                 valid_size_limit=None, \n",
    "                                                 test_size_limit=None,\n",
    "                                                 n_trials=n_trials, \n",
    "                                                 patience=patience, show_metric_interval=50,\n",
    "                                                 output_path=output_path, use_labels_as_y=True, \n",
    "                                                 use_generator=use_generator, reduce_negative_samples=reduce_negative_samples,\n",
    "                                                 show_design=True, seq_data_only=False, rerun_full=False ))\n",
    "        for _model_choice, _model_name, search_space  in test_models:\n",
    "            logging.info('Testing {}'.format(_model_name))\n",
    "            working_dir = '{}_{}_{}'.format(tf_name, _model_name, 'model_outputs')\n",
    "            output_path = os.path.relpath(os.path.join(cwd, tf_dir, working_dir))\n",
    "            # make a path for the outputs\n",
    "            os.makedirs(output_path, exist_ok=True)\n",
    "            logging.info('Running model for {}'.format(_model_choice))\n",
    "            model_statsNhistory.append(run_model(Models.use(_model_choice, _model_name),\n",
    "                                                 search_space,\n",
    "                                                 _model_name, tf_name,\n",
    "                                                 training_size_limit=None, \n",
    "                                                  valid_size_limit=None, \n",
    "                                                 test_size_limit=None,\n",
    "                                                 n_trials=n_trials, \n",
    "                                                 patience=patience,show_metric_interval=50,\n",
    "                                                 output_path=output_path,\n",
    "                                                 use_generator=use_generator, reduce_negative_samples=reduce_negative_samples,\n",
    "                                                 show_design=True, seq_data_only=False, rerun_full=False ))\n",
    "            logging.info('Finished running model for {}'.format(_model_choice))\n",
    "except DebugObject as e:\n",
    "    err_obj = e.content\n",
    "    err = e\n",
    "    print('Error object loaded in err_obj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
